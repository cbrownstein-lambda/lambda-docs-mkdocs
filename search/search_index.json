{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#welcome-to-lambda-docs","title":"Welcome to Lambda Docs","text":"<ul> <li> <p> Public Cloud</p> <p>Instantly launch a 16x to 512x NVIDIA H100 GPU cluster or a single-node instance. No long-term commitments required. Managed Kubernetes, Preinstalled Kubernetes, and Slurm are available. Our Public Cloud includes use of serverless APIs for inference.</p> <p> Introduction</p> </li> <li> <p> Private Cloud</p> <p>Private Cloud is the ideal solution for organizations requiring a single-tenant cluster with greater than 512x GPUs. Private Cloud customers have low-level access to their cluster infrastructure. Managed Kubernetes, Preinstalled Kubernetes, and Slurm are available.</p> <p> Introduction</p> </li> <li> <p> Hardware</p> <p>For individuals and small teams looking for on-prem hardware solutions, we offer our Scalar servers, and our Vector One, Vector, and Vector Pro workstations and desktops. Our servers, workstations, and desktops can be configured to meet your specific needs.</p> <p> Introduction</p> </li> <li> <p> Education</p> <p>Our expert ML Engineers and Researchers have put together guides and tutorials to get you up and running quickly on Lambda's cloud and hardware solutions. Whether you're looking to fine-tune a model or host a load-balanced API server for inference, we have a guide for you.</p> <p> Introduction</p> </li> </ul>"},{"location":"education/","title":"Introduction","text":"<ul> <li>Tutorial: Getting started with training a machine learning model</li> </ul>"},{"location":"education/#generative-ai-gai","title":"Generative AI (GAI)","text":"<ul> <li>How to serve the FLUX.1 prompt-to-image models using Lambda Cloud on-demand instances</li> </ul>"},{"location":"education/#large-language-models-llms","title":"Large language models (LLMs)","text":"<ul> <li>Deploying a Llama 3 inference endpoint</li> <li>Deploying Llama 3.2 3B in a Kubernetes (K8s) cluster</li> <li>Using KubeAI to deploy Nous Research's Hermes 3 and other LLMs</li> <li>Serving the Llama 3.1 8B and 70B models using Lambda Cloud on-demand instances</li> <li>Serving Llama 3.1 405B on a Lambda 1-Click Cluster</li> </ul>"},{"location":"education/#linux-usage-and-system-administration","title":"Linux usage and system administration","text":"<ul> <li>Basic Linux commands and system administration</li> <li>Configuring Software RAID</li> <li>Lambda Stack and recovery images</li> <li>Troubleshooting and debugging</li> <li>Using the Lambda bug report to troubleshoot your system</li> <li>Using the nvidia-bug-report.log file to troubleshoot your system</li> <li>Using KubeAI to deploy Nous Research's Hermes 3 and other LLMs</li> </ul>"},{"location":"education/#programming","title":"Programming","text":"<ul> <li>Virtual environments and Docker containers</li> <li>Integrating Lambda Chat into VS Code</li> </ul>"},{"location":"education/#scheduling-and-orchestration","title":"Scheduling and orchestration","text":"<ul> <li>Deploying models with dstack</li> <li>Using SkyPilot to deploy a Kubernetes cluster</li> </ul>"},{"location":"education/tutorial-getting-started-with-training-a-machine-learning-model/","title":"Tutorial: Getting started with training a machine learning model","text":"","tags":["1-click clusters","automation","on-demand cloud"]},{"location":"education/tutorial-getting-started-with-training-a-machine-learning-model/#overview","title":"Overview","text":"<p>This tutorial walks you through the process of getting ready to start training a model using Lambda solutions. It helps you to scope out and decide what resources you are going to need and which Lambda solution to use.</p> <p>This tutorial is aimed at machine learning engineers and systems administrators responsible for setting up the training environment.</p>","tags":["1-click clusters","automation","on-demand cloud"]},{"location":"education/tutorial-getting-started-with-training-a-machine-learning-model/#before-you-begin","title":"Before you begin","text":"<p>Before you make your Lambda reservation, it\u2019s a good idea to scale out your project so you know which path to take. </p>","tags":["1-click clusters","automation","on-demand cloud"]},{"location":"education/tutorial-getting-started-with-training-a-machine-learning-model/#scope-your-project","title":"Scope your project","text":"<p>Start by getting a sense of the overall scope and strategy of your project. Some factors to consider include:</p> <ul> <li>Batch size: Are you processing a single stream or multiple streams? Processing eight parallel streams at the same time is much different than processing a single stream.</li> <li>Model size and composition: How big is your model? How many copies of your model can you fit on a single GPU, if any? What are the main components of the model? A transformer scales differently with input length than a pure convolutional model.</li> <li>Time: If you have time constraints, can you quantize the model to a lower precision to finish training faster? Or can you distill the model so you can train a smaller one?</li> <li>Data type and size: Is your training data composed of text, images, or something else? Is it a recurring process, like a stream of text? Or do you plan to have all the input processed at once, like with a dataset of documents?</li> </ul> <p>Once you have a general idea of the scope, you can move on to determine how much of each resource (GPUs/CPUs, VRAM, and storage) you need.</p>","tags":["1-click clusters","automation","on-demand cloud"]},{"location":"education/tutorial-getting-started-with-training-a-machine-learning-model/#vram","title":"VRAM","text":"<p>The amount of VRAM on a GPU helps determine how large your dataset and model can be without needing to offload to the system memory, which decreases efficiency. </p> <p>Use the following formula to estimate how much VRAM you need:</p> <pre><code>Memory required \u2248\n(model size x precision) +\n(batch size x sequence length x input dimension x precision) +\n(optimizer state x precision)\n</code></pre> <p>Where:</p> <ul> <li>Model size: is the number of parameters in your model</li> <li>Precision: use 4 bytes for single precision (FP32) or 2 bytes for mixed-precision training</li> <li>Batch size: is the number of samples to process in parallel</li> <li>Sequence length: is the length of the input sequences in bytes</li> <li>Input dimension: is the dimensionality of the input data; for example, use 128 for 128-dimensional word embeddings</li> <li>Optimizer state: is the size of the optimizer's internal state, which depends on the specific optimizer</li> </ul> <p>Lambda also offers its own VRAM calculator for large language models.</p>","tags":["1-click clusters","automation","on-demand cloud"]},{"location":"education/tutorial-getting-started-with-training-a-machine-learning-model/#gpus-and-cpus","title":"GPUs and CPUs","text":"<p>Once you know how much VRAM you need, you can then determine the minimum number of GPUs you need, since the number of GPUs is bound by the memory requirements and the memory limitations of a given GPU. You can then scale out to see how many more GPUs you can use, since utilizing more GPUs for your training speeds it up, reducing training time, in a nearly linear fashion. </p> <p>If you are training an LLM, the Lambda VRAM calculator can tell you the minimum number of GPUs you need. </p> <p>While GPUs occupy most of the spotlight when it comes to their importance in training ML models, CPUs play a very important role as well. Their workloads are more varied and fall into these categories:</p> <ul> <li>ML operations or workload management, including the monitoring stack</li> <li>Dedicated CPU processing tasks, typically in the pre- and post-processing of data sets</li> <li>Feeding the GPUs a consistent stream of data before the training application can take advantage of direct memory access (DMA) between GPUs or the storage devices</li> </ul> <p>Lambda provides sufficient CPU resources for the GPUs you can select when you make a reservation.</p>","tags":["1-click clusters","automation","on-demand cloud"]},{"location":"education/tutorial-getting-started-with-training-a-machine-learning-model/#storage","title":"Storage","text":"<p>To estimate how much storage you need, follow this rule of thumb: take the number of parameters you plan to train on and multiply that number by 2 bytes, as each parameter typically requires 2 bytes for 16-bit precision. For example, a 70 billion parameter model would require roughly 140GB of disk space.</p> <p>For 32-bit precision, double that number to 4 bytes per parameter. So this same 70 billion parameter model would require at least 280GB of storage.</p> <p>That said, training data size can vary drastically between domains, depending on whether you are training on NLP, LLM, vision, or audio data. In general, datasets typically require hundreds of gigabytes of storage, so having 1TB or 2TB in total provides a comfortable cushion.</p>","tags":["1-click clusters","automation","on-demand cloud"]},{"location":"education/tutorial-getting-started-with-training-a-machine-learning-model/#container-or-virtual-environment","title":"Container or virtual environment","text":"<p>A container or virtual environment can help streamline your AI training process by offering a consistent, isolated environment, better resource management, and easier collaboration. </p> <p>Lambda recommends these containers and virtual environments:</p> <ul> <li>Docker: a Docker container includes the whole runtime environment so you can train your model in the exact same environment on any cloud.</li> <li>Conda: Conda provides a virtual environment that is isolated from the system it runs on, so you can train your model consistently on any system while avoiding conflicts with the host system.</li> <li>Python: Python is a programming language with a virtual environment module, <code>venv</code>, that you use to create virtual environments.</li> </ul> <p>For more information on creating the container or virtual environment, read Virtual environments and Docker containers.</p>","tags":["1-click clusters","automation","on-demand cloud"]},{"location":"education/tutorial-getting-started-with-training-a-machine-learning-model/#on-demand-cloud-1-click-cluster-or-private-cloud","title":"On-Demand Cloud, 1-Click Cluster, or Private Cloud","text":"<p>Now that you have a good idea of what resources you need, you can choose the best product to suit your needs.</p> <ul> <li>On-Demand Cloud: is great for smaller models and shorter durations. On-Demand Cloud instances offer a choice from one to eight GPUs that you can rent by the hour.</li> <li>1-Click Clusters: is best for larger models requiring 16 to 512 GPUs that you can rent by the week, with discounts for longer terms.</li> <li>Private Cloud: is aimed at the largest and longest-lasting deployments. Lambda Private Cloud provides you with a cluster of 512 to 2048 GPUs for one to three years.</li> </ul> <p>If you need to start working immediately, choose On-Demand Cloud or reserve a 1-Click Cluster for the near term. If you are on a longer timeline and intend to train your AI over a period of years using a cluster of GPUs, then you should choose Lambda Private Cloud.</p> <p>This tutorial does not cover Private Cloud. For more information about these options and for pricing, visit the Lambda website.</p>","tags":["1-click clusters","automation","on-demand cloud"]},{"location":"education/tutorial-getting-started-with-training-a-machine-learning-model/#launch-a-lambda-instance","title":"Launch a Lambda instance","text":"<p>Before you can launch a Lambda On-Demand Cloud instance or 1-Click Cluster, you need to create a Lambda Public Cloud account if you have not already done so and provide the following information:</p> <ul> <li>Account type (individual or business)</li> <li>Your contact information</li> <li>Your billing information</li> </ul> <p>Agree to Lambda\u2019s terms and conditions, go to your Lambda Cloud dashboard in your browser, and then launch an instance:</p> <ul> <li>To launch an On-Demand Cloud instance, click Launch instance and follow the steps under Launch instances.</li> <li>To launch a 1-Click Cluster, click Request 1-Click Cluster then follow the steps under Reserving 1-Click Clusters.</li> </ul> <p>You also need to generate an SSH key, which you select during the launch process. Read the section on adding or generating an SSH key in the Dashboard chapter. </p>","tags":["1-click clusters","automation","on-demand cloud"]},{"location":"education/tutorial-getting-started-with-training-a-machine-learning-model/#lambda-stack","title":"Lambda Stack","text":"<p>Every Lambda instance comes with the Lambda Stack preinstalled. The Lambda Stack is an AI software suite that includes PyTorch, TensorFlow, CUDA, cuDNN, and NVIDIA drivers. It runs on Ubuntu 22.04 LTS and 20.04 LTS. For more information, read our website and the overview page. </p>","tags":["1-click clusters","automation","on-demand cloud"]},{"location":"education/tutorial-getting-started-with-training-a-machine-learning-model/#configure-the-container-or-virtual-environment","title":"Configure the container or virtual environment","text":"<p>With your instance or cluster up and running, you can create the Docker container or Conda or Python virtual environment in which you will run your training model. For more information, read Virtual environments and Docker containers.</p> <p>Once the container is running, check out your code repository to the container or virtual environment. If you don\u2019t have a testing repository yet, you can try one of the many examples suggested below.</p>","tags":["1-click clusters","automation","on-demand cloud"]},{"location":"education/tutorial-getting-started-with-training-a-machine-learning-model/#next-steps","title":"Next steps","text":"<p>The final step before you start training is to choose a training model.</p> <p>If you need guidance on where to go next, Lambda offers many different examples that you can run in your instance or cluster and see how they perform.  </p> <ul> <li>The Lambda distributed training guide is a comprehensive introduction to training, covering a range of topics from a single GPU to a cluster. Clone the relevant chapter into your instance or cluster and give it a try. Note that these examples run in a Python virtual environment. </li> <li>A single GPU on a single node. This is the most basic example. Since it is limited to one GPU on one node, there is no distributed training involved.</li> <li>Multiple GPUs on a single node. This example is more involved and actually demonstrates distributed training, since it splits the workload \u2014 the batches from the dataset \u2014 over multiple GPUs.</li> <li>Multiple GPUs across multiple nodes. You need a multi-node instance to try this example. It demonstrates the need for identical environments.</li> <li>GPU cluster. In order to test across a cluster, you need a mechanism to launch the training jobs on each node. You can use Bash, DeepSpeed, <code>mpirun</code>, or Slurm as the launcher.</li> <li>For more targeted examples, try these deep learning tutorials:</li> <li>Fine tuning Stable Diffusion. This example runs through performing extra training on a model to get more desirable results.</li> <li>Object detection using YoloV5. This workflow downloads a dataset, prepares it for YoloV5, then downloads pretrained weights files.</li> <li>GPTNeoX LLM distributed training. This example shows how to train a LLM across multiple nodes on Lambda Cloud. It uses the open source <code>gpt-neox</code> repository, which is built on DeepSpeed and MegatronLM.</li> <li>Experiment tracking comparison. This example compares these common experiment tracking libraries: Comet, MLflow, Neptune.ai, and <code>wandb</code>.</li> <li>PyTorch multi-node training. This example shows how to write and launch PyTorch distributed data parallel jobs. We walk you through this example on the Lambda blog</li> <li>Running Nerfstudio on Lambda. This example demonstrates how to create and view NeRF (Neural Radiance Fields) models.</li> </ul> <p>\\ \\ \\</p>","tags":["1-click clusters","automation","on-demand cloud"]},{"location":"education/generative-ai/flux-prompt-to-image/","title":"How to serve the FLUX.1 prompt-to-image models using Lambda Cloud on-demand instances","text":"<p>This tutorial shows you how to use Lambda Cloud A100 and H100 on-demand instances to download and serve a FLUX.1 prompt-to-image model. The model will be served as a Gradio app accessible with a link you can share.</p> <p>Note</p> <p>You can download and serve the FLUX.1 [schnell] model without a Hugging Face account.</p> <p>However, to download and serve the FLUX.1 [dev] model, you need a Hugging Face account and a User Access Token. You also need to review and accept the model license agreement.</p>","tags":["generative ai","stable diffusion"]},{"location":"education/generative-ai/flux-prompt-to-image/#clone-the-flux1-inference-repository","title":"Clone the FLUX.1 inference repository","text":"<p>If you haven't already, use the dashboard or Cloud API to launch a 1x or 8x A100 instance, or a 1x or 8x H100 instance. Then, SSH into the instance. Alternatively, you can use Jupyter Notebook to access the instance.</p> <p>Clone the FLUX.1 inference GitHub repository to your home directory by running:</p> <pre><code>cd \"$HOME\" &amp;&amp; git clone https://github.com/black-forest-labs/flux.git\n</code></pre> <p>Then, change into the repository directory by running:</p> <pre><code>cd ~/flux\n</code></pre>","tags":["generative ai","stable diffusion"]},{"location":"education/generative-ai/flux-prompt-to-image/#install-the-packages-needed-to-serve-the-flux1-model","title":"Install the packages needed to serve the FLUX.1 model","text":"<p>First, create and activate a Python virtual environment for this tutorial by running:</p> <pre><code>python3 -m venv .venv &amp;&amp; source .venv/bin/activate\n</code></pre> <p>Then, install the packages required for this tutorial by running:</p> <pre><code>pip install -e '.[all]'\n</code></pre>","tags":["generative ai","stable diffusion"]},{"location":"education/generative-ai/flux-prompt-to-image/#download-and-serve-the-flux1-model","title":"Download and serve the FLUX.1 model","text":"","tags":["generative ai","stable diffusion"]},{"location":"education/generative-ai/flux-prompt-to-image/#to-download-and-serve-the-flux1-schnell-model","title":"To download and serve the FLUX.1 [schnell] model","text":"<p>Run:</p> <pre><code>python3 demo_gr.py --name flux-schnell --share\n</code></pre>","tags":["generative ai","stable diffusion"]},{"location":"education/generative-ai/flux-prompt-to-image/#to-download-and-serve-the-flux1-dev-model","title":"To download and serve the FLUX.1 [dev] model","text":"<p>First, log into your Hugging Face account by running:</p> <pre><code>huggingface-cli login --token HF-TOKEN\n</code></pre> <p>Replace HF-TOKEN with your Hugging Face User Account Token.</p> <p>Then, run:</p> <pre><code>python3 demo_gr.py --name flux-dev --share\n</code></pre> <p>The FLUX.1 model is being served as a Gradio app once you see output similar to:</p> <pre><code>Running on local URL:  http://127.0.0.1:7860\nRunning on public URL: https://XXXXXXXXXXXXXXXXXX.gradio.live\n\nThis share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n</code></pre>","tags":["generative ai","stable diffusion"]},{"location":"education/generative-ai/flux-prompt-to-image/#access-the-gradio-app-serving-the-flux1-model","title":"Access the Gradio app serving the FLUX.1 model","text":"<p>Open the public URL to access the Gradio app serving the FLUX.1 model.</p> <p>Warning</p> <p>Anyone with the public URL can access the Gradio app serving the FLUX.1 model.</p> <p>As an example prompt, try:</p> <pre><code>a photo of a city skyline that clearly looks like the letters \"Lambda\". award winning font architecture, evening hours, some office windows shine through the night, The skyline says \"Lambda\"\n</code></pre> <p>With the FLUX.1 [dev] model, this prompt should generate an image similar to:</p> <p></p>","tags":["generative ai","stable diffusion"]},{"location":"education/large-language-models/deploying-a-llama-3-inference-endpoint/","title":"Deploying a Llama 3 inference endpoint","text":"<p>Meta's Llama 3 large language models (LLMs) feature generative text models recognized for their state-of-the-art performance in common industry benchmarks.</p> <p>This guide covers the deployment of a Meta Llama 3 inference endpoint using Lambda On-Demand Cloud. This tutorial uses the Llama 3 models hosted by Hugging Face.</p> <p>The model is available in 8B and 70B sizes:</p> Model Size Characteristics 8B (8 billion parameters) More efficient and accessible, suitable for tasks where resources are constrained. The 8B model requires a 1x A100 or H100 GPU node. 70B (70 billion parameters) Superior performance and capabilities ideal for complex or high-stakes applications. The 70B model requires an 8x A100 or H100 GPU node."},{"location":"education/large-language-models/deploying-a-llama-3-inference-endpoint/#prerequisites","title":"Prerequisites","text":"<p>This tutorial assumes the following prerequisites:</p> <ol> <li>Lambda On-Demand Cloud instances appropriate for the Llama 3 model size you want to run.<ul> <li>Model 8B (meta-llama/Meta-Llama-3-8B) requires 1x A100 or H100 GPU node.</li> <li>Model 70B (meta-llama/Meta-Llama-3-70B) requires 8x A100 or H100 GPU nodes.</li> </ul> </li> <li>A Hugging Face user account.</li> <li>An approved Hugging Face user access token that includes repository read permissions for the meta-llama-3 model repository you wish to use.</li> </ol> <p>JSON outputs in this tutorial are formatted using jq.</p>"},{"location":"education/large-language-models/deploying-a-llama-3-inference-endpoint/#set-up-the-inference-point","title":"Set up the inference point","text":"<p>Once you have the appropriate Lambda On-Demand Cloud instances and Hugging Face permissions, begin by setting up an inference point. </p> <ol> <li>Launch your Lambda On-Demand Cloud instance.</li> <li>Add or generate an SSH key to access the instance.</li> <li>SSH into your instance.</li> <li> <p>Create a dedicated python environment.</p> Llama3 8bLlama3 70b <pre><code>python3 -m venv Meta-Llama-3-8B\nsource Meta-Llama-3-8B/bin/activate\npython3 -m pip install vllm==0.4.3 huggingface-hub==0.23.2 torch==2.3.0 numpy==1.26.4\n</code></pre> <pre><code>python3 -m venv Meta-Llama-3-70B\nsource Meta-Llama-3-70B/bin/activate\npython3 -m pip install vllm==0.4.3 huggingface-hub==0.23.2 torch==2.3.0 numpy==1.26.4\n</code></pre> </li> <li> <p>Log in to Hugging Face:</p> <pre><code>huggingface-cli login\n</code></pre> </li> <li> <p>Start the model server (download/cache as necessary). </p> Llama3 8BLlama3 70B <pre><code>python3 -m vllm.entrypoints.openai.api_server \\\n  --host=0.0.0.0 \\\n  --port=8000 \\\n  --model=meta-llama/Meta-Llama-3-8B &amp;&gt; api_server.log &amp; \n</code></pre> <pre><code> python3 -m vllm.entrypoints.openai.api_server \\\n  --host=0.0.0.0 \\\n  --port=8000 \\\n  --model=meta-llama/Meta-Llama-3-70B \\\n  --tensor-parallel-size 8 &amp;&gt; api_server.log &amp;\n\n// The API server may take several minutes to start.\n</code></pre> </li> </ol>"},{"location":"education/large-language-models/deploying-a-llama-3-inference-endpoint/#interact-with-the-model","title":"Interact with the Model","text":"<p>The following request delivers language prompts to the Llama 3 model:</p> Llama3 8BLlama3 70B <pre><code>curl -X POST http://localhost:8000/v1/completions \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\n           \"prompt\": \"What is the name of the capital of France?\",\n           \"model\": \"meta-llama/Meta-Llama-3-8B\",\n           \"temperature\": 0.0,\n           \"max_tokens\": 1   // sets the response length\n         }'\n</code></pre> <pre><code>curl -X POST http://localhost:8000/v1/completions \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\n           \"prompt\": \"What is the name of the capital of France?\",\n           \"model\": \"meta-llama/Meta-Llama-3-70B\",\n           \"temperature\": 0.0,\n           \"max_tokens\": 1   // sets the response length\n         }'\n</code></pre> <p>Llama 3 responds to requests in the following format:</p> Llama3 8BLlama3 70B <pre><code>{\n  \"id\": \"cmpl-d898e2089b7b4855b48e00684b921c95\",\n  \"object\": \"text_completion\",\n  \"created\": 1718221710,\n  \"model\": \"meta-llama/Meta-Llama-3-8B\",\n  \"choices\": [\n    {\n      \"index\": 0,\n      \"text\": \" Paris\",\n      \"logprobs\": null,\n      \"finish_reason\": \"length\",\n      \"stop_reason\": null\n    }\n  ],\n  \"usage\": {\n      \"prompt_tokens\": 11,\n      \"total_tokens\": 12,\n      \"completion_tokens\": 1\n  }\n}\n</code></pre> <pre><code>{\n  \"id\": \"cmpl-d898e2089b7b4855b48e00684b921c95\",\n  \"object\": \"text_completion\",\n  \"created\": 1718221710,\n  \"model\": \"meta-llama/Meta-Llama-3-70B\",\n  \"choices\": [\n    {\n      \"index\": 0,\n      \"text\": \" Paris\",\n      \"logprobs\": null,\n      \"finish_reason\": \"length\",\n      \"stop_reason\": null\n    }\n  ],\n  \"usage\": {\n    \"prompt_tokens\": 11,\n    \"total_tokens\": 12,\n    \"completion_tokens\": 1\n  }\n}\n</code></pre>"},{"location":"education/large-language-models/k8s-ollama-llama-3-2/","title":"Deploying Llama 3.2 3B in a Kubernetes (K8s) cluster","text":"","tags":["api","kubernetes","llama","llm"]},{"location":"education/large-language-models/k8s-ollama-llama-3-2/#introduction","title":"Introduction","text":"<p>In this tutorial, you'll:</p> <ol> <li>Stand up a single-node Kubernetes cluster on an    on-demand instance     using K3s .</li> <li>Install the    NVIDIA GPU Operator     so your cluster can use your instance's GPUs.</li> <li>Deploy    Ollama  in    your cluster to serve the    Llama 3.2 3B model .</li> <li>Install the Ollama client.</li> <li>Interact with the Llama 3.2 3B model.</li> </ol> <p>Note</p> <p>You don't need a Kubernetes cluster to run Ollama and serve the Llama 3.2 3B model. Part of this tutorial is to demonstrate that it's possible to stand up a Kubernetes cluster on on-demand instances.</p>","tags":["api","kubernetes","llama","llm"]},{"location":"education/large-language-models/k8s-ollama-llama-3-2/#stand-up-a-single-node-kubernetes-cluster","title":"Stand up a single-node Kubernetes cluster","text":"<ol> <li> <p>If you haven't already, use the    dashboard     or Cloud API to launch an instance. Then, SSH into your instance.</p> </li> <li> <p>Install K3s (Kubernetes) by running:</p> <pre><code>curl -sfL https://get.k3s.io | K3S_KUBECONFIG_MODE=644 sh -s - --default-runtime=nvidia\n</code></pre> </li> <li> <p>Verify that your Kubernetes cluster is ready by running:</p> <pre><code>k3s kubectl get nodes\n</code></pre> <p>You should see output similar to:</p> <pre><code>NAME              STATUS   ROLES                  AGE    VERSION\n104-171-203-164   Ready    control-plane,master   100s   v1.30.5+k3s1\n</code></pre> </li> <li> <p>Install socat by running:</p> <pre><code>sudo apt -y install socat\n</code></pre> <p>socat is needed to enable port forwarding in a later step.</p> </li> </ol>","tags":["api","kubernetes","llama","llm"]},{"location":"education/large-language-models/k8s-ollama-llama-3-2/#install-the-nvidia-gpu-operator","title":"Install the NVIDIA GPU Operator","text":"<ol> <li> <p>Install the NVIDIA GPU Operator in your Kubernetes cluster by running:</p> <pre><code>cat &lt;&lt;EOF | k3s kubectl apply -f -\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: gpu-operator\n---\napiVersion: helm.cattle.io/v1\nkind: HelmChart\nmetadata:\n  name: gpu-operator\n  namespace: gpu-operator\nspec:\n  repo: https://helm.ngc.nvidia.com/nvidia\n  chart: gpu-operator\n  targetNamespace: gpu-operator\nEOF\n</code></pre> </li> <li> <p>In a few minutes, verify that your instance's GPUs are detected by your    cluster by running:</p> <pre><code>k3s kubectl describe nodes | grep nvidia.com\n</code></pre> <p>You should see output similar to:</p> <pre><code>nvidia.com/cuda.driver-version.full=535.129.03\nnvidia.com/cuda.driver-version.major=535\nnvidia.com/cuda.driver-version.minor=129\nnvidia.com/cuda.driver-version.revision=03\nnvidia.com/cuda.driver.major=535\nnvidia.com/cuda.driver.minor=129\nnvidia.com/cuda.driver.rev=03\nnvidia.com/cuda.runtime-version.full=12.2\nnvidia.com/cuda.runtime-version.major=12\nnvidia.com/cuda.runtime-version.minor=2\nnvidia.com/cuda.runtime.major=12\nnvidia.com/cuda.runtime.minor=2\nnvidia.com/gfd.timestamp=1727461913\nnvidia.com/gpu-driver-upgrade-state=upgrade-done\nnvidia.com/gpu.compute.major=7\nnvidia.com/gpu.compute.minor=0\nnvidia.com/gpu.count=8\nnvidia.com/gpu.deploy.container-toolkit=true\nnvidia.com/gpu.deploy.dcgm=true\nnvidia.com/gpu.deploy.dcgm-exporter=true\nnvidia.com/gpu.deploy.device-plugin=true\nnvidia.com/gpu.deploy.driver=pre-installed\nnvidia.com/gpu.deploy.gpu-feature-discovery=true\nnvidia.com/gpu.deploy.node-status-exporter=true\nnvidia.com/gpu.deploy.operator-validator=true\nnvidia.com/gpu.family=volta\nnvidia.com/gpu.machine=Standard-PC-Q35-ICH9-2009\nnvidia.com/gpu.memory=16384\nnvidia.com/gpu.mode=compute\nnvidia.com/gpu.present=true\nnvidia.com/gpu.product=Tesla-V100-SXM2-16GB\nnvidia.com/gpu.replicas=1\nnvidia.com/gpu.sharing-strategy=none\nnvidia.com/mig.capable=false\nnvidia.com/mig.strategy=single\nnvidia.com/mps.capable=false\nnvidia.com/vgpu.present=false\nnvidia.com/gpu-driver-upgrade-enabled: true\n</code></pre> <p><code>nvidia.com/gpu.count=8</code> indicates that your cluster detects 8 GPUs.</p> <p><code>nvidia.com/gpu.product=Tesla-V100-SXM2-16GB</code> indicates that the detected GPUs are Tesla V100 SXM2 16GB GPUs.</p> <p>Note</p> <p>In this tutorial, Ollama will only use 1 GPU.</p> </li> </ol>","tags":["api","kubernetes","llama","llm"]},{"location":"education/large-language-models/k8s-ollama-llama-3-2/#deploy-ollama-in-your-kubernetes-cluster","title":"Deploy Ollama in your Kubernetes cluster","text":"<ol> <li> <p>Start an Ollama server in your Kubernetes cluster by running:</p> <pre><code>cat &lt;&lt;EOF | k3s kubectl apply -f -\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: ollama\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ollama\n  namespace: ollama\nspec:\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      name: ollama\n  template:\n    metadata:\n      labels:\n        name: ollama\n    spec:\n      containers:\n      - name: ollama\n        image: ollama/ollama:latest\n        env:\n        - name: PATH\n          value: /usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n        - name: LD_LIBRARY_PATH\n          value: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n        - name: NVIDIA_DRIVER_CAPABILITIES\n          value: compute,utility\n        ports:\n        - name: http\n          containerPort: 11434\n          protocol: TCP\n        resources:\n          limits:\n            nvidia.com/gpu: 1\n      tolerations:\n      - key: nvidia.com/gpu\n        operator: Exists\n        effect: NoSchedule\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: ollama\n  namespace: ollama\nspec:\n  type: ClusterIP\n  selector:\n    name: ollama\n  ports:\n  - port: 80\n    name: http\n    targetPort: http\n    protocol: TCP\nEOF\n</code></pre> </li> <li> <p>In a few minutes, run the following command to verify that the Ollama server    is accepting connections and is using a GPU:</p> <pre><code>kubectl logs -n ollama -l name=ollama\n</code></pre> <p>You should see output similar to:</p> <pre><code>2024/09/27 18:51:55 routes.go:1153: INFO server config env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/root/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]\"\ntime=2024-09-27T18:51:55.719Z level=INFO source=images.go:753 msg=\"total blobs: 0\"\ntime=2024-09-27T18:51:55.719Z level=INFO source=images.go:760 msg=\"total unused blobs removed: 0\"\ntime=2024-09-27T18:51:55.719Z level=INFO source=routes.go:1200 msg=\"Listening on [::]:11434 (version 0.3.12)\"\ntime=2024-09-27T18:51:55.720Z level=INFO source=common.go:49 msg=\"Dynamic LLM libraries\" runners=\"[cpu_avx cpu_avx2 cuda_v11 cuda_v12 cpu]\"\ntime=2024-09-27T18:51:55.720Z level=INFO source=gpu.go:199 msg=\"looking for compatible GPUs\"\ntime=2024-09-27T18:51:55.942Z level=INFO source=types.go:107 msg=\"inference compute\" id=GPU-d8c505a1-8af4-7ce4-517d-4f57fa576097 library=cuda variant=v12 compute=7.0 driver=12.2 name=\"Tesla V100-SXM2-16GB\" total=\"15.8 GiB\" available=\"15.5 GiB\"\n</code></pre> <p>The last line in the example output above shows that Ollama is using a single <code>Tesla V100-SXM2-16GB</code> GPU.</p> </li> <li> <p>Start a tmux session by running:</p> <pre><code>tmux\n</code></pre> <p>Then, run the following command to make Ollama accessible from outside of your Kubernetes cluster:</p> <pre><code>k3s kubectl -n ollama port-forward service/ollama 11434:80\n</code></pre> <p>You should see output similar to:</p> <pre><code>Forwarding from 127.0.0.1:11434 -&gt; 11434\nForwarding from [::1]:11434 -&gt; 11434\n</code></pre> </li> </ol>","tags":["api","kubernetes","llama","llm"]},{"location":"education/large-language-models/k8s-ollama-llama-3-2/#install-the-ollama-client","title":"Install the Ollama client","text":"<ul> <li> <p>Press Ctrl + B, then press C to open a new tmux window.</p> <p>Download and install the Ollama client by running:</p> <pre><code>curl -L https://ollama.com/download/ollama-linux-amd64.tgz -o ollama-linux-amd64.tgz\nsudo tar -C /usr -xzf ollama-linux-amd64.tgz\n</code></pre> </li> </ul>","tags":["api","kubernetes","llama","llm"]},{"location":"education/large-language-models/k8s-ollama-llama-3-2/#serve-and-interact-with-the-llama-32-3b-model","title":"Serve and interact with the Llama 3.2 3B model","text":"<ol> <li> <p>Serve the Llama 3.2 3B model using Ollama by running:</p> <pre><code>ollama run llama3.2:3b-instruct-fp16\n</code></pre> <p>You can interact with the model once you see the following prompt:</p> <pre><code>&gt;&gt;&gt; Send a message (/? for help)\n</code></pre> </li> <li> <p>Test the model by entering a prompt, for example:</p> <pre><code>What is machine learning? Explain like I'm five.\n</code></pre> <p>You should see output similar to:</p> <pre><code>MACHINE LEARNING IS SO COOL!\n\nImagine you have a toy box filled with different toys, like blocks, dolls, and cars. Now, imagine you want to teach a robot to pick up the toys from the box and put them away in their own boxes.\n\nAt first, the robot doesn't know which toy goes where. So, you show it a few toys and say, \"Hey, this is a block! Put it in the blocks' box!\" The robot looks at the toy and says, \"Okay, I think it's a block!\"\n\nThen, you show it another toy and say, \"This one is a doll! Put it in the dolls' box!\" And so on.\n\nThe robot keeps learning and trying to figure out which toys are which. It's like playing a game of \"match me\" with all the different toys!\n\nAs the robot plays this game over and over, it gets better and better at recognizing the toys. Eventually, it can look at a new toy and say, \"Oh, I know that one! That's a block! Put it in the blocks' box!\"\n\nThat's basically what machine learning is: teaching a computer to recognize patterns and make decisions on its own, just like the robot did with the toys!\n\nBut instead of toys, computers use special math equations to learn from data (like pictures, words, or sounds). And instead of a toy box, they have big databases filled with lots of information.\n\nSo, machine learning is all about helping computers get smarter and better at doing things on their own!\n</code></pre> </li> </ol>","tags":["api","kubernetes","llama","llm"]},{"location":"education/large-language-models/kubeai-hermes-3/","title":"Using KubeAI to deploy Nous Research's Hermes 3 and other LLMs","text":"","tags":["kubernetes","llama","llm"]},{"location":"education/large-language-models/kubeai-hermes-3/#introduction","title":"Introduction","text":"<p>KubeAI: Private Open AI on Kubernetes is a Kubernetes solution for running inference on open-weight large language models (LLMs), including Nous Research's Hermes 3 fine-tuned Llama 3.1 8B model and NVIDIA's Nemotron fine-tuned Llama 3.1 70B model.</p> <p>Using model servers such as vLLM and Ollama, KubeAI enables you to interact with LLMs using both a web UI powered by Open WebUI and an OpenAI-compatible API.</p> <p>In this tutorial, you'll:</p> <ol> <li>Stand up a single-node Kubernetes cluster on an 8x H100    on-demand instance    using K3s.</li> <li>Install the    NVIDIA GPU Operator    so your Kubernetes cluster can use your instance's GPUs.</li> <li>Deploy KubeAI in your Kubernetes cluster to serve both Nous Research's Hermes    3 model and NVIDIA's Nemotron model.</li> <li>Interact with the models using KubeAI's web UI.</li> <li>Interact with the models using KubeAI's OpenAI-compatible API.</li> <li>Use NVTOP to    observe GPU utilization.</li> </ol>","tags":["kubernetes","llama","llm"]},{"location":"education/large-language-models/kubeai-hermes-3/#stand-up-a-single-node-kubernetes-cluster","title":"Stand up a single-node Kubernetes cluster","text":"<ol> <li> <p>Use the    dashboard    or Cloud API to launch    an 8x H100 instance. Then, SSH into your instance by running:</p> <pre><code>ssh ubuntu@&lt;INSTANCE-IP-ADDRESS&gt; -L 8080:localhost:8080\n</code></pre> <p>Replace <code>&lt;INSTANCE-IP-ADDRESS&gt;</code> with the IP address of your instance.</p> <p>Note</p> <p>The <code>-L 8080:localhost:8080</code> option enables local port forwarding. Local port forwarding is needed to securely access KubeAI's web UI from your computer. See the SSH man page to learn more.</p> </li> <li> <p>Install socat    (which is needed for the <code>kubectl port-forward</code> command you'll run later in    this tutorial) by running:</p> <pre><code>sudo apt update &amp;&amp; sudo apt -y install socat\n</code></pre> </li> <li> <p>Install K3s (Kubernetes) by running:</p> <pre><code>curl -sfL https://get.k3s.io | K3S_KUBECONFIG_MODE=644 sh -s - --default-runtime=nvidia\n</code></pre> </li> <li> <p>Verify that your Kubernetes cluster is ready by running:</p> <pre><code>kubectl get nodes\n</code></pre> <p>You should see output similar to:</p> <pre><code>NAME             STATUS   ROLES                  AGE   VERSION\n192-222-54-148   Ready    control-plane,master   1s    v1.30.5+k3s1\n</code></pre> </li> </ol> <p>Tip</p> <p>You can enable tab completion for <code>kubectl</code> by running:</p> <pre><code>echo \"source &lt;(kubectl completion bash)\" &gt;&gt; ~/.bashrc &amp;&amp; source ~/.bashrc\n</code></pre>","tags":["kubernetes","llama","llm"]},{"location":"education/large-language-models/kubeai-hermes-3/#install-the-nvidia-gpu-operator","title":"Install the NVIDIA GPU Operator","text":"<ol> <li> <p>Install the NVIDIA GPU Operator in your Kubernetes cluster by running:</p> <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: gpu-operator\n---\napiVersion: helm.cattle.io/v1\nkind: HelmChart\nmetadata:\n  name: gpu-operator\n  namespace: gpu-operator\nspec:\n  repo: https://helm.ngc.nvidia.com/nvidia\n  chart: gpu-operator\n  targetNamespace: gpu-operator\nEOF\n</code></pre> </li> <li> <p>In a few minutes, verify that your instance's GPUs are detected by your    cluster by running:</p> <pre><code>kubectl describe nodes | grep nvidia.com\n</code></pre> <p>You should see output similar to:</p> <pre><code>nvidia.com/cuda.driver-version.full=535.129.03\nnvidia.com/cuda.driver-version.major=535\nnvidia.com/cuda.driver-version.minor=129\nnvidia.com/cuda.driver-version.revision=03\nnvidia.com/cuda.driver.major=535\nnvidia.com/cuda.driver.minor=129\nnvidia.com/cuda.driver.rev=03\nnvidia.com/cuda.runtime-version.full=12.2\nnvidia.com/cuda.runtime-version.major=12\nnvidia.com/cuda.runtime-version.minor=2\nnvidia.com/cuda.runtime.major=12\nnvidia.com/cuda.runtime.minor=2\nnvidia.com/gfd.timestamp=1729173628\nnvidia.com/gpu-driver-upgrade-state=upgrade-done\nnvidia.com/gpu.compute.major=9\nnvidia.com/gpu.compute.minor=0\nnvidia.com/gpu.count=8\nnvidia.com/gpu.deploy.container-toolkit=true\nnvidia.com/gpu.deploy.dcgm=true\nnvidia.com/gpu.deploy.dcgm-exporter=true\nnvidia.com/gpu.deploy.device-plugin=true\nnvidia.com/gpu.deploy.driver=pre-installed\nnvidia.com/gpu.deploy.gpu-feature-discovery=true\nnvidia.com/gpu.deploy.mig-manager=true\nnvidia.com/gpu.deploy.node-status-exporter=true\nnvidia.com/gpu.deploy.operator-validator=true\nnvidia.com/gpu.family=hopper\nnvidia.com/gpu.machine=Standard-PC-Q35-ICH9-2009\nnvidia.com/gpu.memory=81559\nnvidia.com/gpu.mode=compute\nnvidia.com/gpu.present=true\nnvidia.com/gpu.product=NVIDIA-H100-80GB-HBM3\nnvidia.com/gpu.replicas=1\nnvidia.com/gpu.sharing-strategy=none\nnvidia.com/mig.capable=true\nnvidia.com/mig.config=all-disabled\nnvidia.com/mig.config.state=success\nnvidia.com/mig.strategy=single\nnvidia.com/mps.capable=false\nnvidia.com/vgpu.present=false\nnvidia.com/gpu-driver-upgrade-enabled: true\n</code></pre> <p><code>nvidia.com/gpu.count=8</code> indicates that your cluster detects 8 GPUs.</p> <p><code>nvidia.com/gpu.product=NVIDIA-H100-80GB-HBM3</code> indicates that the detected GPUs are NVIDIA-H100-80GB-HBM3 GPUs.</p> </li> </ol>","tags":["kubernetes","llama","llm"]},{"location":"education/large-language-models/kubeai-hermes-3/#install-kubeai","title":"Install KubeAI","text":"<ol> <li> <p>Install KubeAI by running:</p> <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: kubeai\n---\napiVersion: helm.cattle.io/v1\nkind: HelmChart\nmetadata:\n  name: kubeai\n  namespace: kubeai\nspec:\n  repo: https://www.kubeai.org\n  chart: kubeai\n  targetNamespace: kubeai\nEOF\n</code></pre> <p>Ths installation usually takes 2 to 3 minutes to finish.</p> </li> <li> <p>To know when KubeAI is installed and ready, run:</p> <pre><code>kubectl get -n kubeai -w --field-selector=status.phase=Running pods\n</code></pre> <p>This command watches for and displays pods running in the <code>kubeai</code> namespace.</p> <p>KubeAI is installed and ready once you see output similar to:</p> <pre><code>kubeai-5f6cb9984b-nghpj   1/1     Running   0          10s\n</code></pre> <p>Note <code>1/1</code> which indicates 1 pod out of the 1 requested is ready.</p> </li> <li> <p>To stop watching, press Ctrl + C.</p> </li> </ol>","tags":["kubernetes","llama","llm"]},{"location":"education/large-language-models/kubeai-hermes-3/#download-and-serve-the-hermes-3-and-nemotron-models","title":"Download and serve the Hermes 3 and Nemotron models","text":"<ol> <li> <p>Download and serve both Nous Research's Hermes 3 model and NVIDIA's Nemotron    model using vLLM by running:</p> <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: kubeai.org/v1\nkind: Model\nmetadata:\n  name: hermes-3-llama-3.1-8b\n  namespace: kubeai\nspec:\n  features: [TextGeneration]\n  owner: NousResearch\n  url: hf://NousResearch/Hermes-3-Llama-3.1-8B\n  engine: VLLM\n  resourceProfile: nvidia-gpu-h100:1\n  minReplicas: 1\n---\napiVersion: kubeai.org/v1\nkind: Model\nmetadata:\n  name: llama-3.1-nemotron-70b-instruct\n  namespace: kubeai\nspec:\n  features: [TextGeneration]\n  owner: nvidia\n  url: hf://nvidia/Llama-3.1-Nemotron-70B-Instruct-HF\n  engine: VLLM\n  resourceProfile: nvidia-gpu-h100:4\n  minReplicas: 1\n  args:\n    - --tensor-parallel-size=4\nEOF\n</code></pre> <p>Note</p> <p>The Hermes 3 model can be loaded onto a single GPU. Accordingly, <code>spec.resourceProfile</code> is set to <code>nvidia-gpu-h100:1</code> to allocate a single GPU to the Hermes 3 model.</p> <p>On the other hand, the Nemotron model can't be loaded onto a single GPU. Accordingly, <code>spec.resourceProfile</code> is set to <code>nvidia-gpu-h100:4</code> to allocate 4 GPUs to the Nemotron model. Also, the argument <code>--tensor-parallel-size=4</code> is passed to vLLM to divide the Nemotron model among the 4 allocated GPUs using a tensor parallelism strategy.</p> <p>See vLLM's documentation on distributed inference and serving to learn more.</p> </li> <li> <p>To know when the models are downloaded and being served, run:</p> <pre><code>kubectl get -n kubeai -w --field-selector=status.phase=Running pods | grep model-\n</code></pre> </li> <li> <p>The models are downloaded and being served once you see output similar to.</p> <pre><code>model-hermes-3-llama-3.1-8b-79cdb64947-cb7cd             1/1     Running   0          2m21s\nmodel-llama-3.1-nemotron-70b-instruct-57cf757d9d-nc9l4   1/1     Running   0          5m5s\n</code></pre> <p>Note <code>1/1</code> for each model, which indicates 1 pod out of the 1 pod requested for each model is running.</p> </li> <li> <p>To stop watching, press Ctrl + C.</p> </li> </ol>","tags":["kubernetes","llama","llm"]},{"location":"education/large-language-models/kubeai-hermes-3/#interact-with-the-models-using-kubeais-web-ui","title":"Interact with the models using KubeAI's web UI","text":"<ol> <li> <p>Run the following command to make the KubeAI web UI accessible from your    computer:</p> <pre><code>kubectl -n kubeai port-forward service/openwebui 8080:80 &amp;&gt; /dev/null &amp;\n</code></pre> </li> <li> <p>In your web browser, go to http://localhost:8080. Then, at the top-left of    the page, click Select a model and choose either:</p> <ul> <li>hermes-3-llama-3.1-8b for the Hermes 3 model.</li> <li>llama-3.1-nemotron-70b-instruct for the Nemotron model.</li> </ul> </li> <li> <p>At the bottom of the page, in the Send a Message field, enter a prompt to    test the chosen model, for example:</p> <pre><code>In a small table, compare and contrast machine learning and deep learning.\n</code></pre> <p>You should see output similar to:</p> <p></p> </li> </ol>","tags":["kubernetes","llama","llm"]},{"location":"education/large-language-models/kubeai-hermes-3/#interact-with-the-models-using-kubeais-openai-compatible-api","title":"Interact with the models using KubeAI's OpenAI-compatible API","text":"<ol> <li> <p>On your instance, install <code>curl</code> and <code>jq</code> by running:</p> <pre><code>sudo apt update &amp;&amp; sudo apt -y install curl jq\n</code></pre> </li> <li> <p>Enable access to KubeAI's OpenAI-compatible API by running:</p> <pre><code>kubectl -n kubeai port-forward service/kubeai 8081:80 &amp;&gt; /dev/null &amp;\n</code></pre> </li> <li> <p>List the models KubeAI is serving using the API's <code>/models</code> endpoint by    running:</p> <pre><code>curl -sS http://localhost:8081/openai/v1/models | jq .\n</code></pre> <p>You should see output similar to:</p> <pre><code>{\n  \"object\": \"list\",\n  \"data\": [\n    {\n      \"id\": \"llama-3.1-nemotron-70b-instruct\",\n      \"created\": 1729174478,\n      \"object\": \"model\",\n      \"owned_by\": \"nvidia\",\n      \"features\": [\n        \"TextGeneration\"\n      ]\n    },\n    {\n      \"id\": \"hermes-3-llama-3.1-8b\",\n      \"created\": 1729174478,\n      \"object\": \"model\",\n      \"owned_by\": \"NousResearch\",\n      \"features\": [\n        \"TextGeneration\"\n      ]\n    }\n  ]\n}\n</code></pre> </li> <li> <p>Submit an example prompt to the API by running:</p> <pre><code>curl -sS -d @- http://localhost:8081/openai/v1/completions -H \"Content-Type: application/json\" &lt;&lt;EOF | jq .\n{\n  \"model\": \"llama-3.1-nemotron-70b-instruct\",\n  \"prompt\": \"Machine learning engineers are \",\n  \"temperature\": 0\n}\nEOF\n</code></pre> <p>In the above example, the Nemotron model is responding to the prompt <code>Machine learning engineers are</code>.</p> <p>You should see output similar to:</p> <pre><code>{\n  \"id\": \"cmpl-04a8ee4abf7247b5859755e7684ade21\",\n  \"object\": \"text_completion\",\n  \"created\": 1729179938,\n  \"model\": \"llama-3.1-nemotron-70b-instruct\",\n  \"choices\": [\n    {\n      \"index\": 0,\n      \"text\": \"2023\u2019s most in-demand tech professionals\\nAccording to a new report from Indeed\",\n      \"logprobs\": null,\n      \"finish_reason\": \"length\",\n      \"stop_reason\": null,\n      \"prompt_logprobs\": null\n    }\n  ],\n  \"usage\": {\n    \"prompt_tokens\": 6,\n    \"total_tokens\": 22,\n    \"completion_tokens\": 16\n  }\n}\n</code></pre> </li> </ol> <p>Tip</p> <p>See vLLM's documentation to learn more about using the API.</p>","tags":["kubernetes","llama","llm"]},{"location":"education/large-language-models/kubeai-hermes-3/#use-nvtop-to-observe-gpu-utilization","title":"Use NVTOP to observe GPU utilization","text":"<ol> <li> <p>On your instance, run:</p> <pre><code>nvtop -d 2\n</code></pre> <p>You should see output similar to:</p> <p></p> <p>The above example output shows the models are loaded on:</p> <ul> <li><code>GPU1</code></li> <li><code>GPU4</code></li> <li><code>GPU5</code></li> <li><code>GPU6</code></li> <li><code>GPU7</code></li> </ul> <p>Note the memory usage.</p> <p>The output further shows the following GPUs are unallocated and can be used for other work:</p> <ul> <li><code>GPU0</code></li> <li><code>GPU2</code></li> <li><code>GPU3</code></li> </ul> <p>Again, note the memory usage or, in this case, the lack of memory usage.</p> </li> <li> <p>In your browser, go again to    http://localhost:8080. Select the    <code>llama-3.1-nemotron-70b-instruct</code> model.</p> </li> <li> <p>Enter an example prompt, such as:</p> <pre><code>In 10,000 words or more, explain what large language models are.\n</code></pre> <p>In <code>nvtop</code>, you should see output that momentarily looks like:</p> <p></p> <p>The output shows that <code>GPU4</code> through <code>GPU7</code> are utilized for running inference on the Nematron model.</p> </li> <li> <p>In your browser, select the <code>hermes-3-llama-3.1-8b</code> model and enter the same    prompt. You should see output that momentarily looks like:</p> <p></p> <p>The output shows that <code>GPU1</code> is utilized for running inference on the Hermes 3 model.</p> </li> </ol>","tags":["kubernetes","llama","llm"]},{"location":"education/large-language-models/serving-llama-3-1-405b/","title":"Serving Llama 3.1 405B on a Lambda 1-Click Cluster","text":"<p>In this tutorial, you'll learn how to use a 1-Click Cluster (1CC) to serve the Meta Llama 3.1 405B model using vLLM with pipeline parallelism.</p>"},{"location":"education/large-language-models/serving-llama-3-1-405b/#prerequisites","title":"Prerequisites","text":"<p>For this tutorial, you need: </p> <ul> <li>A Lambda Cloud account.</li> <li>A 1-Click Cluster. </li> <li>A Hugging Face account to download the Llama 3.1 405B model. </li> <li>A User Access Token with the Read role.</li> <li>Before you can download the Llama 3.1 405B model, you need to review and accept the model's license agreement. Once you accept the agreement, a request to access the repository will be submitted for approval; approval tends to be fast. You can see the status of the request in your Hugging Face account settings.</li> </ul>"},{"location":"education/large-language-models/serving-llama-3-1-405b/#download-the-llama-31-405b-model-and-set-up-a-head-node","title":"Download the Llama 3.1 405B model and set up a head node","text":"<p>First, follow the instructions for accessing your 1CC, which includes steps to set up SSH.</p> <p>Then SSH into one of your 1CC GPU nodes. You can find the node names in your 1-Click Clusters dashboard. You\u2019ll use this GPU node as a head node for cluster management. </p> <p>On the head node, set environment variables needed for this tutorial by running:</p> <pre><code>export HEAD_IP=HEAD-IP\nexport SHARED_DIR=/home/ubuntu/FILE-SYSTEM-NAME\nexport HF_TOKEN=HF-TOKEN\nexport HF_HOME=\"${SHARED_DIR}/.cache/huggingface\"\nexport MODEL_REPO=meta-llama/Meta-Llama-3.1-405B-Instruct\n</code></pre> <p>Replace <code>HEAD-IP</code> with the IP address of the head node. You can obtain the IP address from the 1-Click Clusters dashboard.</p> <p>Replace <code>FILE-SYSTEM-NAME</code> with the name of your 1CC's persistent storage file system.</p> <p>Replace <code>HF-TOKEN</code> with your Hugging Face User Access Token.</p> <p>Then, run:</p> <pre><code>mkdir -p \"${HF_HOME}\"\npython3 -m venv llama-3.1\nsource llama-3.1/bin/activate\npip install -U huggingface_hub[cli]\nhuggingface-cli login --token \"${HF_TOKEN}\"\nhuggingface-cli download \"${MODEL_REPO}\"\n</code></pre> <p>These commands:</p> <ol> <li>Create and activate a Python virtual environment on the head node for this tutorial.</li> <li>Download the Llama 3.1 405B model to your 1CC's persistent storage file system.</li> </ol> <p>Note</p> <p>The Llama 3.1 405B model is about 2.3TB in size and can take several hours to download.</p> <p>Still on the head node, start a tmux session by running <code>tmux</code>.</p> <p>Then, run:</p> <pre><code>curl -o \"${SHARED_DIR}/run_cluster.sh\" https://raw.githubusercontent.com/vllm-project/vllm/main/examples/run_cluster.sh\nsudo bash \"${SHARED_DIR}/run_cluster.sh\" \\\n    vllm/vllm-openai \\\n    \"${HEAD_IP}\" \\\n    --head \"${HF_HOME}\" \\\n    --privileged -e NCCL_IB_HCA=^mlx5_0\n</code></pre> <p>These commands:</p> <ol> <li>Download a helper script to your shared persistent storage file system to set up vLLM for multi-node inference and serving.</li> <li>Run the script to start a Ray cluster for serving the Llama 3.1 405B model using vLLM. The Ray cluster uses your 1CC's InfiniBand fabric for optimal performance.</li> </ol>"},{"location":"education/large-language-models/serving-llama-3-1-405b/#connect-another-gpu-node-to-the-head-node","title":"Connect another GPU node to the head node","text":"<p>Next, you'll connect another of your 1CC's GPUs nodes to the head node. This other GPU node will be referred to below as the worker node. </p> <p>In a new terminal, SSH into the worker node, then set environment variables needed for this tutorial by running:</p> <pre><code>export HEAD_IP=HEAD-IP\nexport SHARED_DIR=/home/ubuntu/FILE-SYSTEM-NAME\nexport HF_HOME=\"${SHARED_DIR}/.cache/huggingface\"\n</code></pre> <p>Replace <code>HEAD-IP</code> with the IP address of the head node.</p> <p>Replace <code>FILE-SYSTEM-NAME</code> with the name of your 1CC's persistent storage file system.</p> <p>Run <code>tmux</code> to start a new tmux session. Then, run:</p> <pre><code>sudo bash \"${SHARED_DIR}/run_cluster.sh\" \\\n    vllm/vllm-openai \\\n    \"${HEAD_IP}\" \\\n    --worker \\\n    \"${HF_HOME}\" \\\n    --privileged -e NCCL_IB_HCA=^mlx5_0\n</code></pre> <p>This command connects the worker node to the head node.</p>"},{"location":"education/large-language-models/serving-llama-3-1-405b/#check-the-status-of-the-ray-cluster","title":"Check the status of the Ray cluster","text":"<p>On the worker node, open a new tmux window (press Ctrl + B, then press C). Then, run:</p> <pre><code>sudo docker exec -it node /bin/bash\n</code></pre> <p>Check the status of the Ray cluster by running:</p> <pre><code>ray status\n</code></pre> <p>This command produces output similar to the following:</p> <pre><code>======== Autoscaler status: 2024-07-25 00:20:50.831620 ========\nNode status\n---------------------------------------------------------------\nActive:\n 1 node_d86d9f0f1894c2e463d8168530f6745e32beb08ddf3b908d229d8527\n 1 node_37af7f860c4bab2e035b5a55ba06e2e49dba9fa891d65f8264648804\nPending:\n (no pending nodes)\nRecent failures:\n (no failures)\n\nResources\n---------------------------------------------------------------\nUsage:\n 0.0/416.0 CPU\n 16.0/16.0 GPU (16.0 used of 16.0 reserved in placement groups)\n 0B/3.43TiB memory\n 0B/19.46GiB object_store_memory\n\nDemands:\n (no resource demands)\n</code></pre> <p>This output shows two active nodes (the head node and the worker node) and 16 GPUs in the Ray cluster.</p>"},{"location":"education/large-language-models/serving-llama-3-1-405b/#serve-the-llama-31-405b-model","title":"Serve the Llama 3.1 405B model","text":"<p>After verifying the server status, you can start to serve the model. </p> <pre><code>ls /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-405B-Instruct/snapshots\n</code></pre> <p>This command returns a snapshot with a name similar to: </p> <pre><code>e04e3022cdc89bfed0db69f5ac1d249e21ee2d30\n</code></pre> <p>Now, run the following command to begin serving the Llama 3.1 405B model:</p> <pre><code>vllm serve \"/root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-405B-Instruct/snapshots/SNAPSHOT\" --tensor-parallel-size 8 --pipeline-parallel-size 2\n</code></pre> <p>Replace <code>SNAPSHOT</code> with the name of the Llama 3.1 405B model snapshot you retrieved earlier. </p> <p>Note</p> <p>It can take 15 minutes or more before the model is loaded onto the GPUs and ready to be served.</p> <p>You should begin seeing output similar to:</p> <pre><code>INFO 07-25 04:17:41 api_server.py:219] vLLM API server version 0.5.3.post1\nINFO 07-25 04:17:41 api_server.py:220] args: Namespace(model_tag='/root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-405B-Instruct/snapshots/e04e3022cdc89bfed0db69f5ac1d249e21ee2d30', host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='/root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-405B-Instruct/snapshots/e04e3022cdc89bfed0db69f5ac1d249e21ee2d30', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=None, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=2, tensor_parallel_size=8, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=False, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, num_speculative_tokens=None, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, engine_use_ray=False, disable_log_requests=False, max_log_len=None, dispatch_function=&lt;function serve at 0x7de812d13520&gt;)\nINFO 07-25 04:17:41 config.py:715] Defaulting to use ray for distributed inference\n</code></pre> <p>The Llama 3.1 405B model is ready to be served once you see output similar to:</p> <pre><code>INFO:     Started server process [24469]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n</code></pre>"},{"location":"education/large-language-models/serving-llama-3-1-405b/#test-the-llama-31-405b-model","title":"Test the Llama 3.1 405B model","text":"<p>Still on the worker node, open a new tmux window (Ctrl + B, then press C).</p> <p>Then, run:</p> <pre><code>python3 -m venv llama-3.1\nsource llama-3.1/bin/activate\npip install -U openai\ncurl -o ${SHARED_DIR}/inference_test.py 'https://raw.githubusercontent.com/vllm-project/vllm/main/examples/openai_chat_completion_client.py'\n</code></pre> <p>These commands:</p> <ol> <li>Create and activate a Python virtual environment on the worker node.</li> <li>Download vLLM\u2019s example Open AI chat completion client.</li> </ol> <p>Finally, to test the Llama 3.1 405B model, run:</p> <pre><code>python3 ${SHARED_DIR}/inference_test.py\n</code></pre> <p>This command produces output similar to:</p> <pre><code>Chat completion results:\nChatCompletion(id='chat-8eba7fa7e2f7442aafa82a1683bfc77f', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The 2020 World Series was played at Globe Life Field in Arlington, Texas. This was a neutral site due to COVID-19 restrictions and was also referred to as a \"bubble\" environment.', role='assistant', function_call=None, tool_calls=[]), stop_reason=None)], created=1721884178, model='/root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-405B-Instruct/snapshots/e04e3022cdc89bfed0db69f5ac1d249e21ee2d30', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=41, prompt_tokens=59, total_tokens=100))\n</code></pre>"},{"location":"education/large-language-models/serving-llama-3-1-405b/#acknowledgement","title":"Acknowledgement","text":"<p>We'd like to thank the vLLM team for their partnership in developing this guide and their pioneering work in streamlining LLM serving.</p>"},{"location":"education/large-language-models/serving-llama-3-1-docker/","title":"Serving the Llama 3.1 8B and 70B models using Lambda Cloud on-demand instances","text":"<p>This tutorial shows you how to use a Lambda Cloud 1x or 8x A100 or H100 on-demand instance to serve the Llama 3.1 8B and 70B models. You'll serve the model using vLLM running inside of a Docker container.</p>","tags":["docker","llama","llm"]},{"location":"education/large-language-models/serving-llama-3-1-docker/#start-the-vllm-api-server","title":"Start the vLLM API server","text":"<p>If you haven't already, use the dashboard or Cloud API to launch an instance. Then, SSH into your instance.</p> <p>Run:</p> <pre><code>export HF_TOKEN=HF-TOKEN HF_HOME=\"/home/ubuntu/.cache/huggingface\" MODEL_REPO=meta-llama/MODEL\n</code></pre> <p>Replace HF-TOKEN with your Hugging Face User Access Token.</p> <p>Replace MODEL with:</p> <ul> <li>If you're serving the 8B model:   <pre><code>Meta-Llama-3.1-8B-Instruct\n</code></pre></li> <li>If you're serving the 70B model:   <pre><code>Meta-Llama-3.1-70B-Instruct\n</code></pre></li> </ul> <p>These commands set environment variables needed for this tutorial.</p> <p>Note</p> <p>The 70B model requires an 8x A100 or H100.</p> <p>Start a tmux session by running <code>tmux</code>.</p> <p>If you're serving the 8B model, run:</p> <pre><code>sudo docker run \\\n  --gpus all \\\n  --ipc=host \\\n  -v \"${HF_HOME}\":/root/.cache/huggingface \\\n  -p 8000:8000 \\\n  --env \"HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}\" \\\n  vllm/vllm-openai --model \"${MODEL_REPO}\" \\\n    --disable-log-requests\n</code></pre> <p>If you're serving the 70B model using an 8x A100 or H100 instance (required), instead run:</p> <pre><code>sudo docker run \\\n  --gpus all \\\n  --ipc=host \\\n  -v \"${HF_HOME}\":/root/.cache/huggingface \\\n  -p 8000:8000 \\\n  --env \"HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}\" \\\n  vllm/vllm-openai --model \"${MODEL_REPO}\" \\\n    --disable-log-requests \\\n    --tensor-parallel-size 8\n</code></pre> <p>Both commands, above:</p> <ul> <li>Download the model you're serving.</li> <li>Start vLLM's API server with the chosen model.</li> </ul> <p>Note</p> <p>The difference betweeen the two commands, above, is that the second command enables the tensor parallel strategy to use 8x GPUs. See vLLM's docs to learn more about the distributed inference strategies.</p> <p>The vLLM API server is running once you see:</p> <pre><code>INFO 08-01 19:11:07 api_server.py:292] Available routes are:\nINFO 08-01 19:11:07 api_server.py:297] Route: /openapi.json, Methods: GET, HEAD\nINFO 08-01 19:11:07 api_server.py:297] Route: /docs, Methods: GET, HEAD\nINFO 08-01 19:11:07 api_server.py:297] Route: /docs/oauth2-redirect, Methods: GET, HEAD\nINFO 08-01 19:11:07 api_server.py:297] Route: /redoc, Methods: GET, HEAD\nINFO 08-01 19:11:07 api_server.py:297] Route: /health, Methods: GET\nINFO 08-01 19:11:07 api_server.py:297] Route: /tokenize, Methods: POST\nINFO 08-01 19:11:07 api_server.py:297] Route: /detokenize, Methods: POST\nINFO 08-01 19:11:07 api_server.py:297] Route: /v1/models, Methods: GET\nINFO 08-01 19:11:07 api_server.py:297] Route: /version, Methods: GET\nINFO 08-01 19:11:07 api_server.py:297] Route: /v1/chat/completions, Methods: POST\nINFO 08-01 19:11:07 api_server.py:297] Route: /v1/completions, Methods: POST\nINFO 08-01 19:11:07 api_server.py:297] Route: /v1/embeddings, Methods: POST\nINFO:     Started server process [1]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n</code></pre>","tags":["docker","llama","llm"]},{"location":"education/large-language-models/serving-llama-3-1-docker/#test-the-vllm-api-server","title":"Test the vLLM API server","text":"<p>To test that the API server is serving the Llama 3.1 model:</p> <p>Press Ctrl + B, then press C to open a new tmux window.</p> <p>Then, run:</p> <pre><code>curl -X POST http://localhost:8000/v1/completions \\\n     -H \"Content-Type: application/json\" \\\n     -d \"{\n           \\\"prompt\\\": \\\"What is the name of the capital of France?\\\",\n           \\\"model\\\": \\\"${MODEL_REPO}\\\",\n           \\\"temperature\\\": 0.0,\n           \\\"max_tokens\\\": 1\n         }\"\n</code></pre> <p>You should see output similar to:</p> <pre><code>{\"id\":\"cmpl-d3a33498b5d74d9ea09a7c256733b8df\",\"object\":\"text_completion\",\"created\":1722545598,\"model\":\"meta-llama/Meta-Llama-3.1-70B-Instruct\",\"choices\":[{\"index\":0,\"text\":\" Paris\",\"logprobs\":null,\"finish_reason\":\"length\",\"stop_reason\":null}],\"usage\":{\"prompt_tokens\":11,\"total_tokens\":12,\"completion_tokens\":1}}\n</code></pre> <p>Tip</p> <p>You can make the output more human-readable using jq. To do this, first install jq by running:</p> <pre><code>sudo apt update &amp;&amp; sudo apt install -y jq\n</code></pre> <p>Then, append <code>| jq .</code> to the <code>curl</code> command, above.</p> <p>The complete command should be:</p> <pre><code>curl -X POST http://localhost:8000/v1/completions \\\n     -H \"Content-Type: application/json\" \\\n     -d \"{\n           \\\"prompt\\\": \\\"What is the name of the capital of France?\\\",\n           \\\"model\\\": \\\"${MODEL_REPO}\\\",\n           \\\"temperature\\\": 0.0,\n           \\\"max_tokens\\\": 1\n         }\" | jq .\n</code></pre> <p>The output should now look similar to:</p> <pre><code>{\n  \"id\": \"cmpl-529d01c83069409fa5c166e1d137e21e\",\n  \"object\": \"text_completion\",\n  \"created\": 1722545913,\n  \"model\": \"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n  \"choices\": [\n    {\n      \"index\": 0,\n      \"text\": \" Paris\",\n      \"logprobs\": null,\n      \"finish_reason\": \"length\",\n      \"stop_reason\": null\n    }\n  ],\n  \"usage\": {\n    \"prompt_tokens\": 11,\n    \"total_tokens\": 12,\n    \"completion_tokens\": 1\n  }\n}\n</code></pre>","tags":["docker","llama","llm"]},{"location":"education/linux-usage/basic-linux-commands-and-system-administration/","title":"Basic Linux commands and system administration","text":""},{"location":"education/linux-usage/basic-linux-commands-and-system-administration/#importing-ssh-keys-from-github-accounts","title":"Importing SSH keys from GitHub accounts","text":"<p>To import an SSH key from a GitHub account and add it to your server (or Lambda GPU Cloud on-demand instance):</p> <ol> <li> <p>Using your existing SSH key, SSH into your server.</p> <p>Alternatively, if you're using an on-demand instance, open a terminal in Jupyter Notebook.</p> </li> <li> <p>Import the SSH key from the GitHub account by running:</p> <pre><code>ssh-import-id gh:USERNAME\n</code></pre> <p>Replace <code>USERNAME</code> with the GitHub account's username.</p> </li> </ol> <p>If the SSH key is successfully imported, <code>ssh-import-id</code> will output a message similar to:</p> <pre><code>2023-08-04 15:03:52,622 INFO Authorized key ['256', 'SHA256:C6pl0q4evVYZWcyByVF69D6fdbdKa7F8ei8V2F/bTW0', 'cbrownstein-lambda@github/67649580', '(ED25519)']\n2023-08-04 15:03:52,623 INFO [1] SSH keys [Authorized]\n</code></pre> <p>If the SSH key isn't successfully imported, <code>ssh-import-id</code> will output a message similar to:</p> <pre><code>2023-08-04 15:06:36,425 ERROR Username \"fake-cbrownstein-lambda\" not found at GitHub API. status_code=404 user=fake-cbrownstein-lambda\n</code></pre>"},{"location":"education/linux-usage/basic-linux-commands-and-system-administration/#using-rsync-to-copy-and-synchronize-files","title":"Using rsync to copy and synchronize files","text":"<p><code>rsync</code> is a tool that you can use to copy files between your computer and a remote server.</p> <p><code>rsync</code> can also be used to copy files directly between remote servers, bypassing your computer entirely.</p> <p>Tip</p> <p><code>rsync</code> is useful for copying files between Cloud persistent storage file systems in different regions.</p> <p>Note</p> <p><code>rsync</code> copies files using SSH. For this reason, to copy files between your computer and a remote server, you need to be able to SSH into the remote server.</p> <p>To use <code>rsync</code> to copy files between remote servers directly, you need to be able to SSH into the remote servers using public key authentication with an SSH agent.</p>"},{"location":"education/linux-usage/basic-linux-commands-and-system-administration/#copy-files-between-your-computer-and-a-remote-server","title":"Copy files between your computer and a remote server","text":"<p>To copy files from your computer to a remote server using<code>rsync</code>, run:</p> <pre><code>rsync -av --info=progress2 FILES USERNAME@SERVER-IP:REMOTE-PATH\n</code></pre> <p>Replace <code>FILES</code> with the files you want to copy to the remote server. Alternatively, you can specify a directory.</p> <p>Replace <code>USERNAME</code> with your username on the remote server.</p> <p>Replace <code>SERVER-IP</code> with the IP address of the remote server.</p> <p>Replace <code>REMOTE-PATH</code> with the directory into which you want to copy files.</p> <p>In the below example, <code>rsync</code> was used to copy the local directory <code>rsync_example_dir</code>, containing a single empty file named <code>EXAMPLE_FILE</code>, into the home directory of the user <code>ubuntu</code> on a remote server with the IP address <code>146.235.208.193</code>.</p> <pre><code>$ rsync -a --progress rsync_example_dir ubuntu@146.235.208.193:~\nsending incremental file list\nrsync_example_dir/\nrsync_example_dir/EXAMPLE_FILE\n              0 100%    0.00kB/s    0:00:00 (xfr#1, to-chk=0/2)\n</code></pre>"},{"location":"education/linux-usage/basic-linux-commands-and-system-administration/#copy-files-directly-between-remote-servers","title":"Copy files directly between remote servers","text":"<p>Note</p> <p>To copy files directly between remote servers using <code>rsync</code>, you must use public key (rather than password) authentication for SSH with an SSH agent.</p> <p>You can add your private key to the SSH agent by running:</p> <pre><code>ssh-add SSH-PRIVATE-KEY\n</code></pre> <p>Replace <code>SSH-PRIVATE-KEY</code> with the path to your SSH private key, for example, <code>~/.ssh/id_ed25519</code>.</p> <p>You can confirm your key was added to the SSH agent by running:</p> <pre><code>ssh-add -L\n</code></pre> <p>Your public key will be listed in the output.</p> <p>To copy files directly between remote servers using <code>rsync</code>, first SSH into the server you want to copy files from by running:</p> <pre><code>ssh -A USERNAME-1@SERVER-IP-1\n</code></pre> <p>Replace <code>SERVER-IP-1</code> with the IP address of the server you want to copy files from, referred to below as Server 1.</p> <p>Replace <code>USERNAME-1</code> with your username on Server 1.</p> <p>Tip</p> <p>It's recommended to run the <code>rsync</code> command, below, in a <code>tmux</code> or <code>screen</code> session. This way, you can log out of Server 1 and the <code>rsync</code> command will continue to run.</p> <p>Then, on Server 1, run:</p> <pre><code>rsync -av --info=progress2 FILES USERNAME-2@SERVER-IP-2:REMOTE-PATH\n</code></pre> <p>Replace <code>SERVER-IP-2</code> with the IP address of the server you want to copy files to, referred to below as Server 2.</p> <p>Replace <code>FILES</code> with the files (or directory) you want to copy to Server 2.</p> <p>Replace <code>USERNAME-2</code> with your username on Server 2.</p> <p>Replace <code>SERVER-IP-2</code> with the IP address of Server 2.</p> <p>Replace <code>REMOTE-PATH</code> with the directory into which you want to copy files.</p>"},{"location":"education/linux-usage/basic-linux-commands-and-system-administration/#preventing-system-from-suspending-or-sleeping","title":"Preventing system from suspending or sleeping","text":"<p>To prevent your system from going to sleep or suspending, run:</p> <pre><code>sudo systemctl mask hibernate.target hybrid-sleep.target \\\nsuspend-then-hibernate.target sleep.target suspend.target\n</code></pre>"},{"location":"education/linux-usage/basic-linux-commands-and-system-administration/#creating-additional-user-accounts-in-ubuntu-desktop","title":"Creating additional user accounts in Ubuntu Desktop","text":"<p>By having their own accounts, users can manage their own files, datasets, and programs, as well as manage their own [Python virtual wconda virtual environments, and Docker containers.</p> <p>Also, by having additional accounts, you can assign system administrator privileges to other users.</p> <p>You can add user accounts from the Users panel in GNOME Settings:</p> <ol> <li> <p>Press the Super key on your keyboard to open the Activities overview. Then, type <code>users</code>.</p> <p>Tip</p> <p>The Super key on your keyboard is located between the Ctrl and Alt keys.</p> </li> <li> <p>Click Users to open the Users panel in GNOME Settings.</p> </li> <li>Click Unlock at the top of the panel, then click Add User.</li> <li>For Account Type, choose either Standard or Administrator.    - Standard account users can create, modify, and delete only their own      files, not system files or other users' files. Standard account users also      can change their own settings only, not system settings or other users\u2019      settings.    - Administrator account users have the same privileges as standard      account users. However, administrator account users can also create,      modify, and delete system files and other users' files. Administrator      account users can also change their system settings and other users'      settings.</li> <li>For Full Name, enter the user's full name, that is, their \"real\" name or    name they use to identify themselves.</li> <li>For Username, enter the name the user will use to log into the system.    This name will also be the name of the user's home directory, for example,    <code>/home/username</code>.</li> <li>Under Password, choose either Allow user to set a password when they     next login, or Set a password now. If you choose to set a password     now, in the Password field, enter a custom password, or click the     Settings button to automatically generate a password.</li> <li>Click Add at the top of the dialog to add the user.</li> </ol>"},{"location":"education/linux-usage/basic-linux-commands-and-system-administration/#creating-encrypted-data-drives","title":"Creating encrypted data drives","text":"<p>Warning</p> <p>These instructions erase any existing data on the drive you're encrypting!</p> <p>Before proceeding with these instructions, back up all data that you want to keep.</p> <p>Make sure you correctly choose the drive you want to encrypt.</p> <p>To create an encrypted data drive that automatically mounts when you boot your system:</p> <ol> <li> <p>Identify the drive you want to encrypt by running:</p> <pre><code>lsblk -e 7 -o NAME,VENDOR,MODEL,SIZE,TYPE,MOUNTPOINTS\n</code></pre> <p>The output will be similar to:</p> <pre><code>NAME                      VENDOR   MODEL         SIZE TYPE  MOUNTPOINTS\nvda                       0x1af4                  25G disk\n\u251c\u2500vda1                                             1M part\n\u251c\u2500vda2                                             2G part /boot\n\u2514\u2500vda3                                            23G part\n\u2514\u2500ubuntu--vg-ubuntu--lv                       11.5G lvm  /\nvdb                       0x1af4                   1G disk\n</code></pre> <p>The above example output shows 2 drives: <code>vda</code> and <code>vdb</code>.</p> <p>Warning</p> <p>Be 100% sure you're identifying the correct drive! Look especially at the mountpoints to make sure they're not system mounts such as <code>/</code>, <code>/home</code>, and <code>/var</code>.</p> <p>Any existing data on the drive is unrecoverable once the drive is encrypted!</p> </li> <li> <p>Partition the drive you want to encrypt by running:</p> <pre><code>sudo parted -s /dev/DRIVE mklabel gpt mkpart PARTITION-TO-ENCRYPT 0% 100%\n</code></pre> <p>Replace <code>DRIVE</code> with the drive you want to encrypt.</p> <p>Replace <code>PARTITION-TO-ENCRYPT</code> with the label (name) you want to assign to the partition you're creating.</p> <p>The above command creates a single partition that uses the entire capacity of the drive.</p> <p>Obtain the name of the partition by running:</p> <pre><code>lsblk /dev/DRIVE\n</code></pre> <p>Replace <code>DRIVE</code> with the drive you're encrypting.</p> <p>You'll see output similar to:</p> <pre><code>NAME   MAJ:MIN RM  SIZE RO TYPE MOUNTPOINTS\nvdb    252:16   0    1G  0 disk\n\u2514\u2500vdb1 252:17   0 1022M  0 part\n</code></pre> <p>In the above example output, the newly created partition is <code>vdb1</code>.</p> </li> <li> <p>Install cryptsetup by running:</p> <pre><code>sudo apt update &amp;&amp; sudo apt -y install cryptsetup\n</code></pre> <p>Then, encrypt the partition you created in the previous step by running:</p> <pre><code>sudo cryptsetup --verbose --verify-passphrase luksFormat /dev/PARTITION\n</code></pre> <p>Replace <code>PARTITION</code> with the name of the partition you created in the previous step.</p> <p>You'll be prompted with:</p> <pre><code>WARNING!\n========\nThis will overwrite data on /dev/vdb1 irrevocably.\n\nAre you sure? (Type 'yes' in capital letters):\n</code></pre> <p>At the prompt, follow the instruction to confirm that you want to proceed.</p> <p>You'll be asked to enter a passphrase, then you'll be asked to verify your passphrase.</p> <p>Once encryption of the partition has finished, you'll see <code>Command successful.</code></p> <p>Warning</p> <p>Make sure not to lose your passphrase! Your passphrase can't be recovered if it's lost and, unless you also create a keyfile (optional), it's impossible to decrypt your data without your passphrase.</p> <p>Tip</p> <p>In addition to having a passphrase to decrypt your data, you can create a keyfile to automatically decrypt your data when you boot your system.</p> <p>To create a keyfile:</p> <ol> <li> <p>Run <code>sudo dd if=/dev/urandom of=PATH-TO-KEYFILE bs=1024 count=4</code>.</p> <p>Replace <code>PATH-TO-KEYFILE</code> with the path to the keyfile you're creating.</p> <p>For security, it's recommended to create the keyfile in your <code>/root</code> directory, for example, <code>/root/keyfile</code>. Also, restrict permissions to the keyfile by running <code>sudo chmod 600 /root/keyfile</code>.</p> </li> <li> <p>Add the keyfile to the encrypted partition by running:</p> <pre><code>sudo cryptsetup luksAddKey /dev/PARTITION PATH-TO-KEYFILE\n</code></pre> <p>Replace <code>PARTITION</code> with the name of the partition you just encrypted.</p> <p>Replace <code>PATH-TO-KEYFILE</code> with the path to the keyfile you just created.</p> <p>When prompted to do so, enter the passphrase you used to encrypt the partition.</p> </li> </ol> </li> <li> <p>Unlock the encrypted partition by running:</p> <pre><code>sudo cryptsetup open /dev/PARTITION PARTITION-NAME\n</code></pre> <p>Enter your passphrase when prompted to do so.</p> <p>Replace <code>PARTITION</code> with the name of the partition you just encrypted.</p> <p>Replace <code>PARTITION-NAME</code> with a name you want to use for the partition while it's decrypted.</p> </li> <li> <p>Create a file system on the partition by running:</p> <pre><code>sudo mkfs.ext4 /dev/mapper/PARTITION-NAME\n</code></pre> <p>Replace <code>PARTITION-NAME</code> with the name you gave the partition in the previous step.</p> </li> <li> <p>Obtain the UUID of your encrypted partition by running:</p> <pre><code>sudo blkid -c /dev/null | grep /dev/PARTITION | cut -d ' ' -f 2\n</code></pre> <p>Replace <code>PARTITION</code> with the name of your partition (vdb1 in the above examples).</p> <p>The command output will look similar to:</p> <pre><code>UUID=\"908f6b4c-3103-4ad3-96e6-96babe8fc8db\"\n</code></pre> <p>Then, create the file <code>/etc/crypttab</code> and add the line:</p> <pre><code>PARTITION-NAME UUID=PARTITION-UUID KEYFILE luks\n</code></pre> <p>Replace <code>PARTITION-NAME</code> with the name you gave the partition in step 4.</p> <p>Replace <code>UUID</code> with the partition's UUID.</p> <p>Replace <code>KEYFILE</code> with <code>none</code> if you didn't create a keyfile. If you did create a keyfile, replace <code>KEYFILE</code> with the path to your keyfile.</p> <p>The complete line will look similar to:</p> <pre><code>encrypted-drive UUID=908f6b4c-3103-4ad3-96e6-96babe8fc8db none luks\n</code></pre> </li> <li> <p>Create a mount point for your encrypted drive by running:</p> <pre><code>sudo mkdir --parents MOUNT-POINT\n</code></pre> <p>Replace <code>MOUNT-POINT</code> with the path you want your encrypted drive to be accessible at, for example, <code>/mnt/encrypted-drive</code>.</p> <p>Add to <code>/etc/fstab</code> the line:</p> <pre><code>/dev/mapper/PARTITION-NAME MOUNT-POINT ext4 defaults 0 2\n</code></pre> <p>Replace <code>PARTITION-NAME</code> with the name you gave your partition.</p> <p>Replace <code>MOUNT-POINT</code> with the mount point you created.</p> <p>The complete line will look similar to:</p> <pre><code>/dev/mapper/encrypted-drive /mnt/data ext4 defaults 0 2\n</code></pre> </li> <li> <p>Reboot your system and when prompted to do so, enter the passphrase for your    encrypted partition. Your encrypted drive will be accessible at the mount    point you created.</p> </li> </ol> <p>Tip</p> <p>To create a directory on your encrypted drive that your normal, unprivileged (non-root) account can create files and directories in, run:</p> <pre><code>sudo mkdir MOUNT-POINT/USER-DIRECTORY &amp;&amp; \\\nsudo chown $(id -u):$(id -g) MOUNT-POINT-DIRECTORY/USER-DIRECTORY\n</code></pre> <p>Replace <code>MOUNT-POINT</code> with the mount point you created in the previous step.</p> <p>Replace <code>USER-DIRECTORY</code> with a name for the directory you want to create.</p> <p>The complete command will look similar to:</p> <pre><code>sudo mkdir /mnt/encrypted-drive/ubuntu &amp;&amp; \\\nsudo chown $(id -u):$(id -g) /mnt/encrypted-drive/ubuntu\n</code></pre>"},{"location":"education/linux-usage/configuring-software-raid/","title":"Configuring Software RAID","text":"<p>Software RAID (redundant array of independent disks) provides fast and resilient storage for your machine learning data. This document shows you how to configure software RAID in your cluster using <code>mdadm</code>.</p> <ol> <li>Install new drives as needed, then power on the machine.</li> <li>Check that the drives are present with <code>lsblk</code>. Your output should look similar to the following:</li> </ol> <pre><code>ubuntu@ubuntu:~$ lsblk\nNAME    MAJ:MIN RM SIZE RO TYPE MOUNTPOINTS\nnvme1n1  259:1   0 1.8T  0 disk\nnvme3n1  259:2   0 1.8T  0 disk\nnvme2n1  259:3   0 1.8T  0 disk\n...\n</code></pre> <ol> <li>Use <code>parted</code> to partition and format the drives. </li> </ol> <p>Warning</p> <p>Before running this step, ensure you back up all your data on this drive. Formatting the drive makes your data unrecoverable.</p> <pre><code>ubuntu@ubuntu:~$ sudo parted /dev/nvme1n1\nGNU Parted 3.4\nUsing /dev/nvme1n1\nWelcome to GNU Parted! Type 'help' to view a list of commands.\n(parted) mklabel gpt\n(parted) mkpart primary ext4 0% 100%\n(parted) set 1 raid on\n(parted) print\nModel: MSI M480 PRO 2TB (nvme)\nDisk /dev/nvme1n1: 2000GB\nSector size (logical/physical): 512B/512B\nPartition Table: gpt\nDisk Flags:\n\nNumber  Start   End     Size    File system  Name     Flags\n   1    1049kB  2000GB  2000GB  ext4         primary  raid\n\n(parted) quit\nInformation: You may need to update /etc/fstab.\n</code></pre> <ol> <li>Repeat the previous step for every drive listed in step 2.</li> <li>Use <code>mdadm</code> to create the RAID array with the new drives. </li> </ol> <p>Note</p> <p>If mdadm is not on the system already, you can install it by running:</p> <p><code>ubuntu@ubuntu:~$ sudo apt update &amp;&amp; sudo apt install mdadm</code></p> <p>This example uses RAID level 5. To use a different RAID level (see below), set <code>--level</code> to the desired RAID level. Your output should look similar to the following:</p> <pre><code>ubuntu@ubuntu:~$ sudo mdadm --create /dev/md0 --level=5 --raid-devices=3 /dev/nvme1n1p1 /dev/nvme2n1p1 /dev/nvme3n1p1\nmdadm: Defaulting to version 1.2 metadata\nmdadm: array /dev/md0 started.\n</code></pre> <ol> <li>Format the RAID array:</li> </ol> <pre><code>ubuntu@ubuntu:~$ sudo mkfs.ext4 /dev/md0\n</code></pre> <ol> <li>Update the <code>mdadm</code> configuration file so that the software RAID persists through reboots:</li> </ol> <pre><code>ubuntu@ubuntu:~$ sudo mdadm --detail --scan &gt;&gt; /etc/mdadm/mdadm.conf\nubuntu@ubuntu:~$ sudo update-initramfs -u\n</code></pre> <ol> <li>Create a mount point and mount the array:</li> </ol> <pre><code>ubuntu@ubuntu:~$ sudo mkdir -p /mnt/raid5 sudo mount /dev/md0 /mnt/raid5\n</code></pre> <ol> <li>Get the the block ID for your RAID array, then add a line to your <code>/etc/fstab</code> so it mounts at boot:</li> </ol> <pre><code>ubuntu@ubuntu:~$ sudo blkid /dev/md0\n#Use a text editor to edit /etc/fstab and add the following line\nUUID-your-uuid-here /mnt/raid5 ext4 defaults,nofail 0 2\n</code></pre>"},{"location":"education/linux-usage/configuring-software-raid/#understanding-raid-levels","title":"Understanding RAID Levels","text":"<p>There are a number of RAID levels, each performing a slightly different function. RAID 5, used in the steps above, provides a good balance between performance and availability. Many sources online \u2014 Wikipedia for example \u2014 provide more information about RAID levels.</p>"},{"location":"education/linux-usage/lambda-stack-and-recovery-images/","title":"Lambda Stack and recovery images","text":""},{"location":"education/linux-usage/lambda-stack-and-recovery-images/#removing-and-reinstalling-lambda-stack","title":"Removing and reinstalling Lambda Stack","text":"<p>To remove and reinstall Lambda Stack:</p> <p>Uninstall (purge) the existing Lambda Stack by running:</p> <pre><code>sudo rm -f /etc/apt/sources.list.d/{graphics,nvidia,cuda}* &amp;&amp; \\\ndpkg -l | \\\nawk '/cuda|lib(accinj64|cu(blas|dart|dnn|fft|inj|pti|rand|solver|sparse)|magma|nccl|npp|nv[^p])|nv(idia|ml)|tensor(flow|board)|torch/ { print $2 }' | \\\nsudo xargs -or apt -y remove --purge\n</code></pre> <p>Then, install the latest Lambda Stack by running:</p> <pre><code>wget -nv -O- https://lambdalabs.com/install-lambda-stack.sh | sh -\n</code></pre>"},{"location":"education/linux-usage/lambda-stack-and-recovery-images/#recovery-images","title":"Recovery images","text":"<p>Note</p> <p>See the Install Ubuntu desktop tutorial, specifically steps 3 and 4, to learn how to create and boot a USB stick (flash drive) using the below recovery images.</p>"},{"location":"education/linux-usage/lambda-stack-and-recovery-images/#workstations","title":"Workstations","text":"<p>Recovery ISO images for Vector and Vector One can be downloaded using the following links:</p> <ul> <li>Lambda Recovery (Focal) (based on Ubuntu 20.04 LTS focal)</li> <li>Lambda Recovery (Jammy) (based on Ubuntu 22.04 LTS jammy)</li> </ul>"},{"location":"education/linux-usage/lambda-stack-and-recovery-images/#tensorbook","title":"Tensorbook","text":"<p>The recovery ISO image for Tensorbook can be downloaded using the following link:</p> <ul> <li>Lambda Recovery for Tensorbook (Jammy) (based on Ubuntu 22.04 LTS jammy)</li> </ul> <p>Note</p> <p>This recovery image is for the Razer x Lambda Tensorbook only and won't work on older Tensorbook models.</p> <p>Note</p> <p>The recovery images contain software distributed under various licenses, including the Software License Agreement (SLA) for NVIDIA cuDNN. The licenses can be viewed in the recovery images at <code>/usr/share/doc/*/copyright</code>. By using the software contained in the recovery images, you agree to these licenses.</p>"},{"location":"education/linux-usage/lambda-stack-and-recovery-images/#servers","title":"Servers","text":"<p>Recovery images aren't available for servers.</p> <p>To reinstall Ubuntu and Lambda Stack on your Lambda server, download the Ubuntu 22.04 Server install image, then follow Ubuntu's Server installation instructions.</p> <p>Install the latest Lambda Stack by logging into Ubuntu and running:</p> <pre><code>wget -nv -O- https://lambdalabs.com/install-lambda-stack.sh | I_AGREE_TO_THE_CUDNN_LICENSE=1 sh -\n</code></pre>"},{"location":"education/linux-usage/troubleshooting-and-debugging/","title":"Troubleshooting and debugging","text":"","tags":["troubleshooting"]},{"location":"education/linux-usage/troubleshooting-and-debugging/#linux","title":"Linux","text":"","tags":["troubleshooting"]},{"location":"education/linux-usage/troubleshooting-and-debugging/#generate-a-lambda-bug-report","title":"Generate a Lambda bug report","text":"<p>Lambda bug reports are useful for troubleshooting systems with NVIDIA GPUs, including Cloud instances.</p> <p>To generate a Lambda bug report, run:</p> <pre><code>wget -nv -O - https://raw.githubusercontent.com/lambdal-support/lambda-public-tools/main/lambda-bug-report.sh | bash -\n</code></pre> <p>Warning</p> <p>You can run this script only on Lambda workstations and On-Demand Cloud instances. Do not run it on a cluster.</p> <p>This command creates a file named <code>lambda-bug-report.tar.gz</code> in your current directory. This file contains the Lambda bug report.</p> <p>You can view the contents of the Lambda bug report by running <code>tar -zxf lambda-bug-report.tar.gz</code>. This command creates a directory named <code>lambda-bug-report</code> containing the contents of the Lambda bug report.</p> <p>The Lambda bug report includes the NVIDIA bug report log file, which contains a lot of information about the GPU. For tips on troubleshooting using the NVIDIA bug report file, see Using the nvidia-bug-report.log file to troubleshoot your system.</p> <p>Note</p> <p>Be sure to generate and provide a Lambda bug report whenever submitting a support ticket to Lambda Support. Providing a Lambda bug report when you first submit a ticket helps Support more quickly troubleshoot any problems you're experiencing with your Lambda system.</p>","tags":["troubleshooting"]},{"location":"education/linux-usage/troubleshooting-and-debugging/#fix-blank-black-screen-with-blinking-cursor","title":"Fix blank black screen with blinking cursor","text":"<p>If your system boots to a blank black screen with a blinking cursor, rather than the expected GUI, you might need to reinstall Lambda Stack. To reinstall Lambda Stack when the GUI doesn't launch:</p> <p>Note</p> <p>If your boot drive is encrypted, first enter your passphrase at the blank screen, then press Enter.</p> <ol> <li>Press Alt + Ctrl + F3 to switch to a virtual console, then log into Ubuntu using your usual login and password.</li> <li> <p>Follow the instructions for removing and reinstalling Lambda Stack. Then, run <code>sudo reboot</code> to restart your system.</p> <p>If after restarting you're still encountering a blank black screen with a blinking cursor, follow step 1 again, then run <code>sudo apt update &amp;&amp; sudo apt -y full-upgrade</code> to update your system. When the update is finished, restart your system by running <code>sudo reboot</code>.</p> </li> </ol> <p>Tip</p> <p>If your system is set up for remote access via SSH, you can instead SSH into your system to run the commands listed in step 2.</p>","tags":["troubleshooting"]},{"location":"education/linux-usage/troubleshooting-and-debugging/#windows","title":"Windows","text":"","tags":["troubleshooting"]},{"location":"education/linux-usage/troubleshooting-and-debugging/#how-do-i-gather-system-information-in-windows-using-dxdiag","title":"How do I gather system information in Windows using DxDiag?","text":"<p>You can use the DirectX Diagnostic Tool (DxDiag) in Windows to gather system information that might be helpful for troubleshooting.</p> <p>To use DxDiag to gather system information:</p> <ol> <li> <p>Press the Windows key + R to open the Run window. Type</p> <pre><code>dxdiag /dontskip /whql:off /64bit /t %userprofile%/Desktop/DxDiag.txt\n</code></pre> <p>then press Enter. 2. After 5-10 seconds, a file named <code>DxDiag.txt</code> will be created on the Desktop. This file contains information about the system.</p> </li> </ol> <p>If you prefer to use the graphical interface of DxDiag to gather system information:</p> <ol> <li>Press the Windows key + R to open the Run window.</li> <li>In the Run window, type <code>dxdiag</code> then press Enter.</li> <li>Click Save All Information to save the system information to a text file.</li> </ol>","tags":["troubleshooting"]},{"location":"education/linux-usage/troubleshooting-and-debugging/#how-do-i-gather-system-information-in-windows-using-msinfo32","title":"How do I gather system information in Windows using MSINFO32?","text":"<p>You can use MSINFO32, also known as Microsoft System Information or Msinfo32.exe, to gather useful information for troubleshooting your computer.</p> <p>To use MSINFO32:</p> <ol> <li>Press the Windows key + R to open the Run window. Type <code>cmd</code> then    press Ctrl + Shift + Enter to open an elevated command prompt.</li> <li> <p>In the elevated command prompt, run:</p> <pre><code>MSInfo32 /report %userprofile%/Desktop/sysinfo.txt\n</code></pre> </li> </ol> <p>This will create a file on your Desktop named <code>sysinfo.txt</code> that contains useful information about your computer.</p> <p>Note</p> <p>You can additionally use DxDiag to gather useful information for troubleshooting.</p>","tags":["troubleshooting"]},{"location":"education/linux-usage/troubleshooting-and-debugging/#how-do-i-prevent-an-incorrect-clock-in-windows-after-booting-ubuntu","title":"How do I prevent an incorrect clock in Windows after booting Ubuntu?","text":"<p>Ubuntu saves the current time to the machine hardware clock in UTC. Windows saves the current time to the machine hardware clock in the local time. Because Ubuntu and Windows save the current time to the machine hardware clock differently, the current time displays incorrectly in Windows after booting Ubuntu.</p> <p>To fix this problem:</p> <ol> <li> <p>Press the Windows key + R to open the Run window. Type <code>cmd</code> then press Ctrl + Shift + Enter.</p> <p>This command opens the Command Prompt window with elevated (admin) privileges. 2.  In the Command Prompt window, run the command: <code>reg add \"HKEY_LOCAL_MACHINE\\System\\CurrentControlSet\\Control\\TimeZoneInformation\" /v RealTimeIsUniversal /d 1 /t REG_DWORD /f</code>.</p> <p>If Windows asks to update the clock due to DST (Daylight Savings Time), let Windows update the clock. The current time will be saved to the machine hardware clock in UTC but will display in Windows in the local time.</p> </li> </ol> <p>To learn more about this problem, see the Arch Linux documentation on system time.</p>","tags":["troubleshooting"]},{"location":"education/linux-usage/troubleshooting-and-debugging/#how-do-i-run-a-filesystem-check-on-windows-and-repair-system-files","title":"How do I run a filesystem check on Windows and repair system files?","text":"<p>If Windows is freezing or behaving erratically, there might be a problem with the filesystem or system files. Examples of erratic behavior include:</p> <ul> <li>blue screens</li> <li>crashing applications</li> <li>sluggish performance</li> <li>failed Windows updates</li> </ul> <p>You can use the built-in Deployment Image Servicing and Management (DISM) and System File Checker (SFC) tools to try to fix the problem.</p> <p>To use the DISM tool to fix problems with the filesystem or system files:</p> <ol> <li>Press the Windows key + R to open the Run window.</li> <li> <p>In the Run window, type <code>cmd</code> then press Ctrl + Shift + Enter.</p> <p>This command opens the Command Prompt window with elevated (admin) privileges. 3.  In the Command Prompt window, run the command:</p> <pre><code>DISM.exe /Online /Cleanup-Image /RestoreHealth\n</code></pre> <p>This command performs cleanup and recovery operations on the system files. For more information, see Microsoft\u2019s documentation on repairing a Windows image. 4. After <code>DISM.exe</code> finishes, run the following command in the Command Prompt window: <code>sfc /scannow</code>. This command scans all protected system files and replaces corrupted files. For more information, see Microsoft\u2019s documentation on using SFC to repair missing or corrupted system files.</p> </li> </ol> <p>Run the <code>sfc /scannow</code> command up to 3 times until you receive the message: <code>Windows Resource Protection did not find any integrity violations.</code></p> <p>Note</p> <p>If after running this command 3 times you do not receive the message <code>Windows Resource Protection did not find any integrity violations</code>, it is unlikely that a problem with the filesystem or system files is the cause of Windows freezing or behaving erratically.</p> <p>Reboot the machine and test to see if Windows continues to freeze or behave erratically.</p>","tags":["troubleshooting"]},{"location":"education/linux-usage/troubleshooting-and-debugging/#where-can-i-find-my-windows-license-key","title":"Where can I find my Windows license key?","text":"<p>If you purchased your system with Windows 11 preinstalled, you can download and reinstall Windows 11 without a license key.</p> <p>If you purchased your system with Windows 10 preinstalled, your Windows license key is located on a cardboard envelope included with your system.</p> <p>Cardboard envelope with Windows license key</p> <p>Warning</p> <p>Make sure not to lose your license key! We can't replace lost license keys.</p> <p>Inside of the license key envelope is a sticker with the license key on it. We strongly suggest affixing this sticker to your system's chassis.</p>","tags":["troubleshooting"]},{"location":"education/linux-usage/using-the-lambda-bug-report-to-troubleshoot-your-system/","title":"Using the Lambda bug report to troubleshoot your system","text":"<p>The Lambda bug report helps simplify the process of troubleshooting by collecting system information for you into one place. This article helps you utilize the <code>lambda-bug-report.log</code> file to troubleshoot common issues.</p> <p>Warning</p> <p>The <code>lambda-bug-report.sh</code> is intended for use on Vector, Scalar, Hyperplane, and On-Demand products only. Do not run this script on a cluster as it installs packages that may cause unintended outcomes.</p> <p>To generate a report, run:</p> <pre><code>wget -nv -O - https://raw.githubusercontent.com/lambdal-support/lambda-public-tools/main/lambda-bug-report.sh | bash -\n</code></pre> <p>This command creates a file named <code>lambda-bug-report.tar.gz</code> in your current directory that contains the Lambda bug report.</p>","tags":["troubleshooting"]},{"location":"education/linux-usage/using-the-lambda-bug-report-to-troubleshoot-your-system/#understanding-the-bug-report-log-file","title":"Understanding the bug report log file","text":"<p>The log file is organized by system components, making it easier to locate relevant information. It includes outputs from various commands, scripts, and logs that you might otherwise have to collect manually.</p> <p>While the log file is extensive, certain sections are more commonly referenced for troubleshooting. Outlined below are the key directories and files you should consider examining when troubleshooting.</p>","tags":["troubleshooting"]},{"location":"education/linux-usage/using-the-lambda-bug-report-to-troubleshoot-your-system/#baseboard-management-controller-bmc-information","title":"Baseboard Management Controller (BMC) information","text":"<p>The <code>bmc-info</code> folder collects sensor information and error history for systems with a BMC. This information is useful if you suspect hardware malfunctions or want to check the health status of your system components.</p> <p>Note</p> <p>Only Hyperplane, Scalar, Vector, and Vector Pro products have a BMC. The Vector One does not include a BMC, so these files, while present in the bug report run on a Vector One, do not contain Intelligent Platform Management Interface (IPMI) data.</p> <p>Files:</p> <p><code>ipmi-elist.txt, ipmi-sdr.txt</code></p>","tags":["troubleshooting"]},{"location":"education/linux-usage/using-the-lambda-bug-report-to-troubleshoot-your-system/#drives-and-storage","title":"Drives and storage","text":"<p>This section provides information about disk usage, mounted filesystems, RAID configurations, and disk I/O statistics. Look here when experiencing issues related to storage, such as disk failures, insufficient disk space, or mounting errors.</p> <p>Files:</p> <p><code>df.txt, fstab.txt, iostat.txt, lsblk.txt, mdadm-conf.txt, mdadm-scan.txt, mdstat.txt, mounts.txt</code></p>","tags":["troubleshooting"]},{"location":"education/linux-usage/using-the-lambda-bug-report-to-troubleshoot-your-system/#gpu-memory-errors","title":"GPU memory errors","text":"<p>The following files are logs of GPU memory errors, including error-correcting code (ECC) errors and remapped memory regions. They are relevant if you\u2019re encountering GPU-related issues like crashes during computation or suspect faulty GPU memory.</p> <p>Files:</p> <p><code>ecc-errors.txt, remapped-memory.txt, uncorrected-ecc_errors.txt</code></p>","tags":["troubleshooting"]},{"location":"education/linux-usage/using-the-lambda-bug-report-to-troubleshoot-your-system/#grub","title":"Grub","text":"<p>The <code>grub</code> folder stores the GRUB bootloader configuration files and boot command-line parameters. These files can be helpful when troubleshooting boot issues or modifying boot parameters for kernel debugging.</p> <p>Files:</p> <p><code>grub.d/50-cloudimg-settings.cfg, grub.d/init-select.cfg, grub.txt, proc_cmdline.txt</code></p>","tags":["troubleshooting"]},{"location":"education/linux-usage/using-the-lambda-bug-report-to-troubleshoot-your-system/#hardware-list","title":"Hardware list","text":"<p>This section lists all recognized hardware devices on the system. It\u2019s useful to verify if all hardware components are detected by the system or to identify missing devices.</p> <p>Files:</p> <p><code>hw-list.txt</code></p>","tags":["troubleshooting"]},{"location":"education/linux-usage/using-the-lambda-bug-report-to-troubleshoot-your-system/#networking","title":"Networking","text":"<p>You can find network configuration and status here, including IP addresses, firewall settings, and active network connections. This section is helpful when facing network connectivity issues, firewall problems, or to check network configurations.</p> <p>Files:</p> <p><code>ip-addr.txt, iptables.txt, netplan.txt, resolvectl-status.txt, ss.txt, ufw-status.txt</code></p>","tags":["troubleshooting"]},{"location":"education/linux-usage/using-the-lambda-bug-report-to-troubleshoot-your-system/#nvidia-bug-report-and-smi","title":"NVIDIA bug report and SMI","text":"<p>The <code>nvidia-bug-report.log</code> and <code>nvidia-smi.txt</code> files contain detailed information about NVIDIA GPU drivers and hardware status. Use this information for diagnosing GPU-related issues, driver problems, or performance bottlenecks involving NVIDIA GPUs. For tips on how to best use the NVIDIA bug report file, see Using the NVIDIA bug report to troubleshoot your system.</p> <p>Files:</p> <p><code>nvidia-bug-report.log, nvidia-smi.txt</code></p>","tags":["troubleshooting"]},{"location":"education/linux-usage/using-the-lambda-bug-report-to-troubleshoot-your-system/#repositories-and-packages","title":"Repositories and packages","text":"<p>This section lists installed packages, their sources, and repository configurations. Use these files to identify software conflicts, check package versions, or verify repository settings.</p> <p>Files:</p> <p><code>dpkg.txt, listd-repos.txt, pip-list.txt, sources-list.txt</code></p>","tags":["troubleshooting"]},{"location":"education/linux-usage/using-the-lambda-bug-report-to-troubleshoot-your-system/#sensors","title":"Sensors","text":"<p>The <code>sensors.txt</code> file contains internal thermal sensor readings. This information is helpful when investigating overheating issues or thermal throttling.</p> <p>Files:</p> <p><code>sensors.txt</code></p>","tags":["troubleshooting"]},{"location":"education/linux-usage/using-the-lambda-bug-report-to-troubleshoot-your-system/#system-logs","title":"System logs","text":"<p>This section aggregates various system logs that record events and errors from different system components. It provides a broad overview of system events, kernel messages, package installation history, and error tracking.</p> <p>Files:</p> <p><code>apt-history.log, dmesg, dmesg-errors.txt, dpkg.log, journalctl.txt, kern.log, syslog</code></p>","tags":["troubleshooting"]},{"location":"education/linux-usage/using-the-lambda-bug-report-to-troubleshoot-your-system/#systemctl-services","title":"Systemctl services","text":"<p>The <code>systemctl-services.txt</code> file contains the status of <code>systemd</code> services. Check this section to verify essential services are either running or have failed.</p> <p>Files:</p> <p><code>systemctl-services.txt</code></p>","tags":["troubleshooting"]},{"location":"education/linux-usage/using-the-lambda-bug-report-to-troubleshoot-your-system/#top","title":"Top","text":"<p>Top captures a snapshot of system processes and resource usage at the time the bug report was generated. This section helps identify processes consuming excessive CPU or memory resources that may lead to performance degradation.</p> <p>Files:</p> <p><code>top.txt</code></p>","tags":["troubleshooting"]},{"location":"education/linux-usage/using-the-lambda-bug-report-to-troubleshoot-your-system/#bug-report-folder-hierarchy","title":"Bug report folder hierarchy","text":"<p>The files in the Lambda bug report are organized into the following folders:</p> <pre><code>\u251c\u2500\u2500 bmc-info\n    \u251c\u2500\u2500 ipmi-elist.txt\n    \u2514\u2500\u2500 ipmi-sdr.txt\n\u251c\u2500\u2500 drives-and-storage\n    \u251c\u2500\u2500 df.txt\n    \u251c\u2500\u2500 fstab.txt\n    \u251c\u2500\u2500 iostat.txt\n    \u251c\u2500\u2500 lsblk.txt\n    \u251c\u2500\u2500 mdadm-conf.txt\n    \u251c\u2500\u2500 mdadm-scan.txt\n    \u251c\u2500\u2500 mdstat.txt\n    \u2514\u2500\u2500 mounts.txt\n\u251c\u2500\u2500 gpu-memory-errors\n    \u251c\u2500\u2500 ecc-errors.txt\n    \u251c\u2500\u2500 remapped-memory.txt\n    \u2514\u2500\u2500 uncorrected-ecc_errors.txt\n\u251c\u2500\u2500 grub\n    \u251c\u2500\u2500 grub.d\n    \u2502   \u251c\u2500\u2500 50-cloudimg-settings.cfg\n    \u2502   \u2514\u2500\u2500 init-select.cfg\n    \u251c\u2500\u2500 grub.txt\n    \u2514\u2500\u2500 proc_cmdline.txt\n\u251c\u2500\u2500 hibernation-settings.txt\n\u251c\u2500\u2500 hw-list.txt\n\u251c\u2500\u2500 ibstat.txt\n\u251c\u2500\u2500 lsmod.txt\n\u251c\u2500\u2500 networking\n    \u251c\u2500\u2500 ip-addr.txt\n    \u251c\u2500\u2500 iptables.txt\n    \u251c\u2500\u2500 netplan.txt\n    \u251c\u2500\u2500 resolvectl-status.txt\n    \u251c\u2500\u2500 ss.txt\n    \u2514\u2500\u2500 ufw-status.txt\n\u251c\u2500\u2500 nvidia-bug-report.log\n\u251c\u2500\u2500 nvidia-smi.txt\n\u251c\u2500\u2500 repos-and-packages\n    \u251c\u2500\u2500 dpkg.txt\n    \u251c\u2500\u2500 listd-repos.txt\n    \u251c\u2500\u2500 pip-list.txt\n    \u2514\u2500\u2500 sources-list.txt\n\u251c\u2500\u2500 sensors.txt\n\u251c\u2500\u2500 sysctl-all.txt\n\u251c\u2500\u2500 system-logs\n    \u251c\u2500\u2500 apt-history.log\n    \u251c\u2500\u2500 dmesg\n    \u251c\u2500\u2500 dmesg-errors.txt\n    \u251c\u2500\u2500 dpkg.log\n    \u251c\u2500\u2500 journalctl.txt\n    \u251c\u2500\u2500 kern.log\n    \u2514\u2500\u2500 syslog\n\u251c\u2500\u2500 systemctl-services.txt\n\u2514\u2500\u2500 top.txt\n</code></pre>","tags":["troubleshooting"]},{"location":"education/linux-usage/using-the-lambda-bug-report-to-troubleshoot-your-system/#contact-lambda","title":"Contact Lambda","text":"<p>If you can\u2019t discover the cause for the issue you are experiencing, contact Lambda Support and provide us with your Lambda bug report.</p>","tags":["troubleshooting"]},{"location":"education/linux-usage/using-the-lambda-bug-report-to-troubleshoot-your-system/#other-resources","title":"Other resources","text":"<ul> <li>Using the NVIDIA bug report log file to troubleshoot your system</li> <li>Troubleshooting and debugging</li> </ul>","tags":["troubleshooting"]},{"location":"education/linux-usage/using-the-nvidia-bug-report.log-file-to-troubleshoot-your-system/","title":"Using the nvidia-bug-report.log file to troubleshoot your system","text":"<p>NVIDIA provides a script that generates a log file that you can use to troubleshoot issues with NVIDIA GPUs. This log file has comprehensive information about your system, including information about individual devices, configuration of NVIDIA drivers, system journals, and more.</p>"},{"location":"education/linux-usage/using-the-nvidia-bug-report.log-file-to-troubleshoot-your-system/#generate-the-log-file","title":"Generate the log file","text":"<p>To generate the log file, log in as the root user or use <code>sudo</code>, then run the following command:</p> <pre><code>sudo nvidia-bug-report.sh\n</code></pre> <p>This script generates a zipped file called nvidia-bug-report.log.gz in the current directory. To verify that the script ran successfully, run <code>ls -la nvidia*</code> and look for a row similar to the following:</p> <pre><code>-rw-r--r-- 1 root   root   207757 Aug 28 20:11 nvidia-bug-report.log.gz\n</code></pre> <p>After you generate the log archive file, you can expand it and open the log in a text editor.</p>"},{"location":"education/linux-usage/using-the-nvidia-bug-report.log-file-to-troubleshoot-your-system/#troubleshoot-the-log-file","title":"Troubleshoot the log file","text":"<p>The log file is comprehensive, as it collects information from various sources. The following are suggestions on where to start looking, depending on the issue you are seeing. You might see output in the log file from the same or a related Linux command you run on your system, or both.</p>"},{"location":"education/linux-usage/using-the-nvidia-bug-report.log-file-to-troubleshoot-your-system/#use-the-check-nvidia-bug-report-shell-script","title":"Use the check-nvidia-bug-report shell script","text":"<p>To make the NVIDIA log report easier to use, Lambda provides a shell script, <code>check-nvidia-bug-report.sh</code>, that parses and summarizes the report. This script scans the report for:</p> <ul> <li>Xid errors</li> <li>Thermal slowdown messages</li> <li>Segfaults</li> <li>CPU throttling and bad CPU errors</li> <li>Hardware errors</li> <li>\u201cFallen off the bus\u201d errors</li> <li>RmInit failures</li> </ul> <p>You can find this script in the lambda-public-tools GitHub repository. To use the script, first clone the repository:</p> <pre><code>git clone https://github.com/lambdal-support/lambda-public-tools.git\n</code></pre> <p>After you generate the NVIDIA log file, run the Lambda script. The following example assumes the Lambda script and the NVIDIA log file are in the same directory:</p> <pre><code>cd lambda-public-tools/check-nvidia-bug-report\n./check-nvidia-bug-report.sh\n</code></pre>"},{"location":"education/linux-usage/using-the-nvidia-bug-report.log-file-to-troubleshoot-your-system/#verify-hardware-with-dmidecode","title":"Verify hardware with dmidecode","text":"<p>If you prefer to investigate the log file on your own, a good place to start is to check that all the hardware reported by the BIOS is installed, available, and seen by the system. Use <code>dmidecode</code>, or search for dmidecode in the log file.</p> <pre><code>/sbin/dmidecode\n</code></pre> <p>You should see output similar to the following:</p> <pre><code># dmidecode 3.3\nGetting SMBIOS data from sysfs.\nSMBIOS 2.8 present.\n53 structures occupying 3036 bytes.\nTable at 0x7FFFF420.\n\nHandle 0x0000, DMI type 0, 24 bytes\nBIOS Information\nVendor: SeaBIOS\nVersion: 1.13.0-1ubuntu1.1\nRelease Date: 04/01/2014\nAddress: 0xE8000\nRuntime Size: 96 kB\nROM Size: 64 kB\nCharacteristics:\nBIOS characteristics not supported\nTargeted content distribution is supported\nBIOS Revision: 0.0\n\nHandle 0x0100, DMI type 1, 27 bytes\nSystem Information\nManufacturer: QEMU\nProduct Name: Standard PC (Q35 + ICH9, 2009)\nVersion: pc-q35-8.0\nSerial Number: Not Specified\nUUID: 55c8d550-3188-4747-9711-89067cca4646\nWake-up Type: Power Switch\nSKU Number: Not Specified\nFamily: Not Specified\n\nHandle 0x0300, DMI type 3, 22 bytes\nChassis Information\nManufacturer: QEMU\nType: Other\nLock: Not Present\nVersion: pc-q35-8.0\nSerial Number: Not Specified\nAsset Tag: Not Specified\nBoot-up State: Safe\nPower Supply State: Safe\nThermal State: Safe\nSecurity Status: Unknown\nOEM Information: 0x00000000\nHeight: Unspecified\nNumber Of Power Cords: Unspecified\nContained Elements: 0\nSKU Number: Not Specified\n\nHandle 0x0400, DMI type 4, 42 bytes\nProcessor Information\nSocket Designation: CPU 0\nType: Central Processor\nFamily: Other\nManufacturer: QEMU\nID: A6 06 06 00 FF FB 8B 0F\nVersion: pc-q35-8.0\nVoltage: Unknown\nExternal Clock: Unknown\nMax Speed: 2000 MHz\nCurrent Speed: 2000 MHz\nStatus: Populated, Enabled\nUpgrade: Other\nL1 Cache Handle: Not Provided\nL2 Cache Handle: Not Provided\nL3 Cache Handle: Not Provided\nSerial Number: Not Specified\nAsset Tag: Not Specified\nPart Number: Not Specified\nCore Count: 1\nCore Enabled: 1\nThread Count: 1\nCharacteristics: None\n\nHandle 0x0401, DMI type 4, 42 bytes\nProcessor Information\nSocket Designation: CPU 1\nType: Central Processor\nFamily: Other\n\u2026\n</code></pre>"},{"location":"education/linux-usage/using-the-nvidia-bug-report.log-file-to-troubleshoot-your-system/#check-for-xid-or-sxid-errors","title":"Check for Xid or SXid errors","text":"<p>An Xid message is an NVIDIA error report that prints to the kernel log or event log. Xid messages indicate that a general GPU error occurred, typically due to the driver programming the GPU incorrectly or by corruption of the commands sent to the GPU. The messages may indicate a hardware problem, an NVIDIA software problem, or an application problem. To understand the Xid message, read the NVIDIA documentation and review these common Xid errors.</p> <p>NVIDIA drivers for NVSwitch report error conditions relating to NVSwitch hardware in kernel logs through a mechanism similar to Xids. SXid (or switch Xids) are errors relating to the NVIDIA switch hardware; they appear in kernel logs similar to Xids. For more information about SXids, read appendixes D.4 through D.7 in the NVIDIA documentation.</p> <p>Search the log file for Xid or SXid and see what errors are associated with them, or run <code>dmesg</code> as root:</p> <pre><code>sudo dmesg | grep SXid\n</code></pre> <p>You should see output like the following:</p> <pre><code>[   60.006338] kernel: nvidia-nvswitch3: SXid (PCI:0000:0a:00.0): 22013, Non-fatal, Link 55 Minion Link DLREQ interrupt\n[   60.008040] kernel: nvidia-nvswitch3: SXid (PCI:0000:0a:00.0): 22013, Severity 0 Engine instance 55 Sub-engine instance 00\n[   60.010058] kernel: nvidia-nvswitch3: SXid (PCI:0000:0a:00.0): 22013, Data {0x00000037, 0x00000037, 0x00000000, 0x00000037, 0x80005302, 0x00000000, 0x00000000, 0x00000000, 0x00000000}\n[   60.013283] kernel: nvidia-nvswitch3: SXid (PCI:0000:0a:00.0): 22013, Non-fatal, Link 54 Minion Link DLREQ interrupt\n[   60.015379] kernel: nvidia-nvswitch3: SXid (PCI:0000:0a:00.0): 22013, Severity 0 Engine instance 54 Sub-engine instance 00\n[   60.017388] kernel: nvidia-nvswitch3: SXid (PCI:0000:0a:00.0): 22013, Data {0x00000036, 0x00000036, 0x00000000, 0x00000036, 0x80005302, 0x00000000, 0x00000000, 0x00000000, 0x00000000}\n[   60.020455] kernel: nvidia-nvswitch3: SXid (PCI:0000:0a:00.0): 22013, Non-fatal, Link 51 Minion Link DLREQ interrupt\n[   60.022557] kernel: nvidia-nvswitch3: SXid (PCI:0000:0a:00.0): 22013, Severity 0 Engine instance 51 Sub-engine instance 00\n[   60.024563] kernel: nvidia-nvswitch3: SXid (PCI:0000:0a:00.0): 22013, Data {0x00000033, 0x00000033, 0x00000000, 0x00000033, 0x80005302, 0x00000000, 0x00000000, 0x00000000, 0x00000000}\n[   60.027627] kernel: nvidia-nvswitch3: SXid (PCI:0000:0a:00.0): 22013, Non-fatal, Link 50 Minion Link DLREQ interrupt\n[   60.029737] kernel: nvidia-nvswitch3: SXid (PCI:0000:0a:00.0): 22013, Severity 0 Engine instance 50 Sub-engine instance 00\n[   60.031772] kernel: nvidia-nvswitch3: SXid (PCI:0000:0a:00.0): 22013, Data {0x00000032, 0x00000032, 0x00000000, 0x00000032, 0x80005302, 0x00000000, 0x00000000, 0x00000000, 0x00000000}\n</code></pre>"},{"location":"education/linux-usage/using-the-nvidia-bug-report.log-file-to-troubleshoot-your-system/#detect-the-gpu","title":"Detect the GPU","text":"<p>To make sure your system can see the GPUs, run <code>nvidia-smi</code>:</p> <pre><code>nvidia-smi\n</code></pre> <p>The output below is for a single GPU system:</p> <pre><code>Wed Aug 28 20:01:47 2024\n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  NVIDIA A10                     On  | 00000000:08:00.0 Off |                    0 |\n|  0%   26C    P8               8W / 150W |      4MiB / 23028MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n\n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n|  No running processes found                                                           |\n+---------------------------------------------------------------------------------------+\n</code></pre>"},{"location":"education/linux-usage/using-the-nvidia-bug-report.log-file-to-troubleshoot-your-system/#check-disk-usage","title":"Check disk usage","text":"<p>You can use <code>df -h</code> to see disk usage on your nodes. Running low on disk space could affect performance of your system.</p> <pre><code>df -h\n</code></pre> <p>The output should look something like the following:</p> <pre><code>Filesystem                            Size  Used Avail Use% Mounted on\ntmpfs                                  23G  1.2M   23G   1% /run\n/dev/vda1                             1.4T   25G  1.4T   2% /\ntmpfs                                 112G     0  112G   0% /dev/shm\ntmpfs                                 5.0M     0  5.0M   0% /run/lock\nae17b7ce-8c3b-464d-9f7c-325bc1080289  8.0E     0  8.0E   0% /home/ubuntu/lambda-pete-fs1\n/dev/vda15                            105M  6.1M   99M   6% /boot/efi\n</code></pre>"},{"location":"education/linux-usage/using-the-nvidia-bug-report.log-file-to-troubleshoot-your-system/#check-the-systemd-journal-for-errors","title":"Check the systemd journal for errors","text":"<p>To check the <code>systemd</code> journal, either search for <code>journalctl</code> entries in the log file or run the <code>journalctl</code> command. You should see output similar to the following:</p> <pre><code>Aug 28 19:18:40 lambda-node jupyter[897]: [W 2024-08-28 19:18:40.342 ServerApp] ServerApp.token config is deprecated in 2.0. Use IdentityProvider.token.\nAug 28 19:18:40 lambda-node jupyter[897]: [W 2024-08-28 19:18:40.342 ServerApp] ServerApp.allow_password_change config is deprecated in 2.0. Use PasswordIde&gt;\nAug 28 19:18:40 lambda-node jupyter[897]: [I 2024-08-28 19:18:40.349 ServerApp] Package jupyterlab took 0.0000s to import\nAug 28 19:18:40 lambda-node jupyter[897]: [I 2024-08-28 19:18:40.371 ServerApp] Package jupyter_collaboration took 0.0215s to import\nAug 28 19:18:40 lambda-node jupyter[897]: [I 2024-08-28 19:18:40.382 ServerApp] Package jupyter_lsp took 0.0103s to import\nAug 28 19:18:40 lambda-node jupyter[897]: [W 2024-08-28 19:18:40.382 ServerApp] A `_jupyter_server_extension_points` function was not found in jupyter_lsp. &gt;\nAug 28 19:18:40 lambda-node jupyter[897]: [I 2024-08-28 19:18:40.382 ServerApp] Package jupyter_server_fileid took 0.0000s to import\nAug 28 19:18:40 lambda-node jupyter[897]: [I 2024-08-28 19:18:40.387 ServerApp] Package jupyter_server_terminals took 0.0047s to import\n\u2026\n</code></pre>"},{"location":"education/linux-usage/using-the-nvidia-bug-report.log-file-to-troubleshoot-your-system/#check-your-nvlink-topology","title":"Check Your NVLink topology","text":"<p>To confirm that your NVLink topology is correct, search the log file for <code>nvidia-smi nvlink --status</code> output or run the following command:</p> <pre><code>nvidia-smi nvlink --status\n</code></pre> <p>This command checks the status of each NVLink connection for each GPU. The output shows information about each NVLink, including the utilization and active or inactive status. It\u2019s similar to the following truncated output for an eight-GPU system:</p> <pre><code>GPU 0: NVIDIA H100 80GB HBM3 (UUID: GPU-4f495419-b618-a303-a09b-8246fed175ac)\nLink 0: 26.562 GB/s\nLink 1: 26.562 GB/s\nLink 2: 26.562 GB/s\nLink 3: 26.562 GB/s\nLink 4: 26.562 GB/s\nLink 5: 26.562 GB/s\nLink 6: 26.562 GB/s\nLink 7: 26.562 GB/s\nLink 8: 26.562 GB/s\nLink 9: 26.562 GB/s\nLink 10: 26.562 GB/s\nLink 11: 26.562 GB/s\nLink 12: 26.562 GB/s\nLink 13: 26.562 GB/s\nLink 14: 26.562 GB/s\nLink 15: 26.562 GB/s\nLink 16: 26.562 GB/s\nLink 17: 26.562 GB/s\nGPU 1: NVIDIA H100 80GB HBM3 (UUID: GPU-fe4b42e7-b7ad-49be-6765-66a18c186dc2)\nLink 0: 26.562 GB/s\nLink 1: 26.562 GB/s\nLink 2: 26.562 GB/s\n\n...\n\nGPU 7: NVIDIA H100 80GB HBM3 (UUID: GPU-2d4d1957-9704-9732-3c2d-628238913816)\nLink 0: 26.562 GB/s\nLink 1: 26.562 GB/s\nLink 2: 26.562 GB/s\nLink 3: 26.562 GB/s\nLink 4: 26.562 GB/s\nLink 5: 26.562 GB/s\nLink 6: 26.562 GB/s\nLink 7: 26.562 GB/s\nLink 8: 26.562 GB/s\nLink 9: 26.562 GB/s\nLink 10: 26.562 GB/s\nLink 11: 26.562 GB/s\nLink 12: 26.562 GB/s\nLink 13: 26.562 GB/s\nLink 14: 26.562 GB/s\nLink 15: 26.562 GB/s\nLink 16: 26.562 GB/s\nLink 17: 26.562 GB/s\n</code></pre>"},{"location":"education/linux-usage/using-the-nvidia-bug-report.log-file-to-troubleshoot-your-system/#show-specific-output","title":"Show specific output","text":"<p>The <code>nvidia-smi</code> command can return a wealth of content. You can fine-tune your results to isolate a specific issue by using the <code>-d</code> (display) option along with the <code>-q</code> (query) option. For example, if you suspect a memory issue, you can choose to display only memory-related output:</p> <pre><code>nvidia-smi -q -d MEMORY\n</code></pre> <p>The output looks similar to the following:</p> <pre><code>==============NVSMI LOG==============\n\nTimestamp                                 : Thu Aug 29 16:45:11 2024\nDriver Version                            : 535.129.03\nCUDA Version                              : 12.2\n\nAttached GPUs                             : 1\nGPU 00000000:08:00.0\n    FB Memory Usage\n        Total                             : 23028 MiB\n        Reserved                          : 512 MiB\n        Used                              : 4 MiB\n        Free                              : 22511 MiB\n    BAR1 Memory Usage\n        Total                             : 32768 MiB\n        Used                              : 1 MiB\n        Free                              : 32767 MiB\n    Conf Compute Protected Memory Usage\n        Total                             : 0 MiB\n        Used                              : 0 MiB\n        Free                              : 0 MiB\n</code></pre> <p>The output you can display includes:</p> <ul> <li><code>MEMORY</code>: displays memory usage in the system.</li> <li><code>UTILIZATION</code>: displays GPU, memory, and encode/decode utilization rates, including sampling data with maximum, minimum, and average.</li> <li><code>ECC</code>: displays error correction code mode and errors.</li> <li><code>TEMPERATURE</code>: displays temperature data for the GPU.</li> <li><code>POWER</code>: displays power readings, including sampling data with maximum, minimum, and average.</li> <li><code>CLOCK</code>: displays data for all the clocks in the GPU, including sampling data with maximum, minimum, and average.</li> <li><code>COMPUTE</code>: displays the compute mode for the GPU.</li> <li><code>PIDS</code>: displays running processes.</li> <li><code>PERFORMANCE</code>: displays performance information for the GPU.</li> <li><code>SUPPORTED_CLOCKS</code>: displays the supported frequencies for the GPU clocks.</li> <li><code>PAGE_RETIREMENT</code>: when ECC is enabled, this option displays any framebuffer pages that have been dynamically retired.</li> <li><code>ACCOUNTING</code>: displays which processes are subject to accounting, how many processes are subject to accounting, and whether accounting mode is enabled.</li> </ul> <p>You can specify multiple options by separating them with a comma. For example, the following command displays information about both memory and power usage:</p> <pre><code>nvidia-smi -q -d MEMORY,POWER\n</code></pre> <p>The output looks similar to the following:</p> <pre><code>==============NVSMI LOG==============\n\nTimestamp                                 : Thu Aug 29 16:54:52 2024\nDriver Version                            : 535.129.03\nCUDA Version                              : 12.2\n\nAttached GPUs                             : 1\nGPU 00000000:08:00.0\n    FB Memory Usage\n        Total                             : 23028 MiB\n        Reserved                          : 512 MiB\n        Used                              : 4 MiB\n        Free                              : 22511 MiB\n    BAR1 Memory Usage\n        Total                             : 32768 MiB\n        Used                              : 1 MiB\n        Free                              : 32767 MiB\n    Conf Compute Protected Memory Usage\n        Total                             : 0 MiB\n        Used                              : 0 MiB\n        Free                              : 0 MiB\n    GPU Power Readings\n        Power Draw                        : 8.97 W\n        Current Power Limit               : 150.00 W\n        Requested Power Limit             : 150.00 W\n        Default Power Limit               : 150.00 W\n        Min Power Limit                   : 100.00 W\n        Max Power Limit                   : 150.00 W\n    Power Samples\n        Duration                          : 117.96 sec\n        Number of Samples                 : 119\n        Max                               : 16.00 W\n        Min                               : 9.12 W\n        Avg                               : 9.74 W\n    Module Power Readings\n        Power Draw                        : N/A\n        Current Power Limit               : N/A\n        Requested Power Limit             : N/A\n        Default Power Limit               : N/A\n        Min Power Limit                   : N/A\n        Max Power Limit                   : N/A\n</code></pre>"},{"location":"education/linux-usage/using-the-nvidia-bug-report.log-file-to-troubleshoot-your-system/#contact-lambda","title":"Contact Lambda","text":"<p>If you can\u2019t discover the cause for the issue you are experiencing, contact Lambda Support and generate then upload the Lambda bug report, which includes data from the NVIDIA bug report. For example:</p> <pre><code>wget -nv -O - https://raw.githubusercontent.com/lambdal-support/lambda-public-tools/main/lambda-bug-report.sh | bash -\n</code></pre> <p>Warning</p> <p>You can run this script only on Lambda workstations and On-Demand Cloud instances. Do not run it on a cluster.</p>"},{"location":"education/linux-usage/using-the-nvidia-bug-report.log-file-to-troubleshoot-your-system/#other-resources","title":"Other resources","text":"<p>NVIDIA has a wealth of resources on their processes and tools. For more information, read the following:</p> <ul> <li>NVIDIA GPU debug guidelines</li> <li>Useful nvidia-smi queries</li> <li>nvidia-smi.txt</li> </ul>"},{"location":"education/programming/virtual-environments-containers/","title":"Virtual environments and Docker containers","text":"","tags":["Virtualization"]},{"location":"education/programming/virtual-environments-containers/#what-are-virtual-environments","title":"What are virtual environments?","text":"<p>Virtual environments allow you to create and maintain development environments that are isolated from each other. Lambda recommends using either:</p> <ul> <li>Python venv</li> <li>conda</li> </ul>","tags":["Virtualization"]},{"location":"education/programming/virtual-environments-containers/#creating-a-python-virtual-environment","title":"Creating a Python virtual environment","text":"<ol> <li> <p>Create a Python virtual environment using the <code>venv</code> module by running:</p> <pre><code>python -m venv --system-site-packages NAME\n</code></pre> <p>Replace <code>NAME</code> with the name you want to give to your virtual environment.</p> <p>Note</p> <p>The command, above, creates a virtual environment that has access to Lambda Stack packages and packages installed from Ubuntu repositories.</p> <p>To create a virtual environment that doesn't have access to Lambda Stack and Ubuntu packages, omit the <code>--system-site-packages</code> option.</p> </li> <li> <p>Activate the virtual environment by running:</p> <pre><code>source NAME/bin/activate\n</code></pre> <p>Replace <code>NAME</code> with the name you gave your virtual environment in the previous step.</p> </li> </ol> <p>Python packages you install in your virtual environment are isolated from the base environment and other virtual environments.</p> <p>Note</p> <p>Locally installed packages can conflict with packages installed in virtual environments. For this reason, it's recommended to uninstall locally installed packages by running:</p> <p>To uninstall packages installed locally for your user only, run:</p> <pre><code>pip uninstall -y $(pip -v list | grep ${HOME}/.local | awk '{printf \"%s \", $1}')\n</code></pre> <p>To uninstall packages installed locally, system-wide (for all users), run:</p> <pre><code>sudo pip uninstall -y $(pip -v list | grep /usr/local | awk '{printf \"%s \", $1}')\n</code></pre> <p>Warning</p> <p>Don't run the above uninstall commands on Lambda GPU Cloud on-demand instances!</p> <p>The above uninstall commands remove all locally installed packages and, on on-demand instances, break programs including pip and JupyterLab.</p> <p>Tip</p> <p>See the Python venv module documentation to learn more about Python virtual environments.</p>","tags":["Virtualization"]},{"location":"education/programming/virtual-environments-containers/#creating-a-conda-virtual-environment","title":"Creating a conda virtual environment","text":"<p>To create a conda virtual environment:</p> <ol> <li> <p>Download the latest version of    Miniconda3    by running:</p> <pre><code>curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n</code></pre> <p>Then, install Miniconda3 by running the command:</p> <pre><code>sh Miniconda3-latest-Linux-x86_64.sh\n</code></pre> <p>Follow the installer prompts. Install Miniconda3 in the default location. Allow the installer to initialize Miniconda3.</p> </li> <li> <p>If you want to create a conda virtual environment immediately after    installing Miniconda3, you need to load the changes made to your <code>.bashrc</code>.</p> <p>You can either:</p> <ul> <li>Exit and reopen your shell (terminal).</li> <li>Run <code>source ~/.bashrc</code>.</li> </ul> <p>Tip</p> <p>For compatibility with the Python venv module, it's recommended that you disable automatic activation of the conda base environment by running:</p> <pre><code>conda config --set auto_activate_base false\n</code></pre> </li> <li> <p>Create a conda virtual environment using Miniconda3 by running:</p> <pre><code>conda create OPTIONS -n NAME PACKAGES\n</code></pre> <p>Replace <code>NAME</code> with the name you want to give your virtual environment.</p> <p>Replace <code>PACKAGES</code> with the list of packages you want to install in your virtual environment.</p> <p>(Optional) Replace <code>OPTIONS</code> with options for the <code>conda create</code> command. See the <code>conda create</code> documentation to learn more about available options.</p> <p>For example, to create a conda virtual environment for PyTorch\u00ae with CUDA 11.8, run the below command and follow the prompts:</p> <pre><code>conda create -c pytorch -c nvidia -n pytorch+cuda_11-8 pytorch torchvision torchaudio pytorch-cuda=11.8\n</code></pre> </li> <li> <p>Activate the conda virtual environment by running:</p> <pre><code>conda activate NAME\n</code></pre> <p>Replace <code>NAME</code> with the name of the virtual environment created in the previous step.</p> <p>For instance, to activate the example PyTorch with CUDA 11.8 virtual environment mentioned in the previous step, run:</p> <pre><code>conda activate pytorch+cuda_11-8\n</code></pre> <p>Once activated, you can test the example virtual environment is working by running:</p> <pre><code>python -c 'import torch ; print(\"\\nIs available: \", torch.cuda.is_available()) ; print(\"Pytorch CUDA Compiled version: \", torch._C._cuda_getCompiledVersion()) ; print(\"Pytorch version: \", torch.__version__) ; print(\"pytorch file: \", torch.__file__) ; num_of_gpus = torch.cuda.device_count(); print(\"Number of GPUs: \",num_of_gpus)'\n</code></pre> <p>You should see output similar to:</p> <pre><code>Is available:  True\nPytorch CUDA Compiled version:  11080\nPytorch version:  2.0.1\npytorch file:  /home/ubuntu/miniconda3/envs/pytorch+cuda_11-8/lib/python3.11/site-packages/torch/__init__.py\nNumber of GPUs:  1\n</code></pre> </li> </ol> <p>Note</p> <p>Locally installed packages can conflict with packages installed in virtual environments. For this reason, it's recommended to uninstall locally installed packages by running:</p> <p>To uninstall packages installed locally for your user only, run:</p> <pre><code>pip uninstall -y $(pip -v list | grep ${HOME}/.local | awk '{printf \"%s \", $1}')\n</code></pre> <p>To uninstall packages installed locally, system-wide (for all users), run:</p> <pre><code>sudo pip uninstall -y $(pip -v list | grep /usr/local | awk '{printf \"%s \", $1}')\n</code></pre> <p>Warning</p> <p>Don\u2019t run the above uninstall commands on Lambda GPU Cloud on-demand instances!</p> <p>The above uninstall commands remove all locally installed packages and, on on-demand instances, break programs including pip and JupyterLab.</p> <p>See the Conda documentation to learn more about how to manage conda virtual environments.</p>","tags":["Virtualization"]},{"location":"education/programming/virtual-environments-containers/#installing-docker-and-creating-a-container","title":"Installing Docker and creating a container","text":"<p>Note</p> <p>Docker and NVIDIA Container Toolkit are preinstalled on Cloud on-demand instances.</p> <p>If you're using an on-demand instance, skip step 1, below.</p> <p>To create and run a Docker container:</p> <ol> <li> <p>Install Docker and    NVIDIA Container Toolkit    by running:</p> <pre><code>sudo apt -y update &amp;&amp; sudo apt -y install docker.io nvidia-container-toolkit &amp;&amp; \\\nsudo systemctl daemon-reload &amp;&amp; \\\nsudo systemctl restart docker\n</code></pre> </li> <li> <p>Add your user to the <code>docker</code> group by running:</p> <pre><code>sudo adduser \"$(id -un)\" docker\n</code></pre> <p>Then, exit and reopen a shell (terminal) so that your user can create and run Docker containers.</p> </li> <li> <p>Locate the Docker image for the container you want to create. For example,    the    NVIDIA NGC Catalog{ .external    target=\"_blank\" }    has    images for creating TensorFlow NGC containers.</p> </li> <li> <p>Create a container from the Docker image, and run a command in the container, by running:</p> <pre><code>docker run --gpus all -it IMAGE COMMAND\n</code></pre> <p>Replace <code>IMAGE</code> with the URL to the image for the container you want to create.</p> <p>Replace <code>COMMAND</code> with the command you want to run in the container.</p> <p>For example, to create a TensorFlow NGC container and run a command to get the container's TensorFlow build information, run:</p> <pre><code>docker run --gpus all -it nvcr.io/nvidia/tensorflow:23.05-tf2-py3 python -c \"import tensorflow as tf ; sys_details = tf.sysconfig.get_build_info() ; print(sys_details)\"\n</code></pre> </li> </ol> <p>You should see output similar to the following:</p> <pre><code>================\n== TensorFlow ==\n================\n\nNVIDIA Release 23.05-tf2 (build 59341886)\nTensorFlow Version 2.12.0\n\nContainer image Copyright (c) 2023, NVIDIA CORPORATION &amp; AFFILIATES. All rights reserved.\nCopyright 2017-2023 The TensorFlow Authors.  All rights reserved.\n\nVarious files include modifications (c) NVIDIA CORPORATION &amp; AFFILIATES.  All rights reserved.\n\nThis container image and its contents are governed by the NVIDIA Deep Learning Container License.\nBy pulling and using the container, you accept the terms and conditions of this license:\nhttps://developer.nvidia.com/ngc/nvidia-deep-learning-container-license\n\nNOTE: CUDA Forward Compatibility mode ENABLED.\n  Using CUDA 12.1 driver version 530.30.02 with kernel driver version 525.85.12.\n  See https://docs.nvidia.com/deploy/cuda-compatibility/ for details.\n\nNOTE: The SHMEM allocation limit is set to the default of 64MB.  This may be\n   insufficient for TensorFlow.  NVIDIA recommends the use of the following flags:\n   docker run --gpus all --ipc=host --ulimit memlock=-1 --ulimit stack=67108864 ...\n\n2023-06-08 17:09:50.643793: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2023-06-08 17:09:50.680974: I tensorflow/core/platform/cpu_feature_guard.cc:183] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX, in other operations, rebuild TensorFlow with the appropriate compiler flags.\nOrderedDict([('cpu_compiler', '/opt/rh/devtoolset-9/root/usr/bin/gcc'), ('cuda_compute_capabilities', ['sm_52', 'sm_60', 'sm_61', 'sm_70', 'sm_75', 'sm_80', 'sm_86', 'compute_90']), ('cuda_version', '12.1'), ('cudnn_version', '8'), ('is_cuda_build', True), ('is_rocm_build', False), ('is_tensorrt_build', True)])\n</code></pre> <p>See the Docker documentation to learn more about using Docker.</p> <p>You can also check out the Lambda blog post: NVIDIA NGC Tutorial: Run A PyTorch Docker Container Using Nvidia-Container-Toolkit On Ubuntu.</p>","tags":["Virtualization"]},{"location":"education/programming/vs-code-lambda-chat/","title":"Integrating Lambda Chat into VS Code","text":"","tags":["api","llama","llm","programming"]},{"location":"education/programming/vs-code-lambda-chat/#introduction","title":"Introduction","text":"<p>Using the CodeGPT VS Code (Visual Studio Code) extension , you can integrate Lambda Chat  into VS Code. Once integrated, Lambda Chat can help explain code, find problems in code, and much more. See CodeGPT's documentation to learn more .</p> <p>For this tutorial, you need to generate a Cloud API key . You can also use an existing Cloud API key.</p>","tags":["api","llama","llm","programming"]},{"location":"education/programming/vs-code-lambda-chat/#install-the-codegpt-extension","title":"Install the CodeGPT extension","text":"<ol> <li> <p>Open VS Code.</p> </li> <li> <p>Press Ctrl + P to open Quick Open. Enter the following:</p> <pre><code>ext install DanielSanMedium.dscodegpt\n</code></pre> </li> <li> <p>Press Enter to install the CodeGPT extension.</p> </li> </ol>","tags":["api","llama","llm","programming"]},{"location":"education/programming/vs-code-lambda-chat/#configure-the-lambda-chat-integration","title":"Configure the Lambda Chat integration","text":"<ol> <li> <p>In the Primary Sidebar, click the CODEGPT button.</p> <p></p> <p>Select Select Your AI &gt; Provider &gt; Custom.</p> <p></p> <p>In the Model field, enter:</p> <pre><code>hermes-3-llama-3.1-405b-fp8\n</code></pre> </li> <li> <p>Click Set Connection.</p> <p>In the ApiKey field, enter your Cloud API key.</p> <p>In the Custom Link field, enter:</p> <pre><code>https://api.lambdalabs.com/v1/chat/completions\n</code></pre> </li> <li> <p>Click Connect.</p> <p>You should see:</p> <p></p> </li> </ol>","tags":["api","llama","llm","programming"]},{"location":"education/programming/vs-code-lambda-chat/#test-the-lambda-chat-integration","title":"Test the Lambda Chat integration","text":"<ol> <li> <p>In the menu bar, select File &gt; New Text File.</p> </li> <li> <p>Enter a snippet of broken code in the editor, for example:</p> <pre><code>for _ in range(3):\n    printt(\"Hello, World!\")\n</code></pre> </li> <li> <p>Select the code, then right-click the code. Choose CodeGPT: Find    Problems.</p> </li> <li> <p>Near the bottom of the Primary Sidebar, click Send.</p> <p></p> <p>In a few seconds, you should see output similar to:</p> <p></p> </li> </ol>","tags":["api","llama","llm","programming"]},{"location":"education/scheduling-and-orchestration/deploying-models-with-dstack/","title":"Deploying models with dstack","text":"","tags":["api","llama","llm"]},{"location":"education/scheduling-and-orchestration/deploying-models-with-dstack/#introduction","title":"Introduction","text":"<p>dstack is an alternative to Kubernetes for the orchestration of AI and ML applications. With dstack, you can use YAML configuration files to define the Lambda Public Cloud resources needed for your applications. dstack will automatically obtain those resources, that is, launch appropriate on-demand instances, and start your applications.</p> <p>In this tutorial, you'll learn how to set up dstack, and use it to deploy vLLM on a Lambda Public Cloud on-demand instance. vLLM will serve the Hermes 3 fine-tuned Llama 3.1 8B large language model (LLM).</p> <p>All of the instructions in this tutorial should be followed on your computer. This tutorial assumes you already have installed:</p> <ul> <li><code>python3</code></li> <li><code>python3-venv</code></li> <li><code>pip</code></li> <li><code>git</code></li> <li><code>curl</code></li> <li><code>jq</code></li> </ul> <p>You can install these packages by running:</p> <pre><code>sudo apt update &amp;&amp; sudo apt install -y python3 python3-venv pip git curl jq\n</code></pre>","tags":["api","llama","llm"]},{"location":"education/scheduling-and-orchestration/deploying-models-with-dstack/#setting-up-the-dstack-server","title":"Setting up the dstack server","text":"<p>To set up the dstack server:</p> <ol> <li> <p>Generate a Cloud API key from the Lambda Public Cloud    dashboard.</p> </li> <li> <p>Create a directory for this tutorial, and change into the directory by    running:</p> </li> </ol> <pre><code>mkdir ~/lambda-dstack-tutorial &amp;&amp; cd ~/lambda-dstack-tutorial\n</code></pre> <p>Then, create and activate a Python virtual environment by running:</p> <pre><code>python3 -m venv .venv &amp;&amp; source .venv/bin/activate\n</code></pre> <ol> <li>Install dstack by running:</li> </ol> <pre><code>pip install -U \"dstack[all]\"\n</code></pre> <ol> <li>Create a directory for the dstack server and change into the directory by    running:</li> </ol> <pre><code>mkdir -p -m 700 ~/.dstack/server &amp;&amp; cd ~/.dstack/server\n</code></pre> <p>In this directory, create a configuration    file named <code>config.yml</code>    with the following contents:</p> <pre><code>projects:\n- name: main\n  backends:\n    - type: lambda\n      creds:\n        type: api_key\n        api_key: API-KEY\n</code></pre> <p>Replace API-KEY with your actual Cloud API key.</p> <p>Then, start the dstack server by running:</p> <pre><code>dstack server\n</code></pre> <p>You should see output similar to:</p> <pre><code>[16:20:35] INFO     Applying ~/.dstack/server/config.yml...\n[16:20:36] INFO     The admin token is ADMIN-TOKEN\n           INFO     The dstack server 0.18.11 is running at http://127.0.0.1:3000\n</code></pre>","tags":["api","llama","llm"]},{"location":"education/scheduling-and-orchestration/deploying-models-with-dstack/#deploying-vllm-and-serving-hermes-3","title":"Deploying vLLM and serving Hermes 3","text":"<p>To deploy vLLM and serve the Hermes 3 model:</p> <ol> <li>Open another terminal. Then, change into the directory you created for this    tutorial, and activate the Python virtual environment you created earlier, by    running:</li> </ol> <pre><code>cd ~/lambda-dstack-tutorial &amp;&amp; source .venv/bin/activate\n</code></pre> <ol> <li>In this directory, create a new directory named <code>task-hermes-3-vllm</code> and    change into the new directory by running:</li> </ol> <pre><code>mkdir task-hermes-3-vllm &amp;&amp; cd task-hermes-3-vllm\n</code></pre> <p>In this new directory, create a filed named <code>.dstack.yml</code> with the following    contents:</p> <pre><code>type: task\nname: task-hermes-3-vllm\npython: \"3.10\"\nenv:\n  - MODEL=NousResearch/Hermes-3-Llama-3.1-8B\ncommands:\n  - pip install vllm\n  - vllm serve $MODEL\nports:\n  - 8000\nspot_policy: auto\nresources:\n  gpu: 40GB..80GB\n</code></pre> <p>Then, initialize and apply the configuration by running:</p> <pre><code>dstack init &amp;&amp; dstack apply\n</code></pre> <ol> <li>You'll see output similar to:</li> </ol> <pre><code> Configuration          .dstack.yml\n Project                main\n User                   admin\n Pool                   default-pool\n Min resources          2..xCPU, 8GB.., 1xA100, 100GB.. (disk)\n Max price              -\n Max duration           72h\n Spot policy            auto\n Retry policy           no\n Creation policy        reuse-or-create\n Termination policy     destroy-after-idle\n Termination idle time  5m\n\n #  BACKEND  REGION      INSTANCE          RESOURCES                                     SPOT  PRICE\n 1  lambda   us-south-1  gpu_1x_a100       30xCPU, 215GB, 1xA100 (40GB), 549.9GB (disk)  no    $1.29\n 2  lambda   us-west-2   gpu_1x_a100_sxm4  30xCPU, 215GB, 1xA100 (40GB), 549.9GB (disk)  no    $1.29\n 3  lambda   us-east-1   gpu_1x_a100_sxm4  30xCPU, 215GB, 1xA100 (40GB), 549.9GB (disk)  no    $1.29\n    ...\n Shown 3 of 20 offers, $1.29 max\n\nSubmit the run task-vllm-hermes-3? [y/n]:\n</code></pre> <p>Press Y then Enter to submit the run. The run will take several    minutes to complete.</p> <p>dstack will automatically:</p> <ul> <li>Launch an instance with between 40GB and 80GB of VRAM</li> <li>Install vLLM and its dependencies using pip</li> <li>Download the Hermes 3 model</li> <li>Start vLLM and serve the Hermes 3 model</li> </ul> <p>You're billed for all of the time the instance is    running.</p> <p>In the Lambda Public Cloud    dashboard, you can see the instance    launching.</p> <p>vLLM is running and serving the Hermes 3 model once you see output similar    to:</p> <pre><code>INFO:     Started server process [57]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n</code></pre> <ol> <li>In another terminal, test that vLLM is serving the Hermes 3 model by running:</li> </ol> <pre><code>curl -X POST http://localhost:8000/v1/completions \\\n     -H \"Content-Type: application/json\" \\\n     -d \"{\n           \\\"prompt\\\": \\\"What is the name of the capital of France?\\\",\n           \\\"model\\\": \\\"NousResearch/Hermes-3-Llama-3.1-8B\\\",\n           \\\"temperature\\\": 0.0,\n           \\\"max_tokens\\\": 1\n         }\" | jq .\n</code></pre> <p>You should see output similar to:</p> <pre><code>{\n  \"id\": \"cmpl-250265add39147f58e7dba91ab244c8b\",\n  \"object\": \"text_completion\",\n  \"created\": 1724824435,\n  \"model\": \"NousResearch/Hermes-3-Llama-3.1-8B\",\n  \"choices\": [\n    {\n      \"index\": 0,\n      \"text\": \" Paris\",\n      \"logprobs\": null,\n      \"finish_reason\": \"length\",\n      \"stop_reason\": null,\n      \"prompt_logprobs\": null\n    }\n  ],\n  \"usage\": {\n    \"prompt_tokens\": 11,\n    \"total_tokens\": 12,\n    \"completion_tokens\": 1\n  }\n}\n</code></pre> <ol> <li>To quit vLLM and terminate the instance that was launched:</li> </ol> <p>In the previous terminal, that is, the terminal you used to run <code>dstack    apply</code>, press Ctrl + C. You'll be asked if you want to stop the run    before detaching. Press Y then Enter. You'll see <code>Stopped</code> once the    run is stopped.</p> <p>After 5 minutes, the instance will terminate. Alternatively, you can delete    the instance immediately by running:</p> <pre><code>dstack fleet delete task-hermes-3-vllm\n</code></pre> <p>You'll be asked for confirmation that you want to delete the fleet, that is,    the instance launched for this tutorial. Press Y then press Enter.</p> <p>Using the Lambda Public Cloud dashboard, you can confirm that the instance    was terminated.</p> <ol> <li>To shut down the dstack server, press Ctrl + C in the terminal     running the server.</li> </ol>","tags":["api","llama","llm"]},{"location":"education/scheduling-and-orchestration/skypilot-deploy-kubernetes/","title":"Using SkyPilot to deploy a Kubernetes cluster","text":"<p>Note</p> <p>Apply for Cloud Credits and experiment with this tutorial\u2014for free!</p>","tags":["api","kubernetes"]},{"location":"education/scheduling-and-orchestration/skypilot-deploy-kubernetes/#using-skypilot-to-deploy-a-kubernetes-cluster","title":"Using SkyPilot to deploy a Kubernetes cluster","text":"","tags":["api","kubernetes"]},{"location":"education/scheduling-and-orchestration/skypilot-deploy-kubernetes/#introduction","title":"Introduction","text":"<p>SkyPilot  makes it easy to deploy a Kubernetes cluster using Lambda Public Cloud  on-demand instances. The NVIDIA GPU Operator  is preinstalled so you can immediately use your instances' GPUs.</p> <p>In this tutorial, you'll:</p> <ul> <li>Configure your Lambda Public Cloud Firewall and a Cloud API key for SkyPilot   and   Kubernetes.</li> <li>Install SkyPilot.</li> <li>Configure SkyPilot for Lambda Public   Cloud.</li> <li>Use SkyPilot to launch 2 1x A10 on-demand instances and deploy a 2-node   Kubernetes cluster using these   instances.</li> </ul> <p>Note</p> <p>You're billed for all of the time the instances are running.</p> <p>All of the instructions in this tutorial should be followed on your computer.</p> <p>This tutorial assumes you already have installed:</p> <ul> <li><code>python3</code></li> <li><code>python3-venv</code></li> <li><code>python3-pip</code></li> <li><code>curl</code></li> <li><code>netcat</code></li> <li><code>socat</code></li> </ul> <p>You can install these packages by running:</p> <pre><code>sudo apt update &amp;&amp; sudo apt install -y python3 python3-venv python3-pip curl netcat socat\n</code></pre> <p>You also need to install kubectl  by running:</p> <pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\" &amp;&amp; \\\nsudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl\n</code></pre>","tags":["api","kubernetes"]},{"location":"education/scheduling-and-orchestration/skypilot-deploy-kubernetes/#configure-your-lambda-public-cloud-firewall-and-generate-a-cloud-api-key","title":"Configure your Lambda Public Cloud Firewall and generate a Cloud API key","text":"<p>Use the Lambda Public Cloud Firewall feature to add rules allowing incoming traffic to ports TCP/443 and TCP/6443.</p> <p>Generate a Cloud API key for SkyPilot. You can also use an existing Cloud API key.</p>","tags":["api","kubernetes"]},{"location":"education/scheduling-and-orchestration/skypilot-deploy-kubernetes/#install-skypilot","title":"Install SkyPilot","text":"<p>Create a directory for this tutorial and change into the directory by running:</p> <pre><code>mkdir ~/skypilot-tutorial &amp;&amp; cd ~/skypilot-tutorial\n</code></pre> <p>Create and activate a Python virtual environment for this tutorial by running:</p> <pre><code>python3 -m venv ~/skypilot-tutorial/.venv &amp;&amp; source ~/skypilot-tutorial/.venv/bin/activate\n</code></pre> <p>Then, install SkyPilot in your virtual environment by running:</p> <pre><code>pip3 install \"skypilot-nightly[lambda,kubernetes]\"\n</code></pre>","tags":["api","kubernetes"]},{"location":"education/scheduling-and-orchestration/skypilot-deploy-kubernetes/#configure-skypilot-for-lambda-public-cloud","title":"Configure SkyPilot for Lambda Public Cloud","text":"<p>Download the SkyPilot example <code>cloud_k8s.yaml</code>  and <code>launch_k8s.sh</code>  files by running:</p> <pre><code>curl -LO https://raw.githubusercontent.com/skypilot-org/skypilot/master/examples/k8s_cloud_deploy/cloud_k8s.yaml &amp;&amp; \\\ncurl -LO https://raw.githubusercontent.com/skypilot-org/skypilot/master/examples/k8s_cloud_deploy/launch_k8s.sh\n</code></pre> <p>Edit the <code>cloud_k8s.yaml</code> file.</p> <p>At the top of the file, for<code>SKY_K3S_TOKEN</code>, replace mytoken with a strong passphrase.</p> <p>Warning</p> <p>It's important that you use a strong passphrase. Otherwise, the Kubernetes cluster can be compromised, especially if your firewall rules allow incoming traffic from all sources.</p> <p>You can generate a strong passphrase by running:</p> <pre><code>openssl rand -base64 16\n</code></pre> <p>This command will generate a random string of characters such as <code>zPUlZGe4HRcy+Om04RvGmQ==</code>.</p> <p>The top of the <code>deploy_k8s.yaml</code> file should look similar to:</p> <pre><code>resources:\n  cloud: lambda\n  accelerators: A10:1\n#  Uncomment the following line to expose ports on a different cloud\n#  ports: 6443\n\nnum_nodes: 2\n\nenvs:\n  SKY_K3S_TOKEN: zPUlZGe4HRcy+Om04RvGmQ== # Can be any string, used to join worker nodes to the cluster\n</code></pre> <p>Note</p> <p>You can set <code>accelerators</code> to a different instance type, for example, <code>A100:8</code> for an 8x A100 instance or <code>H100:8</code> for an 8x H100 instance.</p> <p>Create a directory in your home directory named <code>.lambda_cloud</code> and change into that directory by running:</p> <pre><code>mkdir -m 700 ~/.lambda_cloud &amp;&amp; cd ~/.lambda_cloud\n</code></pre> <p>Create a file named <code>lambda_keys</code> that contains:</p> <pre><code>api_key = API-KEY\n</code></pre> <p>Tip</p> <p>You can do this by running:</p> <pre><code>echo \"api_key = API-KEY\" &gt; lambda_keys\n</code></pre> <p>Replace API-KEY with your actual Cloud API key.</p>","tags":["api","kubernetes"]},{"location":"education/scheduling-and-orchestration/skypilot-deploy-kubernetes/#use-skypilot-to-launch-instances-and-deploy-kubernetes","title":"Use SkyPilot to launch instances and deploy Kubernetes","text":"<p>Change into the directory you created for this tutorial by running:</p> <pre><code>cd ~/skypilot-tutorial\n</code></pre> <p>Then, launch 2 1x A10 instances and deploy a 2-node Kubernetes cluster using those instances by running:</p> <pre><code>bash launch_k8s.sh\n</code></pre> <p>You'll begin to see output similar to:</p> <pre><code>===== SkyPilot Kubernetes cluster deployment script =====\nThis script will deploy a Kubernetes cluster on the cloud and GPUs specified in cloud_k8s.yaml.\n\n+ CLUSTER_NAME=k8s\n+ sky launch -y -c k8s cloud_k8s.yaml\nSkyPilot collects usage data to improve its services. `setup` and `run` commands are not collected to ensure privacy.\nUsage logging can be disabled by setting the environment variable SKYPILOT_DISABLE_USAGE_COLLECTION=1.\nTask from YAML spec: cloud_k8s.yaml\nI 09-11 16:10:04 optimizer.py:719] == Optimizer ==\nI 09-11 16:10:04 optimizer.py:730] Target: minimizing cost\nI 09-11 16:10:04 optimizer.py:742] Estimated cost: $1.5 / hour\nI 09-11 16:10:04 optimizer.py:742]\nI 09-11 16:10:04 optimizer.py:867] Considered resources (2 nodes):\nI 09-11 16:10:04 optimizer.py:937] ------------------------------------------------------------------------------------------\nI 09-11 16:10:04 optimizer.py:937]  CLOUD    INSTANCE     vCPUs   Mem(GB)   ACCELERATORS   REGION/ZONE   COST ($)   CHOSEN\nI 09-11 16:10:04 optimizer.py:937] ------------------------------------------------------------------------------------------\nI 09-11 16:10:04 optimizer.py:937]  Lambda   gpu_1x_a10   30      200       A10:1          us-east-1     1.50          \u2714\nI 09-11 16:10:04 optimizer.py:937] ------------------------------------------------------------------------------------------\nI 09-11 16:10:04 optimizer.py:937]\nRunning task on cluster k8s...\nI 09-11 16:10:04 cloud_vm_ray_backend.py:4397] Creating a new cluster: 'k8s' [2x Lambda(gpu_1x_a10, {'A10': 1})].\nI 09-11 16:10:04 cloud_vm_ray_backend.py:4397] Tip: to reuse an existing cluster, specify --cluster (-c). Run `sky status` to see existing clusters.\nI 09-11 16:10:05 cloud_vm_ray_backend.py:1314] To view detailed progress: tail -n100 -f /home/lambda/sky_logs/sky-2024-09-11-16-10-03-504822/provision.log\nI 09-11 16:10:06 cloud_vm_ray_backend.py:1721] Launching on Lambda us-east-1\nI 09-11 16:13:24 log_utils.py:45] Head node is up.\nI 09-11 16:14:10 cloud_vm_ray_backend.py:1826] Successfully provisioned or found existing head instance. Waiting for workers.\nI 09-11 16:18:13 cloud_vm_ray_backend.py:1569] Successfully provisioned or found existing VMs.\nI 09-11 16:18:17 cloud_vm_ray_backend.py:3319] Job submitted with Job ID: 1\n</code></pre> <p>It usually takes about 15 minutes for the Kubernetes cluster to be deployed.</p> <p>The Kubernetes cluster is successfully deployed once you see:</p> <pre><code>Checking credentials to enable clouds for SkyPilot.\n  Kubernetes: enabled\n    Hint: Could not detect GPU labels in Kubernetes cluster. If this cluster has GPUs, please ensure GPU nodes have node labels of either of these formats: skypilot.co/accelerator, cloud.google.com/gke-accelerator, karpenter.k8s.aws/instance-gpu-name, nvidia.com/gpu.product, gpu.nvidia.com/class. Please refer to the documentation on how to set up node labels.\n\nTo enable a cloud, follow the hints above and rerun: sky check\nIf any problems remain, refer to detailed docs at: https://skypilot.readthedocs.io/en/latest/getting-started/installation.html\n\n\ud83c\udf89 Enabled clouds \ud83c\udf89\n  \u2714 Kubernetes\n  \u2714 Lambda\n+ set +x\n===== Kubernetes cluster deployment complete =====\nYou can now access your k8s cluster with kubectl and skypilot.\n\n\u2022 View the list of available GPUs on Kubernetes: sky show-gpus --cloud kubernetes\n\u2022 To launch a SkyPilot job running nvidia-smi on this cluster: sky launch --cloud kubernetes --gpus &lt;GPU&gt; -- nvidia-smi\n</code></pre> <p>To test the Kubernetes cluster, launch a job  by running:</p> <pre><code>sky jobs launch --gpus A10 --cloud kubernetes -- 'nvidia-smi'\n</code></pre> <p>You'll see output similar to the following and will be asked if you want to proceed:</p> <pre><code>Task from command: nvidia-smi\nManaged job 'sky-cmd' will be launched on (estimated):\nI 09-07 16:26:18 optimizer.py:718] == Optimizer ==\nI 09-07 16:26:18 optimizer.py:741] Estimated cost: $0.0 / hour\nI 09-07 16:26:18 optimizer.py:741]\nI 09-07 16:26:18 optimizer.py:866] Considered resources (1 node):\nI 09-07 16:26:18 optimizer.py:936] ---------------------------------------------------------------------------------------------------\nI 09-07 16:26:18 optimizer.py:936]  CLOUD        INSTANCE          vCPUs   Mem(GB)   ACCELERATORS   REGION/ZONE   COST ($)   CHOSEN\nI 09-07 16:26:18 optimizer.py:936] ---------------------------------------------------------------------------------------------------\nI 09-07 16:26:18 optimizer.py:936]  Kubernetes   2CPU--8GB--1A10   2       8         A10:1          kubernetes    0.00          \u2714\nI 09-07 16:26:18 optimizer.py:936] ---------------------------------------------------------------------------------------------------\nI 09-07 16:26:18 optimizer.py:936]\nLaunching a managed job 'sky-cmd'. Proceed? [Y/n]:\n</code></pre> <p>Press Enter to proceed.</p> <p>You should see output similar to the following, indicating the job ran successfully:</p> <pre><code>Launching managed job 'sky-cmd' from jobs controller...\nLaunching jobs controller...\nI 09-07 16:26:25 optimizer.py:718] == Optimizer ==\nI 09-07 16:26:25 optimizer.py:741] Estimated cost: $0.0 / hour\nI 09-07 16:26:25 optimizer.py:741]\nI 09-07 16:26:25 optimizer.py:866] Considered resources (1 node):\nI 09-07 16:26:25 optimizer.py:936] ----------------------------------------------------------------------------------------------\nI 09-07 16:26:25 optimizer.py:936]  CLOUD        INSTANCE     vCPUs   Mem(GB)   ACCELERATORS   REGION/ZONE   COST ($)   CHOSEN\nI 09-07 16:26:25 optimizer.py:936] ----------------------------------------------------------------------------------------------\nI 09-07 16:26:25 optimizer.py:936]  Kubernetes   8CPU--24GB   8       24        -              kubernetes    0.00          \u2714\nI 09-07 16:26:25 optimizer.py:936] ----------------------------------------------------------------------------------------------\nI 09-07 16:26:25 optimizer.py:936]\nI 09-07 16:26:25 cloud_vm_ray_backend.py:4354] Creating a new cluster: 'sky-jobs-controller-0b36a124' [1x Kubernetes(8CPU--24GB, cpus=8+, mem=3x, disk_size=50)].\nI 09-07 16:26:25 cloud_vm_ray_backend.py:4354] Tip: to reuse an existing cluster, specify --cluster (-c). Run `sky status` to see existing clusters.\nI 09-07 16:26:25 cloud_vm_ray_backend.py:1314] To view detailed progress: tail -n100 -f /home/c/sky_logs/sky-2024-09-07-16-26-24-870809/provision.log\nI 09-07 16:26:25 common.py:228] Updated Kubernetes catalog.\nI 09-07 16:26:25 provisioner.py:62] Launching on Kubernetes 'sky-jobs-controller-0b36a124'.\nI 09-07 16:26:47 provisioner.py:450] Successfully provisioned or found existing instance.\nI 09-07 16:27:10 provisioner.py:552] Successfully provisioned cluster: sky-jobs-controller-0b36a124\nI 09-07 16:27:10 cloud_vm_ray_backend.py:4383] Processing file mounts.\nI 09-07 16:27:10 cloud_vm_ray_backend.py:4409] To view detailed progress: tail -n100 -f ~/sky_logs/sky-2024-09-07-16-26-24-870809/file_mounts.log\nI 09-07 16:27:10 backend_utils.py:1336] Syncing (to 1 node): /tmp/managed-dag-sky-cmd-0gu861ix -&gt; ~/.sky/managed_jobs/sky-cmd-1b6c.yaml\nI 09-07 16:27:13 cloud_vm_ray_backend.py:3176] Running setup on 1 node.\nCheck &amp; install cloud dependencies on controller: Done for 1 clouds.\nI 09-07 16:27:16 cloud_vm_ray_backend.py:3189] Setup completed.\nI 09-07 16:27:16 cloud_vm_ray_backend.py:4109] Auto-stop is not supported for Kubernetes and RunPod clusters. Skipping.\nI 09-07 16:27:20 cloud_vm_ray_backend.py:3276] Job submitted with Job ID: 1\nI 09-07 23:28:46 log_lib.py:412] Start streaming logs for managed job 1.\nINFO: Tip: use Ctrl-C to exit log streaming (task will not be killed).\nINFO: Waiting for task resources on 1 node. This will block if the cluster is full.\nINFO: All task resources reserved.\nINFO: Reserved IPs: ['10.42.1.16']\n(sky-cmd, pid=1504) Sat Sep  7 23:28:31 2024\n(sky-cmd, pid=1504) +---------------------------------------------------------------------------------------+\n(sky-cmd, pid=1504) | NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n(sky-cmd, pid=1504) |-----------------------------------------+----------------------+----------------------+\n(sky-cmd, pid=1504) | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n(sky-cmd, pid=1504) | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n(sky-cmd, pid=1504) |                                         |                      |               MIG M. |\n(sky-cmd, pid=1504) |=========================================+======================+======================|\n(sky-cmd, pid=1504) |   0  NVIDIA A10                     On  | 00000000:07:00.0 Off |                    0 |\n(sky-cmd, pid=1504) |  0%   30C    P8              22W / 150W |      4MiB / 23028MiB |      0%      Default |\n(sky-cmd, pid=1504) |                                         |                      |                  N/A |\n(sky-cmd, pid=1504) +-----------------------------------------+----------------------+----------------------+\n(sky-cmd, pid=1504)\n(sky-cmd, pid=1504) +---------------------------------------------------------------------------------------+\n(sky-cmd, pid=1504) | Processes:                                                                            |\n(sky-cmd, pid=1504) |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n(sky-cmd, pid=1504) |        ID   ID                                                             Usage      |\n(sky-cmd, pid=1504) |=======================================================================================|\n(sky-cmd, pid=1504) |  No running processes found                                                           |\n(sky-cmd, pid=1504) +---------------------------------------------------------------------------------------+\nI 09-07 23:28:54 utils.py:447] Logs finished for job 1 (status: SUCCEEDED).\n\nI 09-07 16:28:54 cloud_vm_ray_backend.py:3292] Managed Job ID: 1\nI 09-07 16:28:54 cloud_vm_ray_backend.py:3292] To cancel the job:       sky jobs cancel 1\nI 09-07 16:28:54 cloud_vm_ray_backend.py:3292] To stream job logs:      sky jobs logs 1\nI 09-07 16:28:54 cloud_vm_ray_backend.py:3292] To stream controller logs:   sky jobs logs --controller 1\nI 09-07 16:28:54 cloud_vm_ray_backend.py:3292] To view all managed jobs:    sky jobs queue\nI 09-07 16:28:54 cloud_vm_ray_backend.py:3292] To view managed job dashboard:   sky jobs dashboard\n</code></pre>","tags":["api","kubernetes"]},{"location":"hardware/","title":"Introduction","text":"<p>Getting started and troubleshooting instructions for Lambda Servers and Vector workstations.</p> <ul> <li> <p>Servers</p> <p>Find user manuals and common answers about Lambda servers.</p> <p> Getting Started</p> </li> <li> <p>Workstations</p> <p>Find user manuals and common answers for Vector workstations.</p> <p> Getting Started</p> </li> </ul>"},{"location":"hardware/servers/getting-started/","title":"Getting started","text":"","tags":["scalar"]},{"location":"hardware/servers/getting-started/#where-can-i-download-the-user-manual-for-my-server-chassis","title":"Where can I download the user manual for my server chassis?","text":"<p>User manuals for Lambda server chassis can be downloaded below.</p> <p>Tip</p> <p>You can run <code>sudo dmidecode -t 1</code> to know your server chassis. The command will output, for example:</p> <pre><code># dmidecode 3.2\nGetting SMBIOS data from sysfs.\nSMBIOS 3.3.0 present.\n# SMBIOS implementations newer than version 3.2.0 are not\n# fully supported by this version of dmidecode.\nHandle 0x0001, DMI type 1, 27 bytes\nSystem Information\nManufacturer: Supermicro\nProduct Name: AS -1114CS-TNR\nVersion: 0123456789\nSerial Number: S452392X2826686\nUUID: 51605a00-c54f-11ec-8000-3cecefcdb48b\nWake-up Type: Power Switch\nSKU Number: To be filled by O.E.M.\nFamily: To be filled by O.E.M.\n</code></pre> <p><code>Product Name</code> is the server chassis. In this example, the server chassis is AS -1114CS-TNR.</p>","tags":["scalar"]},{"location":"hardware/servers/getting-started/#supermicro","title":"Supermicro","text":"Chassis User manual SuperServer 420GP-TNR Download AS -4124GO-NART Download AS -1124US-TNRP Download AS -1114CS-TNR Download AS -2124GQ-NART Download AS -4023S-TRT Download SuperServer 4029GP-TRT2 Download SuperWorkstation 740GP-TNRT Download AS -2124US-TNRP Download SuperServer 420GP-TNAR Download","tags":["scalar"]},{"location":"hardware/servers/getting-started/#gigabyte","title":"GIGABYTE","text":"Chassis User manual R152-Z33 Download G292-Z43 Download G292-Z24 Download G292-Z22 Download G292-Z42 Download G292-280 Download","tags":["scalar"]},{"location":"hardware/servers/getting-started/#where-do-i-find-my-servers-serial-number","title":"Where do I find my server's serial number?","text":"<p>On the Supermicro server chassis, the serial number is on a service tag located at the front-bottom of the chassis.</p> Serial number on the service tag of a Supermicro server chassis <p>On GIGABYTE 2U server chassis, the serial number is on a sticker on the right side of the chassis.</p> <p>On GIGABYTE 4U server chassis, the serial number is on a sticker on the left side of the chassis.</p> Serial number on a sticker on the left side of a GIGABYTE 4U server chassis","tags":["scalar"]},{"location":"hardware/servers/getting-started/#where-can-i-find-my-servers-ipmi-bmc-password","title":"Where can I find my server's IPMI (BMC) password?","text":"<p>Tip</p> <p>You can choose your own IPMI password for your server from within Ubuntu:</p> <ol> <li>Run <code>sudo apt-get install ipmitool</code> to install <code>ipmitool</code>, which is a program for managing IPMI functions.</li> <li>Run <code>ipmitool user list 1</code> to view the user list. Confirm that <code>ID 2</code> is <code>admin</code> or <code>ADMIN</code>.</li> <li>Run <code>ipmitool user set password 2</code> to set a new IPMI password.</li> </ol>","tags":["scalar"]},{"location":"hardware/servers/getting-started/#supermicro_1","title":"Supermicro","text":"<p>On Supermicro chassis, the IPMI password appears on one or more of the following:</p> <ul> <li>A sticker near the BMC (Baseboard Management Controller).</li> <li>A sticker near the motherboard serial number label.</li> <li>The service tag.</li> </ul> <p>See Supermicro's BMC Unique Password Guide [PDF] for more information.</p>","tags":["scalar"]},{"location":"hardware/servers/getting-started/#gigabyte_1","title":"GIGABYTE","text":"<p>On GIGABYTE chassis, the IPMI password appears on one or more of the following:</p> <ul> <li>A sticker affixed to the motherboard.</li> <li>The chassis itself.</li> </ul> <p>See GIGABYTE's BMC Unique Pre-Programmed Password Reference Guide [PDF] for more information.</p>","tags":["scalar"]},{"location":"hardware/servers/getting-started/#what-are-the-power-requirements-for-my-servers-psus","title":"What are the power requirements for my server's PSUs?","text":"<p>The power requirements for Lambda server power supply units (PSUs) are as follows:</p> Manufacturer Model Wattage Voltage (AC) Current (A) Frequency (Hz) Inlet/Outlet Efficiency Supermicro PWS-2K05A-1R 1000 100-127 12-9.5 60/50 C14/C13 80 Plus Titanium (&gt; 96%) 1800 200-220 10-9.5 60/50 1980 220-230 10-9.8 60/50 2000 230-240 10-9.8 60/50 Supermicro PWS-2K08A-1R 1800 200-220 10-9.8 60/50 C14/C13 80 Plus Titanium (&gt; 96%) 1980 220-230 10-9.8 60/50 2000 230-240 10-9.8 60/50 Supermicro PWS-3K02G-2R 2880 200-207 16-15.7 60/50 C20/C19 80 Plus Titanium (&gt; 96%) 3000 207.1-240 16-14.5 60/50 Supermicro PWS-3K06G-2R 3000 207.1-240 16-14.5 60/50 C20/C19 <p>\\ Power connector inlets and outlets</p> <p>This is what C14 inlets and C13 outlets look like:</p> Power cord with C14 and C13 connectors <p>This is what C20 inlets and C19 outlets look like:</p> Power cord with C20 and C19 connectors","tags":["scalar"]},{"location":"hardware/workstations/getting-started/","title":"Getting started","text":"","tags":["vector","vector one","vector pro"]},{"location":"hardware/workstations/getting-started/#how-do-i-set-up-my-workstation","title":"How do I set up my workstation?","text":"<p>Instructions for setting up your Vector can be found in our Vector quickstart guide.</p> <p>Instructions for setting up your Vector One can be found in our Vector One quickstart guide.</p>","tags":["vector","vector one","vector pro"]},{"location":"hardware/workstations/getting-started/#what-are-the-buttons-and-ports-at-the-front-and-top-of-my-vector-one","title":"What are the buttons and ports at the front and top of my Vector One?","text":"Vector One front and top buttons and ports <p>Your Vector One's power button is located at the front-top.</p> <p>The first button at the top, closest to the front, switches between the various RGB modes. The second button at the top changes the color.</p> <p>The first port at the top, closest to the front, is used to connect USB-C 3.1 devices. The following 2 ports are used to connect USB-A 3.0 devices.</p> <p>The jack at the top is used to connect a headset/microphone.</p>","tags":["vector","vector one","vector pro"]},{"location":"hardware/workstations/getting-started/#how-do-i-fix-wi-fi-issues-with-my-vector-one","title":"How do I fix Wi-Fi issues with my Vector One?","text":"<p>There are known issues in Ubuntu with the Wi-Fi adapter  installed in Vector Ones. In some cases, the Wi-Fi adapter isn't detected at all. In other cases, the Wi-Fi adapter is detected but exhibits slow performance. These issues are fixed in updated firmware for the Wi-Fi adapter.</p> <p>In order to download and install the updated firmware, you need to connect your Vector One to the Internet.</p> <p>If your Wi-Fi adapter isn't detected at all, try booting using a previous kernel version. Your Wi-Fi adapter might be detected and you can download the updated firmware.</p> <p>Note</p> <p>You can also connect your Vector One to the Internet using Ethernet (recommended), a USB Wi-Fi adapter, or by tethering your iPhone or Android phone.</p> <p>Once your Vector One is connected to the Internet, open a terminal and run:</p> <pre><code>sudo apt update &amp;&amp; sudo apt upgrade -y\n</code></pre> <p>Then, reboot your Vector One.</p>","tags":["vector","vector one","vector pro"]},{"location":"hardware/workstations/getting-started/#where-can-i-download-recovery-images-for-my-workstation","title":"Where can I download recovery images for my workstation?","text":"<p>Workstation recovery images can be downloaded from our Lambda Stack and recovery images docs page.</p>","tags":["vector","vector one","vector pro"]},{"location":"hardware/workstations/getting-started/#can-i-dual-boot-ubuntu-and-windows-11","title":"Can I dual boot Ubuntu and Windows 11?","text":"<p>You can't dual boot Ubuntu and Windows 11. You can have either Ubuntu or Windows installed, but not both at the same time on your workstation.</p> <p>Windows 11 requires Secure Boot and TPM 2.0 to be enabled, which can prevent Ubuntu from booting or detecting your workstation's GPUs.</p>","tags":["vector","vector one","vector pro"]},{"location":"hardware/workstations/getting-started/#how-do-i-set-the-fan-speeds-for-my-workstation","title":"How do I set the fan speeds for my workstation?","text":"<p>You can set baseline fan speeds for your workstation using <code>ipmitool</code>. Once baseline fan speeds are set, you can fine-tune the fan speeds in the web-based IPMI interface.</p> <p>Note</p> <p>These instructions are only for workstations using an ASUS Pro WS WRX80E-SAGE SE WIFI motherboard.</p> <p>Before proceeding with these instructions, run <code>sudo dmidecode -t 2 | grep Name</code> to confirm your workstation uses the above motherboard. You should see: <code>Product Name: Pro WS WRX80E-SAGE SE</code>.</p> <p>First, install <code>ipmitool</code> by running:</p> <pre><code>sudo apt -y update &amp;&amp; sudo apt -y install ipmitool\n</code></pre> <p>Then, set the baseline fan speeds by running:</p> <pre><code>sudo ipmitool raw 0x30 0x0E 0x04 0x00 0x32 0x23 0x49 0x46 0x5a 0x64 0x61 0x64 0x61 0x64 &amp;&amp; \\\nsudo ipmitool raw 0x30 0x0E 0x04 0x01 0x32 0x23 0x49 0x46 0x5a 0x64 0x61 0x64 0x61 0x64 &amp;&amp; \\\nsudo ipmitool raw 0x30 0x0E 0x04 0x02 0x32 0x23 0x49 0x46 0x5a 0x64 0x61 0x64 0x61 0x64 &amp;&amp; \\\nsudo ipmitool raw 0x30 0x0E 0x04 0x03 0x32 0x23 0x49 0x46 0x5a 0x64 0x61 0x64 0x61 0x64 &amp;&amp; \\\nsudo ipmitool raw 0x30 0x0E 0x04 0x04 0x32 0x23 0x49 0x46 0x5a 0x64 0x61 0x64 0x61 0x64 &amp;&amp; \\\nsudo ipmitool raw 0x30 0x0E 0x04 0x05 0x32 0x23 0x49 0x46 0x5a 0x64 0x61 0x64 0x61 0x64 &amp;&amp; \\\nsudo ipmitool raw 0x30 0x0E 0x04 0x06 0x32 0x23 0x49 0x46 0x5a 0x64 0x61 0x64 0x61 0x64\n</code></pre> <p>Tip</p> <p>See the  ASUS ASMB9-iKVM Fan Customized Mode User Guide to learn how to customize fan speeds in the web-based IPMI interface.</p> <p>Note that Lambda workstations are high-performance systems and generate plenty of heat. For this reason, it's not recommended to use the guide's power efficiency fan policy.</p>","tags":["vector","vector one","vector pro"]},{"location":"hardware/workstations/getting-started/#what-are-the-power-requirements-for-my-workstations-psu","title":"What are the power requirements for my workstation's PSU?","text":"<p>Warning</p> <p>Lambda workstations are high-performance systems and use a large amount of power. For this reason, Lambda workstations can't reliably be used with uninterruptible power supplies (UPSs, or battery backups).</p> <p>If you use a UPS with your workstation, you might experience system instability and trouble booting.</p> <p>The power requirements for Lambda workstation power supply units (PSUs) are as follows:</p> <p>Note</p> <p>The manufacturer and model of your workstation\u2019s PSU appears on the label on the PSU.</p> Manufacturer Model Wattage Voltage (AC) Current (A) Frequency (Hz) Inlet/Outlet Super Flower SF-1300F14MG V1.0 1300 100-240 15 60/50 C14/C13 Super Flower SF-1600F14HT 1600 115-240 17-10 60/50 C20/C19 Super Flower SF-2000F14HP 2000 200-240 15 50 C20/C19","tags":["vector","vector one","vector pro"]},{"location":"hardware/workstations/getting-started/#how-do-i-upgrade-my-samsung-980-pro-nvme-ssds-firmware","title":"How do I upgrade my Samsung 980 PRO NVMe SSD's firmware?","text":"<p>Follow these instructions to upgrade your Samsung 980 PRO NVMe SSD's firmware.</p> <p>Warning</p> <p>Samsung 980 PRO NVMe SSDs with the older 3B2QGXA7 firmware are known to fail .</p> <p>To know if your SSD is using the 3B2QGXA7 firmware, install the <code>smartmontools</code> package by running <code>sudo apt -y install smartmontools</code>. Then, run <code>sudo smartctl -a /dev/nvme0</code>.</p> <p>If your SSD is using the 3B2QGXA7 firmware, it's recommended that you upgrade the firmware as soon as possible.</p> <p>First, download the latest firmware ISO from Samsung's website by running:</p> <pre><code>wget https://semiconductor.samsung.com/resources/software-resources/Samsung_SSD_980_PRO_5B2QGXA7.iso\n</code></pre> <p>Next, run <code>sudo -s</code> to open a shell with root (administrator) privileges.</p> <p>Finally, run:</p> <pre><code>mkdir /mnt/iso &amp;&amp; mount -o loop Samsung_SSD_980_PRO_5B2QGXA7.iso /mnt/iso &amp;&amp; \\\nmkdir fwupdate &amp;&amp; cd fwupdate &amp;&amp; \\\ngzip -dc /mnt/iso/initrd | cpio -idv --no-absolute-filenames &amp;&amp; \\\ncd root/fumagician &amp;&amp; ./fumagician\n</code></pre> <p>The above command mounts the firmware upgrade ISO, extracts the firmware upgrade, and launches the upgrade.</p> <p>After the firmware upgrade completes, restart your computer.</p> <p>Run <code>sudo smartctl -a /dev/nvme0</code> to confirm your SSD is using the new firmware.</p>","tags":["vector","vector one","vector pro"]},{"location":"hardware/workstations/troubleshooting/","title":"Troubleshooting Workstations and Desktops","text":"","tags":["vector","vector one","vector pro"]},{"location":"hardware/workstations/troubleshooting/#i-cant-access-the-bios-on-a-vector-desktop-with-a-wrx80-motherboard-when-its-connected-to-a-4k-monitor","title":"I can't access the BIOS on a Vector desktop with a WRX80 motherboard when it's connected to a 4K monitor.","text":"<p>If you have a Vector desktop with a WRX80 motherboard and a 4K monitor, when the system boots, the monitor should display the ASUS logo screen; however, it does not show the prompts to access the BIOS. If you press F2 to access the BIOS, the monitor continues to not display the BIOS prompts until after the system restarts. The system boots normally if you don't interact with it, but the video only displays when the Ubuntu OS boots successfully.</p> <p>You can determine whether your system has the WRX80 motherboard by running:</p> <pre><code>sudo dmidecode -t 2 | grep Name\n</code></pre> <p>Your system has a WRX80 motherboard if the command returns: <code>Product Name: Pro WS WRX80E-SAGE SE</code></p> <p>The best solution at this time is to switch to a non-4K monitor, like 1080p.</p>","tags":["vector","vector one","vector pro"]},{"location":"private-cloud/","title":"Introduction","text":"<p>Lambda Private Cloud provides bare metal, single-tenant clusters for defined reservation periods. Lambda builds the cluster to your specifications. Lambda providing 24x7 support for the cluster hardware. Access to the cluster and firewall are restricted solely to you. Lambda can additionally provide managed services, like Managed Kubernetes.</p> <ul> <li> <p>Private Cloud</p> <p>Clusters of 512-2000 NVIDIA H200 SXM or GB200 Grace Blackwell Superchip GPUs reserved for a minimum of one year up to three years.</p> <p> Contact Sales</p> </li> <li> <p>Hyperscale</p> <p>Clusters of 2,000-64,000 NVIDIA GPUs, including NVIDIA H100 Tensorcore, H200 SXM, or GB200 Grace Blackwell Superchip, for reservations between three and ten years.</p> <p> Contact Sales</p> </li> </ul>","tags":["private cloud"]},{"location":"private-cloud/managed-kubernetes/","title":"Overview","text":"<p>During the Private Cloud reservation process, you can choose to configure your cluster as a Managed Kubernetes cluster. In this configuration, Lambda manages your cluster's underlying environment, and you interact with the cluster through a browser-based Kubernetes administration UI and the Kubernetes API.</p> <p>This document outlines the standard configuration for a Managed Kubernetes cluster in Lambda Private Cloud.</p>","tags":["private cloud","managed kubernetes"]},{"location":"private-cloud/managed-kubernetes/#hardware","title":"Hardware","text":"<p>Lambda Private Cloud provides single-tenant clusters that are isolated from other clusters. The hardware details for your specific cluster depend on what you chose when reserving your cluster. Each cluster includes at least three control (CPU) nodes for cluster administration and job scheduling.</p>","tags":["private cloud","managed kubernetes"]},{"location":"private-cloud/managed-kubernetes/#software","title":"Software","text":"<p>Your Managed Kubernetes deployment is configured to use Rancher with Rancher Kubernetes Engine 2 (RKE2).</p> <ul> <li>Rancher     provides a web UI for monitoring and managing aspects of your Kubernetes     cluster. Rancher also provides your cluster's Kubenetes API server.</li> <li>RKE2 is a fully conformant     Kubernetes distribution focused on security and compliance.</li> </ul>","tags":["private cloud","managed kubernetes"]},{"location":"private-cloud/managed-kubernetes/#cluster-management","title":"Cluster management","text":"","tags":["private cloud","managed kubernetes"]},{"location":"private-cloud/managed-kubernetes/#rancher-dashboard","title":"Rancher dashboard","text":"<p>The Rancher dashboard serves as the main UI for your Managed Kubernetes cluster. After you set up your SSL VPN connection, you can access your dashboard at https://10.141.3.1. The login details for your dashboard can be found in your 1Password vault.</p> <p>For details on setting up your SSL VPN connection, see Getting started &gt; Establishing a secure connection to your cluster.</p>","tags":["private cloud","managed kubernetes"]},{"location":"private-cloud/managed-kubernetes/#kubernetes-api","title":"Kubernetes API","text":"<p>The Kubernetes API is available at <code>https://10.141.0.250:6443</code> through your SSL VPN connection. You can obtain your <code>kubeconfig</code> file from the Rancher dashboard. For details, see Getting started &gt; Accessing the Kubernetes API.</p>","tags":["private cloud","managed kubernetes"]},{"location":"private-cloud/managed-kubernetes/#storage","title":"Storage","text":"<p>In the default cluster configuration, your cluster comes with three types of storage, each with its own performance and access characteristics:</p> <ul> <li>Common (Longhorn)</li> <li>Shared workload (Intelliflash)</li> <li>Scratch/HPC (directly attached local storage)</li> </ul> <p>Each type is mapped to a corresponding storage class in Kubernetes:</p> <ul> <li><code>longhorn</code></li> <li><code>intelliflash</code></li> <li><code>local-path</code></li> </ul> <p>Note</p> <p>Each cluster also includes a <code>local-storage</code> storage class that Lambda uses to route monitoring metric replication. You can safely ignore this class\u2014<code>local-path</code> is the dynamic provisioner routed to the fast NVMe arrays.</p>","tags":["private cloud","managed kubernetes"]},{"location":"private-cloud/managed-kubernetes/#common","title":"Common","text":"<p>Longhorn is configured by default to pool the local drives on your control nodes, and is best used for everyday storage needs such as shared home directories, configuration files, code checkouts, metrics output, and so on. It provides fault tolerance through replication and is available as single or shared PVCs.</p> <p>Note</p> <p>If desired, you can extend this common pool to your compute nodes. However, the other storage tiers provide better performance for compute workloads.</p>","tags":["private cloud","managed kubernetes"]},{"location":"private-cloud/managed-kubernetes/#shared-workload","title":"Shared workload","text":"<p>Intelliflash is a dedicated NAS device with its own high availability (HA), replication, and deduplication system. It is capable of much higher performance and is uplinked into the inband network via 4x100Gb links. It is also available as single or shared PVCs.</p> <p>We recommend the Intelliflash as your primary workload storage if you require shared PVCs.</p>","tags":["private cloud","managed kubernetes"]},{"location":"private-cloud/managed-kubernetes/#scratchhpc","title":"Scratch/HPC","text":"<p>The direct-attached local storage on your compute nodes provides the highest performance of the storage tiers available in your cluster. However, this storage is limited to single attachment PVCs only\u2014that is, each pod will provision their own volume out of the local drives on that compute node and attach to only that volume.</p> <p>This direct-attached storage can be used for high-speed scratch space during jobs. Alternatively, if you construct your jobs to do their own data sharding, you can maximize performance by duplicating your whole dataset to each pod's local volume.</p>","tags":["private cloud","managed kubernetes"]},{"location":"private-cloud/managed-kubernetes/#networking","title":"Networking","text":"<p>All cluster networking is directly attached using native routing, with no overlays or tunnels.</p>","tags":["private cloud","managed kubernetes"]},{"location":"private-cloud/managed-kubernetes/#firewall-defaults","title":"Firewall defaults","text":"<p>Your cluster uses FortiGate, Fortinet's Next-Generation Firewall (NGFW), to provide network protection and remote access services. All traffic into and out of the cluster is mediated by the FortiGate firewall appliance. By default, your firewall is deployed in high availability (HA) mode with the following rules:</p> Direction Default behavior Ingress Block, unless it's an SSL VPN connection Egress Allow all traffic going out to Internet","tags":["private cloud","managed kubernetes"]},{"location":"private-cloud/managed-kubernetes/#inband","title":"Inband","text":"<p>Your Inband network is on <code>10.141.0.0/22</code>, as follows:</p> <ul> <li><code>10.141.0.1</code>: Firewall gateway</li> <li><code>10.141.0.250</code>: Kubernetes API virtual IP</li> <li><code>10.141.0.251-253</code>: Control nodes</li> <li><code>10.141.1.1-8</code>: Compute nodes</li> </ul>","tags":["private cloud","managed kubernetes"]},{"location":"private-cloud/managed-kubernetes/#infiniband","title":"Infiniband","text":"<p>The Infiniband network is meshed between the compute nodes on dedicated switches.</p>","tags":["private cloud","managed kubernetes"]},{"location":"private-cloud/managed-kubernetes/#kubernetes-networks","title":"Kubernetes networks","text":"<p>The container network is on <code>10.42.0.0/16</code>, and the service network is on <code>10.43.0.0/16</code>.</p>","tags":["private cloud","managed kubernetes"]},{"location":"private-cloud/managed-kubernetes/#load-balancer-services","title":"Load balancer services","text":"<p>The Managed Kubernetes system reserves part of your inband network for use as an IPAM-managed pool of LoadBalancer service IPs. The first three of these IPs are allocated to the following services:</p> <ul> <li><code>10.141.3.0</code>: Ingress controller</li> <li><code>https://10.141.3.1:443</code>: Rancher dashboard</li> <li><code>http://10.141.3.2:80</code>: Hubble UI</li> </ul> <p>When you apply a new LoadBalancer service, the system automatically allocates one of the remaining IPs (<code>10.141.3.4-254</code>) to the service.</p>","tags":["private cloud","managed kubernetes"]},{"location":"private-cloud/managed-kubernetes/#next-steps","title":"Next steps","text":"<p>For details on accessing your cluster, as well as a walkthrough of the Rancher dashboard, see the Managed Kubernetes getting started for Private Cloud.</p>","tags":["private cloud","managed kubernetes"]},{"location":"private-cloud/managed-kubernetes/getting-started/","title":"Getting started","text":"<p>This document explains how to access your Private Cloud cluster's Managed Kubernetes dashboard and walks you through commonly used sections of your cluster dashboard. It also provides guidance for configuring and running workloads on your Private Cloud cluster.</p> <p>For an overview of your Managed Kubernetes cluster's default specifications and configuration, see the Managed Kubernetes overview for Private Cloud.</p>","tags":["private cloud","managed kubernetes"]},{"location":"private-cloud/managed-kubernetes/getting-started/#establishing-a-secure-connection","title":"Establishing a secure connection to your cluster","text":"<p>Your cluster uses Fortigate, Fortinet's Next-Generation Firewall (NGFW), to protect your network and provide remote access services. To access your cluster from your local computer:</p> <p>Note</p> <p>These steps assume you're running Linux on your local machine. If you use FortiClient VPN on macOS and Windows, the steps might differ slightly.</p> <ol> <li>Download and install FortiClient VPN     from Fortinet's product download portal. If your local computer uses Red     Hat, CentOS, or Fedora, choose the .rpm package. If your computer uses     Debian or Ubuntu, choose the .deb package.</li> <li> <p>In your local terminal, open FortiClient VPN:</p> <pre><code>forticlient gui\n</code></pre> </li> <li> <p>In the left nav of the resulting window, click Remote access.</p> </li> <li>Click Configure VPN.</li> <li> <p>On the New VPN Connection page, fill out the configuration form:</p> <ul> <li>Connection name: <code>Lambda Cluster</code></li> <li>Description: <code>VPN connection to Lambda Cluster</code></li> <li> <p>Remote Gateway: Create an item for each of your control nodes' public     IPs. For each IP, target port 10433 for each as shown in the example     below:</p> <pre><code>https://&lt;NODE-PUBLIC-IP&gt;:10433\n</code></pre> </li> </ul> </li> <li> <p>Click Save to finish creating your VPN connection profile.</p> </li> <li>After setting up your profile, enter your username and password, and then     click Connect to log into FortiClient.</li> </ol> <p>You should now have full access to the cluster, as if it were part of your local network.</p>","tags":["private cloud","managed kubernetes"]},{"location":"private-cloud/managed-kubernetes/getting-started/#accessing-your-cluster-dashboard","title":"Accessing your cluster dashboard","text":"<p>After you establish your FortiClient VPN connection, navigate to https://10.141.3.1 in your browser and then log into your cluster's Rancher dashboard. You can find the login details for your dashboard in your 1Password vault.</p>","tags":["private cloud","managed kubernetes"]},{"location":"private-cloud/managed-kubernetes/getting-started/#navigating-the-cluster-dashboard","title":"Navigating the cluster dashboard","text":"<p>This section highlights some commonly used components of your cluster's Rancher dashboard. For more guidance on using the Rancher dashboard, see the New User Guides section of the Rancher documentation.</p>","tags":["private cloud","managed kubernetes"]},{"location":"private-cloud/managed-kubernetes/getting-started/#viewing-your-cluster","title":"Viewing your cluster","text":"<p>When you log into the Rancher dashboard, you arrive at a page listing all clusters. In the cluster list, click local, which should be the only cluster in the list. You can also click the bull icon in the left nav to access the cluster.</p>","tags":["private cloud","managed kubernetes"]},{"location":"private-cloud/managed-kubernetes/getting-started/#managing-your-nodes","title":"Managing your nodes","text":"<p>To see an overview of your nodes, click Cluster in the left nav of your cluster dashboard and then click Nodes. The Nodes page appears.</p> <p></p> <p>Click a node to see all resources associated with that node. Scroll down past the labels and annotations to see details and metrics for the resources running on your node.</p> <p></p>","tags":["private cloud","managed kubernetes"]},{"location":"private-cloud/managed-kubernetes/getting-started/#managing-your-project-resources","title":"Managing your project resources","text":"<p>Rancher uses the concept of a project to tie users to clusters and namespaces. When you create a new Private Cloud cluster with Managed Kubernetes, Lambda automatically creates a project and namespace for you that your user owns. You can control all of the resources within this project, including its namespaces.</p> <p>To view your projects and namespaces, click Cluster in the left nav of your cluster dashboard, and then click Projects/Namespaces. You arrive at the Projects/Namespaces page.</p> <p></p>","tags":["private cloud","managed kubernetes"]},{"location":"private-cloud/managed-kubernetes/getting-started/#creating-new-namespaces","title":"Creating new namespaces","text":"<p>Because Rancher manages the project/namespace mapping, you must use Rancher to create new namespaces in your project. If you want to automate the creation of new namespaces, you can use the Rancher CLI tool to do so.</p>","tags":["private cloud","managed kubernetes"]},{"location":"private-cloud/managed-kubernetes/getting-started/#filtering-your-resources-by-namespace","title":"Filtering your resources by namespace","text":"<p>The top bar of the dashboard provides a dropdown filter for projects/namespaces, shown below. You can select multiple items from the list or type to filter the entries then select the appropriate match. The filter applies to every view in the UI.</p> <p></p>","tags":["private cloud","managed kubernetes"]},{"location":"private-cloud/managed-kubernetes/getting-started/#managing-your-workloads","title":"Managing your workloads","text":"<p>You interact with your workloads from the Workloads page and its subpages. To access the Workload page, click Workloads in the left nav of your cluster dashboard.</p> <p></p> <p>When you click Workloads, the left nav expands to reveal categories for different resource types. The main Workloads dashboard shows all resources and their resource types. You can switch between viewing the table as folders per namespace or viewing it as a flat list by clicking the buttons to the right of the Filter input above the table. You can also filter the view by entering text into the Filter input.</p>","tags":["private cloud","managed kubernetes"]},{"location":"private-cloud/managed-kubernetes/getting-started/#managing-your-services","title":"Managing your services","text":"<p>To access services, click Service Discovery in the left nav of your cluster dashboard.</p> <p>You might find it useful to filter by namespace here, as described in Managing your project resources above. For example, in the screenshot below, the Services table has been filtered by the namespace kuberay.  Looking at the namespace, you can see an internal service of type <code>ClusterIP</code> and an external service of type <code>LoadBalancer</code>. The Target column contains some details about exposed ports and their names, if set.</p> <p></p> <p>In addition, you can view any ingresses that have been defined by clicking Service Discovery &gt; Ingresses in the left nav.</p>","tags":["private cloud","managed kubernetes"]},{"location":"private-cloud/managed-kubernetes/getting-started/#monitoring-your-cluster","title":"Monitoring your cluster","text":"<p>Your cluster uses a high-availability Prometheus configuration. You can view the resulting metrics by visiting your Grafana dashboard. To access the dashboard, click Monitoring in the left nav of your cluster dashboard and then click Grafana.</p> <p>The main page of the Grafana dashboard covers some basic cluster metrics.</p> <p></p> <p>To browse dashboards for other resources, hover over the four-squares icon in the left nav and select Browse.</p> <p></p> <p>You arrive at a list of available dashboards, as shown in the image below:</p> <p></p>","tags":["private cloud","managed kubernetes"]},{"location":"private-cloud/managed-kubernetes/getting-started/#managing-custom-resources","title":"Managing custom resources","text":"<p>You can view custom resources by clicking More Resources in your cluster's left navigation panel. Your custom resources are grouped by the custom resource definition (CRD) for which they're defined. For example, in the screenshot below, the RayCluster is defined in the ray.io CRD group:</p> <p></p> <p>After clicking on the resource in the nav, you can see any CRD-defined statuses or other information it tracks, and then drill down into the resource itself if you need to. Like most resources in the dashboard, you can edit these resources directly by clicking the \u22ee menu in the upper right corner of the resource and then selecting Edit YAML.</p> <p></p>","tags":["private cloud","managed kubernetes"]},{"location":"private-cloud/managed-kubernetes/getting-started/#accessing-the-kubernetes-api","title":"Accessing the Kubernetes API","text":"<p>You can access the Kubernetes API at <code>https://10.141.0.250:6443</code> through your SSL VPN connection. To download your cluster's <code>kubeconfig</code> file for use with the API, click the page icon in the top right area of your cluster dashboard:</p> <p></p> <p>To copy the <code>kubeconfig</code> to your clipboard, click the copy icon in the top right area of your dashboard.</p> <p></p>","tags":["private cloud","managed kubernetes"]},{"location":"private-cloud/managed-kubernetes/getting-started/#configuring-your-workloads","title":"Configuring your workloads","text":"","tags":["private cloud","managed kubernetes"]},{"location":"private-cloud/managed-kubernetes/getting-started/#adding-tolerations","title":"Adding tolerations","text":"<p>Your compute nodes are tainted to restrict what workloads get scheduled on them to maximize the available resources for GPU jobs. The taint applied matches the standard in other CSPs:</p> <pre><code>nvidia.com/gpu=true:NoSchedule\n</code></pre> <p>As a result, to schedule a workload to your compute nodes, you need to add the following toleration to your deployment, job, or pod specs:</p> <pre><code>tolerations:\n  - key: nvidia.com/gpu\n    value: \"true\"\n    operator: Equal\n    effect: NoSchedule\n# or\ntolerations:\n  - { key: nvidia.com/gpu, value: \"true\", operator: Equal, effect: NoSchedule }\n</code></pre>","tags":["private cloud","managed kubernetes"]},{"location":"private-cloud/managed-kubernetes/getting-started/#requesting-nvidia-resources","title":"Requesting NVIDIA resources","text":"<p>To allow pods to access your cluster's GPU and Infiniband resources, Lambda deploys the NVIDIA GPU Operator and NVIDIA Network Operator in your cluster. You can request these two operators in your specs as follows:</p> <p>Note</p> <p>By default, Lambda configures the NVIDIA Network Operator to deploy a single shared RDMA device. The YAML below reflects this default. You can request a different configuration if needed.</p> <pre><code>limits:\n  cpu:\n  memory: \"1Ti\"\n  nvidia.com/gpu: 8\n  rdma/rdma_shared_device_a: 1\nrequests:\n  cpu: \"128\"\n  memory: \"1Ti\"\n  nvidia.com/gpu: 8\n  rdma/rdma_shared_device_a: 1\n</code></pre>","tags":["private cloud","managed kubernetes"]},{"location":"public-cloud/","title":"Introduction","text":"<p>Lambda Public Cloud lets you launch individual virtual machines or clusters and turn them down on your schedule.</p> <ul> <li> <p> On Demand Cloud</p> <p>Instantly launch a Linux-based, GPU-backed virtual machine instance using a variety of NVIDIA chip configurations.</p> <p> Introduction</p> </li> <li> <p> 1-Click Clusters</p> <p>Launch clusters of GPU instances consisting of 16 to 512 NVIDIA H100 SXM Tensor Core GPUs.</p> <p> Introduction</p> </li> </ul>"},{"location":"public-cloud/cloud-api/","title":"Cloud API","text":"<p>With the Cloud API, you can:</p> <ul> <li>Launch instances,    restart instances, and    terminate instances.</li> <li>List the details of all of your instances.</li> <li>Get the details of a running instance.</li> <li>Get a list of the instance types offered by Lambda GPU Cloud.</li> <li>Manage your SSH keys.</li> <li>List your file systems.</li> </ul> <p>Note</p> <p>Requests to the Cloud API are generally limited to 1 request per second.</p> <p>Requests to the <code>/instance-operations/launch</code> endpoint are limited to 1 request every 10 seconds.</p>","tags":["1-click clusters","automation","on-demand cloud"]},{"location":"public-cloud/cloud-api/#launching-instances","title":"Launching instances","text":"<p>You can launch an instance from the command line using the Cloud API:</p> <ol> <li>Generate an API key.</li> <li> <p>Create a file named <code>request.json</code> that contains the    necessary payload.    For example:</p> <p><pre><code>{\n  \"region_name\": \"us-east-1\",\n  \"instance_type_name\": \"gpu_1x_a100_sxm4\",\n  \"ssh_key_names\": [\n    \"SSH-KEY\"\n  ],\n  \"file_system_names\": [],\n  \"quantity\": 1\n}\n</code></pre> 3.  Run the following command:</p> <pre><code>curl -u API-KEY: https://cloud.lambdalabs.com/api/v1/instance-operations/launch -d @request.json -H \"Content-Type: application/json\" | jq .\n</code></pre> </li> </ol> <p>Replace <code>API-KEY</code> with your actual API key. Don't remove the trailing colon (<code>:</code>).</p>","tags":["1-click clusters","automation","on-demand cloud"]},{"location":"public-cloud/cloud-api/#restarting-instances","title":"Restarting instances","text":"<ol> <li>Generate an API key if you haven't    already generated one.</li> <li> <p>Create a file that contains the    necessary payload.    For example:</p> <pre><code>{\n  \"instance_ids\": [\n    \"0920582c7ff041399e34823a0be62549\"\n  ]\n}\n</code></pre> <p>!!! tip</p> <p>Use the API to obtain the IDs of your instances.</p> </li> <li> <p>Run the following command:</p> <pre><code>curl -u API-KEY: https://cloud.lambdalabs.com/api/v1/instance-operations/restart -d @INSTANCE-IDS -H \"Content-Type: application/json\" | jq .\n</code></pre> </li> </ol> <p>Replace <code>API-KEY</code> with your actual API key. Don't remove the trailing colon (<code>:</code>).</p> <p>Replace <code>INSTANCE-IDS</code> with the name of the payload file you created in the previous step.</p>","tags":["1-click clusters","automation","on-demand cloud"]},{"location":"public-cloud/cloud-api/#terminating-instances","title":"Terminating instances","text":"<ol> <li>Generate an API key if you    haven't already generated one.</li> <li> <p>Create a file that contains the    necessary payload.    For example:</p> <pre><code>{\n  \"instance_ids\": [\n    \"0920582c7ff041399e34823a0be62549\"\n  ]\n}\n</code></pre> <p>!!! tip</p> <p>Use the API to obtain the IDs of your instances.</p> </li> <li> <p>Run the following command:</p> <p><pre><code>curl -u API-KEY: https://cloud.lambdalabs.com/api/v1/instance-operations/terminate -d @INSTANCE-IDS -H \"Content-Type: application/json\" | jq .\n</code></pre>   In this command,</p> <ul> <li>Replace <code>API-KEY</code> with your actual API key. Don't remove the trailing colon (<code>:</code>).</li> <li>Replace <code>INSTANCE-IDS</code> with the name of the payload file you created in the previous step.</li> </ul> </li> </ol>","tags":["1-click clusters","automation","on-demand cloud"]},{"location":"public-cloud/cloud-api/#listing-details-of-running-instances","title":"Listing details of running instances","text":"<p>You can list your running instances from a command line using the Cloud API.</p> <p>First, generate an API key. Then, run the following command:</p> <pre><code>curl -u API-KEY: https://cloud.lambdalabs.com/api/v1/instances | jq .\n</code></pre> <p>In this command, replace <code>API-KEY</code> with your actual API key. Don't remove the trailing colon (<code>:</code>).</p>","tags":["1-click clusters","automation","on-demand cloud"]},{"location":"public-cloud/cloud-api/#getting-details-of-a-specific-instance","title":"Getting details of a specific instance","text":"<p>You can retrieve the details of an instance from a command line using the Cloud API.</p> <p>First, generate an API key. Then, run the following command:</p> <pre><code>curl -u API-KEY: https://cloud.lambdalabs.com/api/v1/instances/INSTANCE-ID | jq .\n</code></pre> <p>In this command,</p> <ul> <li>Replace <code>API-KEY</code> with your actual API key. Don't remove the trailing colon (<code>:</code>).</li> <li>Replace <code>INSTANCE-ID</code> with the ID of the instance you want details about.</li> </ul>","tags":["1-click clusters","automation","on-demand cloud"]},{"location":"public-cloud/cloud-api/#listing-instances-types-offered-by-lambda-gpu-cloud","title":"Listing instances types offered by Lambda GPU Cloud","text":"<p>You can list the instances types offered by Lambda GPU Cloud by first generating an API key, then running the following command:</p> <pre><code>curl -u API-KEY: https://cloud.lambdalabs.com/api/v1/instance-types | jq .\n</code></pre> <p>Replace <code>API-KEY</code> with your actual API key. Don\u2019t remove the trailing colon (<code>:</code>).</p>","tags":["1-click clusters","automation","on-demand cloud"]},{"location":"public-cloud/cloud-api/#managing-ssh-keys","title":"Managing SSH keys","text":"<p>You can use the Cloud API to:</p> <ul> <li>Add an existing SSH key to your account.</li> <li>Generate a new SSH key pair.</li> <li>List the SSH keys saved in your account.</li> <li>Delete an SSH key from your account.</li> </ul> <p>Warning</p> <p>Following these instructions won't add the SSH key to existing instances.</p> <p>To add SSH keys to existing instances, read our FAQ on using more than one SSH key.</p> <p>Note</p> <p>You can add up to 1,024 SSH keys to your account.</p>","tags":["1-click clusters","automation","on-demand cloud"]},{"location":"public-cloud/cloud-api/#add-an-existing-ssh-key-to-your-account","title":"Add an existing SSH key to your account","text":"<p>To add an existing SSH key to your account:</p> <ol> <li>Generate an API key if you don't    have one already.</li> <li> <p>Create a file named <code>ssh-key.json</code> that contains the    necessary payload.    For example:</p> <p><pre><code>{\n  \"name\": \"my-new-key\",\n  \"public_key\": \"ssh-ed25519 KEY COMMENT\"\n}\n</code></pre> 3.  Run the following command:</p> <pre><code>curl -u API-KEY: https://cloud.lambdalabs.com/api/v1/ssh-keys -d @ssh-key.json -H \"Content-Type: application/json\" | jq .\n</code></pre> </li> </ol> <p>In this command, replace <code>API-KEY</code> with your actual API key. Don't remove the trailing colon (<code>:</code>).</p>","tags":["1-click clusters","automation","on-demand cloud"]},{"location":"public-cloud/cloud-api/#generate-a-new-ssh-key-pair","title":"Generate a new SSH key pair","text":"<p>To generate a new SSH key pair:</p> <ol> <li>Generate an API key if you don\u2019t have one already.</li> <li> <p>Run the following command:</p> <pre><code>curl -u API-KEY: https://cloud.lambdalabs.com/api/v1/ssh-keys -d '{ \"name\": \"my-generated-key\" }' -H \"Content-Type: application/json\" | jq -r '.data.private_key' &gt; my-generated-private-key.pem\n</code></pre> <p>In this command, replace <code>API-KEY</code> with your actual API key. Don't remove the trailing colon (<code>:</code>).   This command saves the private key for your SSH key pair as <code>my-generated-private-key.pem</code>.</p> </li> <li> <p>Run <code>chmod 400 my-generated-private-key.pem</code> to set the correct file permissions for your private key.</p> </li> </ol>","tags":["1-click clusters","automation","on-demand cloud"]},{"location":"public-cloud/cloud-api/#list-the-ssh-keys-saved-in-your-account","title":"List the SSH keys saved in your account","text":"<p>To list the SSH keys saved in your account, generate an API key if you don\u2019t already have one. Then, run the following command:</p> <pre><code>curl -u API-KEY: https://cloud.lambdalabs.com/api/v1/ssh-keys | jq .\n</code></pre> <p>In this command, replace <code>API-KEY</code> with your actual API key. Don\u2019t remove the trailing colon (<code>:</code>).</p>","tags":["1-click clusters","automation","on-demand cloud"]},{"location":"public-cloud/cloud-api/#delete-an-ssh-key-from-your-account","title":"Delete an SSH key from your account","text":"<p>To delete an SSH key from your account, generate an API key if you don\u2019t already have one. Then, run the following command:</p> <pre><code>curl -u API-KEY: -X DELETE https://cloud.lambdalabs.com/api/v1/ssh-keys/SSH-KEY-ID\n</code></pre> <p>In this command,</p> <ul> <li>Replace <code>API-KEY</code> with your actual API key. Don't remove the trailing colon (<code>:</code>).</li> <li>Replace <code>SSH-KEY-ID</code> with the ID of the SSH key you want to delete.</li> </ul> <p>Tip</p> <p>Use the API to obtain the IDs of the SSH keys saved in your account.</p>","tags":["1-click clusters","automation","on-demand cloud"]},{"location":"public-cloud/cloud-api/#listing-filesystems","title":"Listing filesystems","text":"<p>To list your persistent storage filesystems using the Cloud API:</p> <ol> <li>Generate an API key if you don\u2019t    already have an API key.</li> <li> <p>Run the following command:</p> <pre><code>curl -u API-KEY: https://cloud.lambdalabs.com/api/v1/file-systems | jq .\n</code></pre> </li> </ol> <p>In this command, replace <code>API-KEY</code> with your actual API key. Don\u2019t remove the trailing colon (<code>:</code>).</p>","tags":["1-click clusters","automation","on-demand cloud"]},{"location":"public-cloud/filesystems/","title":"Filesystems","text":"<p>Filesystems, also known as persistent storage, allow you to store your large datasets and the state of your instance, for example:</p> <ul> <li>Packages installed system-wide using <code>apt-get</code>.</li> <li>Python packages installed using <code>pip</code>.</li> <li>conda    and    Python venv    virtual environments.</li> </ul> <p>Lambda GPU Cloud filesystems have a capacity of 8 exabytes, or 8,000,000 terabytes, and you can have a total of 24 filesystems, except for filesystems created in the Texas, USA (us-south-1) region. The capacity of filesystems created in the Texas, USA (us-south-1) region is 10 terabytes.</p>","tags":["storage"]},{"location":"public-cloud/filesystems/#how-are-filesystems-billed","title":"How are filesystems billed?","text":"<p>Persistent storage is billed per GB used per month, in increments of 1 hour.</p> <p>For example, based on the price of $0.20 per GB used per month:</p> <ul> <li>If you use 1,000 GB of your filesystem capacity for an entire month (30    days, or 720 hours), you\u2019ll be billed $200.00.</li> <li>If you use 1,000 GB of your filesystem capacity for a single day (24 hours), you\u2019ll be billed $6.67.</li> </ul> <p>Note</p> <p>The actual price of persistent storage will be displayed when you create your filesystem.</p>","tags":["storage"]},{"location":"public-cloud/filesystems/#can-filesystems-be-accessed-without-an-instance","title":"Can filesystems be accessed without an instance?","text":"<p>Persistent storage filesystems can't be accessed unless attached to an instance at the time the instance is launched.</p> <p>For this reason, it's recommended that you keep a local copy of the files you have saved in your persistent storage filesystems. This can be done using <code>rsync</code>.</p> <p>Note</p> <p>Filesystems can't be attached to running instances and can't be mounted remotely, for example, using NFS.</p> <p>Moreover, filesystems can only be attached to instances in the same region. For example, a filesystem created in the us-west-1 (California, USA) region can only be attached to instances in the us-west-1 region.</p> <p>Filesystems can't be transferred from one region to another. However, you can copy data between filesystems using tools such as <code>rsync</code>.</p> <p>Lambda GPU Cloud currently doesn't offer block or object storage.</p>","tags":["storage"]},{"location":"public-cloud/filesystems/#can-i-set-a-limit-quota-on-my-filesystem-usage","title":"Can I set a limit (quota) on my filesystem usage?","text":"<p>Currently, you can't set a limit (quota) on your persistent storage filesystem usage.</p> <p>You can see the usage of a persistent storage filesystem from within an instance by running <code>df -h -BG</code>. This command will produce output similar to:</p> <pre><code>Filesystem           1G-blocks  Used   Available Use% Mounted on\nudev                       99G    0G         99G   0% /dev\ntmpfs                      20G    1G         20G   1% /run\n/dev/vda1                1357G   23G       1335G   2% /\ntmpfs                      99G    0G         99G   0% /dev/shm\ntmpfs                       1G    0G          1G   0% /run/lock\ntmpfs                      99G    0G         99G   0% /sys/fs/cgroup\npersistent-storage 8589934592G    0G 8589934592G   0% /home/ubuntu/persistent-storage\n/dev/vda15                  1G    1G          1G   6% /boot/efi\n/dev/loop0                  1G    1G          0G 100% /snap/core20/1822\n/dev/loop1                  1G    1G          0G 100% /snap/lxd/24061\n/dev/loop2                  1G    1G          0G 100% /snap/snapd/18357\ntmpfs                      20G    0G         20G   0% /run/user/1000\n</code></pre> <p>In the example output, above:</p> <ul> <li>The name of the filesystem is <code>persistent-storage</code>.</li> <li>The size of the filesystem is <code>8589934592G</code> (8 exabytes).</li> <li>The available capacity of the filesystem is <code>8589934592G</code>.</li> <li>The used percentage of the filesystem is <code>0%</code>.</li> <li>The filesystem is mounted on <code>/home/ubuntu/persistent-storage</code>.</li> </ul> <p>Note</p> <p>You can also use the Cloud API's <code>/file-systems</code> endpoint to find out your filesystem usage.</p>","tags":["storage"]},{"location":"public-cloud/filesystems/#preserving-the-state-of-your-system","title":"Preserving the state of your system","text":"<p>For saving the state of your system, including:</p> <ul> <li>Packages installed system-wide using <code>apt-get</code></li> <li>Python packages installed using <code>pip</code></li> <li>conda environments</li> </ul> <p>We recommend creating containers using Docker or other software for creating containers.</p> <p>You can also create a script that runs the commands needed to re-create your system state. For example:</p> <pre><code>sudo apt install PACKAGE_0 PACKAGE_1 PACKAGE_2 &amp;&amp; \\\npip install PACKAGE_3 PACKAGE_4 PACKAGE_5\n</code></pre> <p>Run the script each time you start an instance.</p> <p>If you only need to preserve Python packages and not packages installed system-wide, you can create a Python virtual environment.</p> <p>You can also create a conda environment.</p> <p>Tip</p> <p>For the highest performance when training, we recommend copying your dataset, containers, and virtual environments from persistent storage to your home directory. This can take some time but greatly increases the speed of training.</p>","tags":["storage"]},{"location":"public-cloud/firewalls/","title":"Firewalls","text":"<p>You can restrict incoming traffic to your instances by creating firewall rules on the Firewall page in the Lambda Cloud dashboard. Currently, the rules you create apply to all instances associated with your account except those in the Texas, USA (us-south-1) regions.</p> <p>By default, Lambda allows only incoming ICMP traffic and traffic on port 22 (SSH).</p>","tags":["1-click clusters","on-demand cloud"]},{"location":"public-cloud/firewalls/#managing-your-firewall-rules","title":"Managing your firewall rules","text":"","tags":["1-click clusters","on-demand cloud"]},{"location":"public-cloud/firewalls/#creating-a-firewall-rule","title":"Creating a firewall rule","text":"<p>You can create up to 20 firewall rules to restrict incoming traffic to your instances. If you create more than 20 rules, new instances you create might not launch. Also, it\u2019s possible that not all of your rules will be active, which might leave your instances insecure.</p> <p>Warning</p> <p>Each port you open increases the attack surface of your instances. Make sure to vet any services you run on your instances, and be judicious about exposing new ports. If possible, restrict your incoming traffic to known sources.</p> <p>To create a new firewall rule:</p> <ol> <li>Navigate to the Firewall page in    the Lambda Cloud dashboard.</li> <li>In the Inbound Rules section, click Edit to begin creating a rule.</li> <li> <p>In the dropdown menu under Type, select the type of rule you want to    create:</p> <ul> <li>Select Custom TCP to manually configure a rule to allow incoming TCP     traffic.</li> <li>Select Custom UDP to manually configure a rule to allow incoming UDP     traffic.</li> <li>Select HTTPS to automatically configure a rule to allow incoming     HTTPS traffic.</li> <li>Select SSH to automatically configure a rule to allow incoming SSH     traffic.</li> <li>Select All TCP to automatically configure a rule to allow all     incoming TCP traffic.</li> <li>Select All UDP to automatically configure a rule to allow all     incoming UDP traffic.</li> </ul> </li> <li> <p>In the Source field, enter an IP address or range to restrict incoming    traffic to only that set of sources. To allow incoming traffic from any source,    enter <code>0.0.0.0/0</code>.</p> <ul> <li>Click the \ud83d\udd0e to automatically enter your current IP address.</li> <li>Enter a single IP address\u2014for example, <code>203.0.113.1</code>.</li> <li>Enter an IP address range in CIDR notation\u2014for example, <code>203.0.113.0/24</code>.</li> </ul> </li> <li> <p>If you chose Custom TCP or Custom UDP as your rule type, enter a    port range in the Port range field. You can enter either a single port    (for example,<code>8080</code>) or a range of ports (for example, <code>8080-8081</code>).</p> </li> <li>Optionally, add a description for the rule in the Description field.</li> <li>Click Update to apply your changes.</li> </ol>","tags":["1-click clusters","on-demand cloud"]},{"location":"public-cloud/firewalls/#updating-a-firewall-rule","title":"Updating a firewall rule","text":"<p>To update a firewall rule:</p> <ol> <li>Navigate to the Firewall page in    the Lambda Cloud dashboard.</li> <li>In the Inbound Rules section, click Edit.</li> <li>Find the rule you want to update and modify it as needed.</li> <li>Click Update to apply your changes.</li> </ol>","tags":["1-click clusters","on-demand cloud"]},{"location":"public-cloud/firewalls/#deleting-a-firewall-rule","title":"Deleting a firewall rule","text":"<p>To delete a firewall rule:</p> <ol> <li>Navigate to the Firewall page in    the Lambda Cloud dashboard.</li> <li>In the Inbound Rules section, click Edit.</li> <li>Click the x next to any rule you want to delete.</li> <li>Click Update to apply your changes.</li> </ol> <p>Warning</p> <p>If you delete the rule that allows incoming traffic to port TCP/22, you won\u2019t be able to access your instances using SSH.</p>","tags":["1-click clusters","on-demand cloud"]},{"location":"public-cloud/firewalls/#allowing-or-restricting-icmp-traffic","title":"Allowing or restricting ICMP traffic","text":"<p>To allow or restrict ICMP traffic, visit the Firewall page in the Lambda Cloud dashboard and toggle Allow ICMP traffic (ping) in the General Settings section.</p> <p>Note</p> <p>For network diagnostic tools such as <code>ping</code> and <code>mtr</code> to be able to reach your instances, you need to allow incoming ICMP traffic.</p>","tags":["1-click clusters","on-demand cloud"]},{"location":"public-cloud/lambda-chat-api/","title":"Using the Lambda Chat Completions API","text":"<p>The Lambda Chat Completions API enables you to use the Llama 3.1 405B Instruct large language model (LLM), and fine-tuned versions such as Nous Research's Hermes 3 and Liquid AI's LFM 40.3B MoE (Mixture of Experts), without the need to set up your own vLLM API server on an on-demand instance or 1-Click Cluster (1CC).</p> <p>Note</p> <p>Try Lambda Chat!</p> <p>Since the Lambda Chat Completions API is compatible with the OpenAI API, you can use it as a drop-in replacement for applications currently using the OpenAI API. See, for example, our guide on integrating Lambda Chat into VS Code.</p> <p>The Lambda Chat Completions API implements endpoints for:</p> <ul> <li>Creating chat completions (<code>/chat/completions</code>)</li> <li>Creating completions (<code>/completions</code>)</li> <li>Listing models (<code>/models</code>)</li> </ul> <p>Currently, the following models are available:</p> <ul> <li><code>deepseek-coder-v2-lite-instruct</code></li> <li><code>dracarys2-72b-instruct</code></li> <li><code>hermes3-405b</code></li> <li><code>hermes3-405b-fp8-128k</code></li> <li><code>hermes3-70b</code></li> <li><code>hermes3-8b</code></li> <li><code>lfm-40b</code></li> <li><code>llama3.1-405b-instruct-fp8</code></li> <li><code>llama3.1-70b-instruct-fp8</code></li> <li><code>llama3.1-8b-instruct</code></li> <li><code>llama3.2-3b-instruct</code></li> <li><code>llama3.1-nemotron-70b-instruct</code></li> </ul> <p>Note</p> <p>If a request using the <code>hermes3-405b</code> model is made with a context length greater than 18K, the request will fall back to using the <code>hermes-3-llama-3.1-405b-fp8-128k</code> model.</p> <p>To use the Lambda Chat Completions API, first generate a Cloud API key from the dashboard. You can also use a Cloud API key that you've already generated.</p> <p>In the examples below:</p> <ul> <li>Replace <code>&lt;MODEL&gt;</code> with one of the models listed above.</li> <li>Replace <code>&lt;API-KEY&gt;</code> with your actual Cloud API key.</li> </ul>","tags":["api","llama","llm"]},{"location":"public-cloud/lambda-chat-api/#creating-chat-completions","title":"Creating chat completions","text":"<p>The <code>/chat/completions</code> endpoint takes a list of messages that make up a conversation, then outputs a response.</p> CurlPython <p>First, create a file named <code>messages.json</code> that contains the necessary and any optional parameters. For example:</p> <pre><code>{\n  \"model\": \"&lt;MODEL&gt;\",\n  \"messages\": [\n    {\n      \"role\": \"system\",\n      \"content\": \"You are a helpful assistant named Hermes, made by Nous Research.\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"Who won the world series in 2020?\"\n    },\n    {\n      \"role\": \"assistant\",\n      \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"Where was it played?\"\n    }\n  ]\n}\n</code></pre> <p>Then, run:</p> <pre><code>curl https://api.lambdalabs.com/v1/chat/completions -d @messages.json -H \"Authorization: Bearer &lt;API-KEY&gt;\" -H \"Content-Type: application/json\" | jq .\n</code></pre> <p>You should see output similar to:</p> <pre><code>{\n  \"id\": \"chat-f569652bd6a64e77b01dbb1955832998\",\n  \"choices\": [\n    {\n      \"finish_reason\": \"stop\",\n      \"index\": 0,\n      \"message\": {\n        \"content\": \"The 2020 World Series was played at Globe Life Field in Arlington, Texas. Due to the COVID-19 pandemic, the entire series was held at a neutral location to minimize travel and potential exposure to the virus.\",\n        \"role\": \"assistant\",\n        \"tool_calls\": null,\n        \"function_call\": null\n      }\n    }\n  ],\n  \"created\": 1723728526,\n  \"model\": \"hermes3-405b\",\n  \"object\": \"chat.completion\",\n  \"system_fingerprint\": null,\n  \"usage\": {\n    \"completion_tokens\": 45,\n    \"prompt_tokens\": 58,\n    \"total_tokens\": 103\n  },\n  \"service_tier\": null\n}\n</code></pre> <p>First, create and activate a Python virtual environment. Then, install the OpenAI Python API library.</p> <p>Run, for example:</p> <pre><code>from openai import OpenAI\n\nopenai_api_key = \"&lt;API-KEY&gt;\"\nopenai_api_base = \"https://api.lambdalabs.com/v1\"\n\nclient = OpenAI(\n    api_key=openai_api_key,\n    base_url=openai_api_base,\n)\n\nmodel = \"&lt;MODEL&gt;\"\n\nchat_completion = client.chat.completions.create(\n    messages=[{\n        \"role\": \"system\",\n        \"content\": \"You are a helpful assistant named Hermes, made by Nous Research.\"\n    }, {\n        \"role\": \"user\",\n        \"content\": \"Who won the world series in 2020?\"\n    }, {\n        \"role\":\n        \"assistant\",\n        \"content\":\n        \"The Los Angeles Dodgers won the World Series in 2020.\"\n    }, {\n        \"role\": \"user\",\n        \"content\": \"Where was it played?\"\n    }],\n    model=model,\n)\n\nprint(chat_completion)\n</code></pre> <p>You should see output similar to:</p> <pre><code>ChatCompletion(id='chat-e489d950acaa41deb02cb794d1ecfe6b', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The 2020 World Series was played at Globe Life Field in Arlington, Texas. Due to the COVID-19 pandemic, the entire series was held at a neutral site to limit travel and potential exposure to the virus.', refusal=None, role='assistant', function_call=None, tool_calls=None))], created=1723738002, model='hermes3-405b', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=45, prompt_tokens=58, total_tokens=103))\n</code></pre>","tags":["api","llama","llm"]},{"location":"public-cloud/lambda-chat-api/#creating-completions","title":"Creating completions","text":"<p>The <code>/completions</code> endpoint takes a single text string (a prompt) as input, then outputs a response. In comparison, the <code>/chat/completions</code> endpoint takes a list of messages as input.</p> <p>To use the <code>/completions</code> endpoint:</p> CurlPython <p>First, create a file named <code>prompt.json</code> that contains the necessary and any optional parameters. For example:</p> <pre><code>{\n  \"model\": \"&lt;MODEL&gt;\",\n  \"prompt\": \"Computers are\",\n  \"temperature\": 0\n}\n</code></pre> <p>Then, run:</p> <pre><code>curl https://api.lambdalabs.com/v1/completions -d @prompt.json -H \"Authorization: Bearer &lt;API-KEY&gt;\" -H \"Content-Type: application/json\" | jq .\n</code></pre> <p>You should see output similar to:</p> <pre><code>{\n  \"id\": \"cmpl-0ed3e4f07bf242bab6d4ec67c48e0e83\",\n  \"object\": \"text_completion\",\n  \"created\": 1723487710,\n  \"model\": \"hermes3-405b\",\n  \"choices\": [\n    {\n      \"stop_reason\": null,\n      \"finish_reason\": \"stop\",\n      \"index\": 0,\n      \"text\": \" a part of our daily lives, and we use them for various purposes, including work, entertainment, and communication. However, like any other electronic device, computers are prone to malfunctions and errors. When your computer starts acting up, it can be frustrating, especially if you don\u2019t know how to fix the problem. In this article, we will discuss some common computer problems and how to troubleshoot them.\\n1. Slow Performance\\nOne of the most common computer problems is slow performance. If your computer is running slowly, there could be several reasons why. Here are some steps you can take to troubleshoot the issue:\\nCheck your hard drive space: If your hard drive is almost full, it can slow down your computer. Free up some space by deleting unnecessary files or moving them to an external hard drive.\\nClose unused programs: If you have too many programs running at the same time, it can slow down your computer. Close any programs that you\u2019re not using.\\nRun a virus scan: Malware and viruses can slow down your computer. Run a virus scan to check for any malicious software.\\nUpgrade your hardware: If your computer is old, it may not have the necessary hardware to run newer programs efficiently. Consider upgrading your RAM or processor.\\n2. Blue Screen of Death\\nThe Blue Screen of Death (BSOD) is a common error that occurs when your computer encounters a critical system error. When this happens, your computer will restart, and you may lose any unsaved work. Here are some steps you can take to troubleshoot the issue:\\nCheck for updates: Make sure your operating system and drivers are up to date.\\nRun a memory test: A faulty RAM module can cause the BSOD. Run a memory test to check for any errors.\\nCheck your hardware: Loose cables, overheating, and faulty hardware can all cause the BSOD. Check your computer\u2019s hardware and make sure everything is properly connected and functioning.\\n3. Internet Connectivity Issues\\nIf you\u2019re having trouble connecting to the internet, there could be several reasons why. Here are some steps you can take to troubleshoot the issue:\\nCheck your router: Make sure your router is properly connected and turned on. If it\u2019s not working, try restarting it.\\nCheck your network settings: Make sure your computer is set to connect to the correct network and that your network settings are correct.\\nRun a network troubleshooter: Most operating systems have a built-in network troubleshooter that can help diagnose and fix connectivity issues.\\n4. Printer Problems\\nPrinter problems are another common issue that computer users face. If your printer isn\u2019t working, here are some steps you can take to troubleshoot the issue:\\nCheck your printer queue: If there are too many print jobs in the queue, it can cause your printer to stop working. Clear the print queue and try again.\\nCheck your printer drivers: Make sure your printer drivers are up to date. If they\u2019re not, download and install the latest drivers from the manufacturer\u2019s website.\\nCheck your printer settings: Make sure your printer settings are correct. If they\u2019re not, adjust them and try again.\\n5. Audio Issues\\nIf you\u2019re having trouble with your computer\u2019s audio, there could be several reasons why. Here are some steps you can take to troubleshoot the issue:\\nCheck your audio settings: Make sure your audio settings are correct and that your speakers or headphones are properly connected.\\nUpdate your audio drivers: If your audio drivers are outdated, it can cause audio issues. Download and install the latest drivers from the manufacturer\u2019s website.\\nCheck for muted applications: If you\u2019re not hearing any sound from a specific application, make sure it\u2019s not muted.\\nIn conclusion, computer problems can be frustrating, but with the right troubleshooting steps, you can often fix the issue yourself. If you\u2019re still having trouble, consider contacting a professional for help.\\n*Note: This is a sample article and may not reflect the most up-to-date information or best practices. Always consult with a professional for the most accurate and relevant advice.*\",\n      \"logprobs\": null\n    }\n  ],\n  \"usage\": {\n    \"completion_tokens\": 816,\n    \"prompt_tokens\": 4,\n    \"total_tokens\": 820\n  },\n  \"system_fingerprint\": null\n}\n</code></pre> <p>First, create and activate a Python virtual environment. Then, install the OpenAI Python API library.</p> <p>Run, for example:</p> <pre><code>from openai import OpenAI\n\nopenai_api_key = \"&lt;API-KEY&gt;\"\nopenai_api_base = \"https://api.lambdalabs.com/v1\"\n\nclient = OpenAI(\n    api_key=openai_api_key,\n    base_url=openai_api_base,\n)\n\nmodel = \"&lt;MODEL&gt;\"\n\nresponse = client.completions.create(\n  prompt=\"Computers are\",\n  temperature=0,\n  model=model,\n)\n\nprint(response)\n</code></pre> <p>You should see output similar to:</p> <pre><code>Completion(id='cmpl-bed15d67c6894588bc0292c1cc5ed28d', choices=[CompletionChoice(finish_reason='stop', index=0, logprobs=None, text=' a part of our everyday lives. They are used in homes, schools, businesses, and many other places. Computers have changed the way we live, work, and play. They have made many tasks easier and more efficient. However, computers can also be frustrating and challenging to use at times. In this article, we will explore the pros and cons of computers.\\nPros of Computers:\\n1. Increased Productivity: Computers have made many tasks easier and more efficient. They can perform complex calculations, store large amounts of data, and automate repetitive tasks. This has led to increased productivity in many industries.\\n\\n2. Improved Communication: Computers have revolutionized the way we communicate. With email, instant messaging, and video conferencing, we can communicate with people all over the world instantly.\\n\\n3. Access to Information: The internet has made a vast amount of information available to us at our fingertips. We can search for information on any topic and find answers to our questions quickly and easily.\\n\\n4. Entertainment: Computers have also changed the way we entertain ourselves. We can play games, watch movies, listen to music, and read books all on our computers.\\n\\n5. Education: Computers have had a significant impact on education. They have made it possible for students to learn from anywhere in the world and have access to a wealth of educational resources.\\n\\nCons of Computers:\\n1. Dependence: As we become more reliant on computers, we may find it difficult to function without them. This dependence can be problematic if there is a power outage or if our computers break down.\\n\\n2. Health Issues: Prolonged use of computers can lead to health issues such as eye strain, headaches, and back pain. Sitting for long periods can also contribute to obesity and other health problems.\\n\\n3. Security Risks: Computers are vulnerable to security risks such as viruses, malware, and hacking. These risks can compromise our personal information and lead to identity theft.\\n\\n4. Social Isolation: While computers have improved communication, they can also lead to social isolation. People may spend more time interacting with their computers than with other people, leading to feelings of loneliness and disconnection.\\n\\n5. Cost: Computers can be expensive to purchase and maintain. Upgrading hardware and software can also be costly.\\n\\nIn conclusion, computers have both pros and cons. They have made many aspects of our lives easier and more efficient, but they also come with challenges and risks. It is essential to use computers responsibly and to be aware of the potential drawbacks. By doing so, we can maximize the benefits of computers while minimizing the negative impacts.1) + 1\\n    return n\\n\\ndef main():\\n    n = int(input())\\n    print(fibonacci(n))\\n\\nif __name__ == \"__main__\":\\n    main()\\n```\\n\\n## Explanation\\n\\nThe `fibonacci` function takes an integer `n` as input and returns the `n`-th Fibonacci number. The function uses recursion to calculate the Fibonacci number.\\n\\nThe base cases for the recursion are:\\n- If `n` is 0, the function returns 0.\\n- If `n` is 1, the function returns 1.\\n\\nFor any other value of `n`, the function recursively calls itself with `n-1` and `n-2` and returns the sum of the results.\\n\\nIn the `main` function, we read an integer `n` from the user using `input()` and convert it to an integer using `int()`. We then call the `fibonacci` function with `n` and print the result.\\n\\nThe `if __name__ == \"__main__\":` condition ensures that the `main` function is only executed when the script is run directly, and not when it is imported as a module.\\n\\n## Example\\n\\nLet\\'s say the user inputs the number 6. The program will output:\\n```\\n8\\n```\\n\\nThis is because the 6th Fibonacci number is 8 (0, 1, 1, 2, 3, 5, 8).\\n\\nThe recursive calls will be as follows:\\n- `fibonacci(6)` calls `fibonacci(5)` and `fibonacci(4)`\\n  - `fibonacci(5)` calls `fibonacci(4)` and `fibonacci(3)`\\n    - `fibonacci(4)` calls `fibonacci(3)` and `fibonacci(2)`\\n      - `fibonacci(3)` calls `fibonacci(2)` and `fibonacci(1)`\\n        - `fibonacci(2)` calls `fibonacci(1)` and `fibonacci(0)`\\n          - `fibonacci(1)` returns 1\\n          - `fibonacci(0)` returns 0\\n        - `fibonacci(1)` returns 1\\n      - `fibonacci(2)` returns 1\\n    - `fibonacci(3)` returns 2\\n  - `fibonacci(4)` returns 3\\n\\nFinally, `fibonacci(6)` returns 5 + 3 = 8.', stop_reason=None)], created=1723743390, model='hermes3-405b', object='text_completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1026, prompt_tokens=4, total_tokens=1030))\n</code></pre>","tags":["api","llama","llm"]},{"location":"public-cloud/lambda-chat-api/#listing-models","title":"Listing models","text":"<p>The <code>/models</code> endpoint lists the models available for use through the Lambda Chat API.</p> <p>To use the <code>/models</code> endpoint:</p> CurlPython <p>Run:</p> <pre><code>curl https://api.lambdalabs.com/v1/models -H \"Authorization: Bearer &lt;API-KEY&gt;\" -H \"Content-Type: application/json\" | jq .\n</code></pre> <p>You should see output similar to:</p> <pre><code>{\n  \"id\": \"hermes3-70b\",\n  \"object\": \"model\",\n  \"created\": 1724347380,\n  \"owned_by\": \"lambda\"\n},\n{\n  \"id\": \"hermes3-8b\",\n  \"object\": \"model\",\n  \"created\": 1724347380,\n  \"owned_by\": \"lambda\"\n},\n{\n  \"id\": \"lfm-40b\",\n  \"object\": \"model\",\n  \"created\": 1724347380,\n  \"owned_by\": \"lambda\"\n},\n{\n  \"id\": \"llama3.1-405b-instruct-fp8\",\n  \"object\": \"model\",\n  \"created\": 1724347380,\n  \"owned_by\": \"lambda\"\n},\n</code></pre> <p>First, create and activate a Python virtual environment. Then, install the OpenAI Python API library.</p> <p>Run:</p> <pre><code>from openai import OpenAI\n\nopenai_api_key = \"&lt;API-KEY&gt;\"\nopenai_api_base = \"https://api.lambdalabs.com/v1\"\n\nclient = OpenAI(\n    api_key=openai_api_key,\n    base_url=openai_api_base,\n)\n\nclient.models.list()\n</code></pre> <p>You should see output similar to:</p> <pre><code>SyncPage[Model](data=[Model(id='hermes3-405b', created=1677610602, object='model', owned_by='openai'), Model(id='hermes-3-llama-3.1-405b-fp8-128k', created=1677610602, object='model', owned_by='openai')], object='list')\n</code></pre>","tags":["api","llama","llm"]},{"location":"public-cloud/teams/","title":"Teams","text":"","tags":["1-click clusters","automation","on-demand cloud"]},{"location":"public-cloud/teams/#create-a-team","title":"Create a team","text":"<ol> <li>In the dashboard, click Team at the bottom-left of the dashboard.    Then, click Invite at the top-right of the Team dashboard.</li> <li> <p>Enter the email address of the person you want to invite to your team.    Select their role in the team, either an Admin or a Member.    Then, click Send invitation.</p> <p>Warning</p> <p>Be sure to invite only trusted persons to your team!</p> <p>Currently, the only differences between the Admin and Member roles are that an Admin can:</p> <ul> <li>Invite others to the team.</li> <li>Remove others from the team.</li> <li>Modify payment information.</li> <li>Change the team name.</li> </ul> <p>This means that a person with a Member role can, for example:</p> <ul> <li>Launch instances that will incur charges.</li> <li>Terminate instances that should continue to run.</li> </ul> <p>Note</p> <p>You can\u2019t send an invitation to an email address already associated with a Lambda Cloud account. If you try to, you\u2019ll be presented with a message that says there is already a Lambda Cloud account associated with the email address you\u2019re trying to send an invitation to.</p> <p></p> <p>The person you\u2019re inviting to your team must first close their existing Lambda Cloud account before they can be invited to your team.</p> </li> <li> <p>The person you invited to your team will receive an email letting them    know that they\u2019ve been invited to a team on Lambda Cloud. In that email,    they should click Join the Team.</p> <p>Note</p> <p>Until the person you invited to your team accepts their invitation, they will be listed in the Team dashboard as Invitation pending.</p> <p>You can delete the invitation while it\u2019s pending by clicking \u22ee where the person is listed in your Team dashboard, then choosing Delete invitation.</p> <p>Note</p> <p>If the person you invited to your team doesn\u2019t receive their invitation, you have to delete their invitation then invite them again.</p> </li> </ol> <p>In the Team dashboard of the person you invited to your team, the person will see that they are on your team. In your Team dashboard, you\u2019ll see the person you invited listed.</p>","tags":["1-click clusters","automation","on-demand cloud"]},{"location":"public-cloud/teams/#change-a-teammates-role","title":"Change a teammate\u2019s role","text":"<p>To change the role of a person on your team from Member to Admin, click \u22ee where the person is listed in your Team dashboard, then choose Change to Admin.</p> <p>Conversely, to change the role of a person on your team from Admin to Member, click \u22ee where the person is listed in your Team dashboard, then choose Change to Member.</p>","tags":["1-click clusters","automation","on-demand cloud"]},{"location":"public-cloud/teams/#close-a-teammates-account","title":"Close a teammate\u2019s account","text":"<p>To close a teammate\u2019s account, click the \u22ee where your teammate is listed in your Team dashboard. Then, choose Deactivate user.</p> <p>Important</p> <p>Carefully review the information in the dialog box that pops up.</p>","tags":["1-click clusters","automation","on-demand cloud"]},{"location":"public-cloud/teams/#change-team-name","title":"Change team name","text":"<p>To change the name of your team, click Settings at the bottom-left of the dashboard, then click Edit team name. Enter a new name for your team, then click Update team name.</p>","tags":["1-click clusters","automation","on-demand cloud"]},{"location":"public-cloud/1-click-clusters/","title":"Introduction","text":"<p>1-Click Clusters (1CC) are clusters of GPU and CPU instances consisting of 16 to 512 NVIDIA H100 SXM Tensor Core GPUs. Compute (GPU) nodes are interconnected over an NVIDIA Quantum-2 400Gb/s InfiniBand non-blocking fabric in a rail-optimized topology, providing peer-to-peer GPUDirect RDMA communication of up to 3200Gb/s. All nodes are connected via 2x100Gb/s Ethernet for IP communication and are connected to the Internet via 2x100Gb/s Direct Internet Access (DIA) circuits.</p> <p>Each 1CC includes 3x CPU management (head) nodes for use as jump boxes (bastion hosts) and for cluster administration and job scheduling. These management nodes are assigned public IP addresses and are directly accessible over the Internet via SSH.</p> <p>All nodes can be directly accessed using Jupyter Notebook from the Lambda On-Demand Cloud dashboard.</p> <p>1CC nodes are in an isolated private network and can communicate freely with each other using private IP addresses.</p> <p>Generic CPU nodes can optionally be launched in the same regions as 1CCs. These generic CPU nodes run independently of 1CCs and don't terminate when 1CC reservations end.</p> <p>Each compute node includes 24TB of usable local ephemeral NVMe storage. Each management node includes 208GB of usable local ephemeral NVMe storage. Persistent storage is automatically created and attached to each 1CC node, and can also be attached to on-demand instances. Existing file systems in the same region can additionally be attached. You\u2019re billed only for the storage you actually use.</p> <p>All 1CC nodes are preinstalled with Ubuntu 22.04 LTS and Lambda Stack, including NCCL, Open MPI, PyTorch with DDP and FSDP support, TensorFlow, OFED, and other popular libraries and frameworks for distributed ML workloads, allowing ML engineers and researchers to begin their large-scale experiments and other work immediately after launching a 1CC.</p>","tags":["1-click clusters","distributed training"]},{"location":"public-cloud/1-click-clusters/#reserving-1-click-clusters","title":"Reserving 1-Click Clusters","text":"<p>To reserve a 1CC:</p> <ol> <li>Log into your Lambda On-Demand Cloud account. If you don\u2019t already have an account, you can create an account for free.</li> <li>If you haven\u2019t already, generate or add an SSH key for your 1CC.</li> <li>Navigate to the 1-Click Clusters page of the Lambda Cloud dashboard and click Reserve 1-Click Cluster.</li> <li>Use the 1CC reservation wizard to reserve your cluster.<ol> <li>Type/Duration: Enter the duration in weeks that you want to reserve the cluster, and then select the cluster type you want to launch.</li> <li>Region: Select the region in which you want to launch your cluster.</li> <li>Filesystem: Optionally, select an existing file system that you want to attach to the cluster, or click Create a new  filesystem to create a new file system. If you don\u2019t want to attach an existing file system or create a new file system, click Don\u2019t attach a filesystem (an empty one will be created).</li> <li>SSH Key: Select the SSH key that you want to use to access your cluster, then click Next.</li> <li>Cluster name: Enter a name for your cluster, then click Continue to Payment. You\u2019re presented with a billing summary and the billing information you have saved to your Lambda On-Demand Cloud account.</li> </ol> </li> <li> <p>If needed, update your billing information and change the email address where you\u2019d like the invoice for your 1CC reservation to be sent.</p> <p>Note</p> <p>Only the invoice will be sent to this email address. All notifications and other information regarding your 1CC will be sent to the email address associated with your account.</p> </li> <li> <p>Review the listed policies and terms of service. If you agree to the policies and terms of service, click the checkbox under Terms and Conditions.</p> </li> <li>Click Submit reservation request. Within a few minutes, you\u2019ll receive an email confirming your 1CC reservation request.</li> </ol> <p>You can check the status of your 1CC reservation request or cancel your request by visiting the Instances page in the Lambda Cloud dashboard.</p>","tags":["1-click clusters","distributed training"]},{"location":"public-cloud/1-click-clusters/#completing-your-1-click-cluster-reservation","title":"Completing your 1-Click Cluster reservation","text":"<p>When your reservation is approved, you\u2019ll receive an email with your invoice and payment instructions. This email will be sent to the address you entered when making your reservation, which might be different from the email associated with your Lambda account.</p> <p>The invoice must be paid within 10 days of approval of your 1CC reservation. Otherwise, you risk losing your reservation. Daily reminder emails will be sent until the invoice is paid or the reservation is canceled.</p> <p>Your 1CC will automatically launch on the day your reservation begins. You\u2019ll be sent an email informing you that your cluster is launching, and that you should check your dashboard to monitor the status of your 1CC.</p>","tags":["1-click clusters","distributed training"]},{"location":"public-cloud/1-click-clusters/#accessing-your-1-click-cluster","title":"Accessing your 1-Click Cluster","text":"<p>Your 1CC's management nodes have public IP addresses and can be accessed directly via SSH. You can access your compute nodes via SSH using a management node as a jump box. You can also access both your management nodes and your compute nodes using Jupyter Notebook. If you want to access your management nodes through SSH, make sure your Firewall rules are configured to allow SSH traffic.</p> <p>To access your management nodes and compute nodes via SSH:</p> <ol> <li> <p>Log into your Lambda On-Demand Cloud account. Then, click 1-Click Clusters in the left sidebar of the dashboard.</p> </li> <li> <p>At the top of the table of your 1CC\u2019s nodes, next to SSH LOGIN, click SETUP.</p> <p></p> <p>Follow the instructions in the INITIAL CLUSTER SETUP dialog, then click Next.</p> <p>Tip</p> <p>Follow these instructions on any other computers you want to use to access your 1CC. Make sure those computers have the SSH key that you chose when you reserved your 1CC.</p> </li> <li> <p>Follow the instructions in the SETUP INTER-NODE PASSWORDLESS SSH dialog. You can now SSH into each of your nodes using their names, which you can obtain from the dashboard. You can also SSH from each of your nodes into the other nodes.</p> <p>Warning</p> <p>The computer where you\u2019re running the script must have both your cluster SSH config, downloaded in the previous step, and the SSH key that you chose when you reserved your 1CC.</p> </li> <li> <p>Click Done to close the dialog.</p> </li> </ol>","tags":["1-click clusters","distributed training"]},{"location":"public-cloud/1-click-clusters/#adding-ssh-keys","title":"Adding SSH keys","text":"<p>You can add SSH keys to your 1CC to allow others to log in.</p> <p>To add another SSH key:</p> <ol> <li> <p>From the dashboard, save the names of your 1CC nodes into a text file. You can do this by copy and pasting from the dashboard. The file should contain the names of your nodes, each on a separate line, and should look like:</p> <pre><code>us-east-2-1cc-node-1\nus-east-2-1cc-node-2\nus-east-2-1cc-node-3\nus-east-2-1cc-node-4\nus-east-2-1cc-head-1\nus-east-2-1cc-head-2\nus-east-2-1cc-head-3\n</code></pre> </li> <li> <p>Add the additional SSH key to your 1CC by running:</p> <p><pre><code>while read node; \\\ndo ssh -n -F CLUSTER-SSH-CONFIG \"$node\" \"echo 'PUBLIC-KEY' &gt;&gt; ~/.ssh/authorized_keys\" &amp;&amp; echo \"Key added to $node\"; \\\ndone &lt; LIST-OF-NODES\n</code></pre> In this command:</p> <ul> <li>Replace <code>CLUSTER-SSH-CONFIG</code> with the path to your cluster SSH config. The path should look like <code>~/.ssh/config.d/config.us-east-2-1cc</code>.</li> <li> <p>Replace <code>PUBLIC-KEY</code> with the public key you want to add to your 1CC nodes. Public keys look like:</p> <pre><code>ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIK5HIO+OQSyFjz0clkvg+48YAihYMo5J7AGKiq+9Alg8 user@hostname\n</code></pre> </li> <li> <p>Make sure to keep the single quotes (<code>' '</code>).</p> </li> <li>Replace <code>LIST-OF-NODES</code> with the name of the file containing the names of your nodes.</li> </ul> </li> </ol>","tags":["1-click clusters","distributed training"]},{"location":"public-cloud/1-click-clusters/#copying-data-to-your-1-click-cluster","title":"Copying data to your 1-Click Cluster","text":"<p>The recommended way to copy data to your 1CC is to use rsync. rsync allows you to copy files from your computer to your 1CC, as well as directly from a running on-demand instance to your 1CC.</p> <p>See our documentation to learn how to use rsync. When following the instructions, replace SERVER-IP with the name of one of your management nodes. Unless you copy your data to a persistent storage file system, your data will only be accessible from the nodes your data was copied to.</p> <p>If your data isn\u2019t saved to a persistent storage file system, you can copy your data from one node to another by following our instructions on copying files directly between remote servers. For higher performance, you can also use rsync to copy data from your persistent storage file system to your nodes\u2019 local ephemeral storage.</p>","tags":["1-click clusters","distributed training"]},{"location":"public-cloud/1-click-clusters/#ending-your-1-click-cluster-reservation","title":"Ending your 1-Click Cluster reservation","text":"<p>Beginning three days prior to your 1CC reservation ending, you\u2019ll be sent a daily email reminding you that your reservation is ending.</p> <p>Warning</p> <p>All data saved to ephemeral local storage will be deleted and unrecoverable once your reservation ends! You\u2019re responsible for backing up your data, for example, to a persistent storage file system.</p> <p>It\u2019s not currently possible to extend or renew your 1CC reservation. You\u2019ll need to request a new reservation.</p>","tags":["1-click clusters","distributed training"]},{"location":"public-cloud/1-click-clusters/security-posture/","title":"Security posture","text":"<p>This document describes the physical and logical security properties of the Lambda 1-Click Clusters\u2122 (1CC) product, including the default software configuration on cluster nodes. The following diagram illustrates the 1CC network architecture:</p> <p></p>","tags":["1-click clusters","security"]},{"location":"public-cloud/1-click-clusters/security-posture/#compute-gpu-nodes","title":"Compute (GPU) nodes","text":"<p>1CC compute nodes run on single-tenant hardware with tenant isolation enforced using logical network segmentation. Underlying hardware resources, including GPUs, local storage, memory, and network interfaces, aren't shared with or accessible by any other customers.</p> <p>Compute nodes live on a dedicated network segment with no inbound connectivity from the firewall. Compute nodes can be reached either by using a management node as a jump box or via a public reverse tunnel to a JupyterLab service running on each compute node. Each JupyterLab instance is configured with a unique, random authentication token shared via the Lambda Public Cloud dashboard. For more information, see the JupyterLab security documentation .</p> <p>Customers have full control over the configuration of their compute nodes and can reconfigure them at will.</p>","tags":["1-click clusters","security"]},{"location":"public-cloud/1-click-clusters/security-posture/#management-head-nodes","title":"Management (Head) nodes","text":"<p>1CC management nodes run on multi-tenant hardware with tenant isolation enforced using hardware virtualization. Underlying resources, including local storage, memory, and network interfaces, are shared with other customers.</p> <p>By default, management nodes are directly accessible over the internet via SSH and via a public reverse tunnel to a JupyterLab service running on each management node. Each JupyterLab instance is configured with a unique, random authentication token shared via the Lambda Public Cloud dashboard. For more information, see JupyterLab's security documentation .</p> <p>Customers can configure their own inbound firewall rules to expand or reduce the exposure of their management nodes. Customers have full control over the configuration of their management nodes and can reconfigure them at will.</p>","tags":["1-click clusters","security"]},{"location":"public-cloud/1-click-clusters/security-posture/#ethernet-connect","title":"Ethernet connect","text":"<p>1CC compute and management nodes share a logically isolated Ethernet switching fabric. Logical isolation ensures that customers have no interaction with each other.</p>","tags":["1-click clusters","security"]},{"location":"public-cloud/1-click-clusters/security-posture/#infiniband-interconnect","title":"InfiniBand interconnect","text":"<p>1CC compute nodes share a specially isolated InfiniBand fabric that ensures that customer traffic only ever transits physical IB links dedicated to that customer, ensuring complete isolation of customer InfiniBand traffic.</p>","tags":["1-click clusters","security"]},{"location":"public-cloud/1-click-clusters/security-posture/#persistent-file-storage","title":"Persistent file storage","text":"<p>All 1CC compute and management nodes have pre-configured access to a customer-specific portion of a multi-tenant persistent file storage system. The storage system is on an isolated network accessible only to management and compute nodes. All data on the storage system is encrypted at rest using industry standard algorithms and parameters.</p>","tags":["1-click clusters","security"]},{"location":"public-cloud/1-click-clusters/security-posture/#lambda-employee-access","title":"Lambda employee access","text":"<p>Logical and physical access to 1CC infrastructure, such as network and storage solutions, is limited to Lambda employees with a specific need for access. Underlying 1CC infrastructure is monitored for security, utilization, performance, and reliability. Lambda employees do not access customer environments without customers' express authorization. Customers are responsible for all security instrumentation and monitoring of their management and compute nodes.</p>","tags":["1-click clusters","security"]},{"location":"public-cloud/1-click-clusters/security-posture/#physical-security","title":"Physical security","text":"<p>1CC infrastructure is located in secure data centers with the following access controls:</p> <ul> <li>Qualified in-house security personnel on site 24x7x365</li> <li>CCTV surveillance, with a minimum 90 days of retention</li> <li>Multiple security checkpoints:<ul> <li>Controlled fenced access into data center property</li> <li>Lobby mantraps in data center hallway</li> <li>2FA Access Control (Biometric and RFID badge) into data hall</li> <li>2FA Access Control (Biometric and RFID badge) into secured cage</li> </ul> </li> </ul> <p>Authorized Lambda employees may access data centers for maintenance, upgrades, or other hardware infrastructure work.</p>","tags":["1-click clusters","security"]},{"location":"public-cloud/1-click-clusters/serving-llama-3_1-405b/","title":"How to serve the Llama 3.1 405B model using a Lambda 1-Click Cluster","text":"<p>In this tutorial, you'll learn how to use a 1-Click Cluster (1CC) to serve the Meta Llama 3.1 405B model using vLLM with pipeline parallelism.</p> <p>Note</p> <p>You need a Hugging Face account to download the Llama 3.1 405B model. You also need a User Access Token with the Read role.</p> <p>Before you can download the Llama 3.1 405B model, you need to review and accept the model's license agreement. Once you accept the agreement, a request to access the repository will be submitted.</p> <p>You can see the status of the request in your Hugging Face account settings.</p>","tags":["1-click clusters","distributed training"]},{"location":"public-cloud/1-click-clusters/serving-llama-3_1-405b/#download-the-llama-31-405b-model-and-set-up-a-head-node","title":"Download the Llama 3.1 405B model and set up a head node","text":"<p>First, follow the instructions for accessing your 1CC.</p> <p>Once you've followed the instructions for accessing your 1CC, SSH into one of your 1CC GPU nodes. This GPU node will be set up as a head node for this tutorial and will be referred to in this tutorial as the \"head node.\"</p> <p>On the head node, set environment variables needed for this tutorial by running:</p> <pre><code>export HEAD_IP=HEAD-IP\nexport SHARED_DIR=/home/ubuntu/FILE-SYSTEM-NAME\nexport HF_TOKEN=HF-TOKEN\nexport HF_HOME=\"${SHARED_DIR}/.cache/huggingface\"\nexport MODEL_REPO=meta-llama/Meta-Llama-3.1-405B-Instruct\n</code></pre> <p>Replace <code>HEAD-IP</code> with the IP address of the head node. You can obtain the IP address from the 1-Click Clusters dashboard.</p> <p>Replace <code>FILE-SYSTEM-NAME</code> with the name of your 1CC's persistent storage file system.</p> <p>Replace <code>HF-TOKEN</code> with your Hugging Face User Access Token.</p> <p>Then, run:</p> <pre><code>mkdir -p \"${HF_HOME}\"\n\npython3 -m venv llama-3.1\nsource llama-3.1/bin/activate\npip install -U huggingface_hub[cli]\n\nhuggingface-cli login --token \"${HF_TOKEN}\"\nhuggingface-cli download \"${MODEL_REPO}\"\n</code></pre> <p>These commands:</p> <ol> <li> <p>Create and activate a    Python virtual environment    on the head node for this tutorial.</p> </li> <li> <p>Download the Llama 3.1 405B model to your 1CC's persistent storage file    system.</p> </li> </ol> <p>Note</p> <p>The Llama 3.1 405B model is about 2.3TB in size and can take several hours to download.</p> <p>Still on the head node, start a tmux session by running <code>tmux</code>.</p> <p>Then, run:</p> <pre><code>curl -o \"${SHARED_DIR}/run_cluster.sh\" https://raw.githubusercontent.com/vllm-project/vllm/main/examples/run_cluster.sh\n\nsudo bash \"${SHARED_DIR}/run_cluster.sh\" \\\n     vllm/vllm-openai \\\n     \"${HEAD_IP}\" \\\n     --head \"${HF_HOME}\" \\\n     --privileged -e NCCL_IB_HCA=^mlx5_0\n</code></pre> <p>These commands:</p> <ol> <li> <p>Download to your shared persistent storage file system a helper script to    set up vLLM for multi-node inference and serving.</p> </li> <li> <p>Run the script to start a    Ray cluster    for serving the Llama 3.1 405B model using vLLM. The Ray cluster uses your    1CC's InfiniBand fabric for optimal performance.</p> </li> </ol>","tags":["1-click clusters","distributed training"]},{"location":"public-cloud/1-click-clusters/serving-llama-3_1-405b/#connect-another-gpu-node-to-the-head-node","title":"Connect another GPU node to the head node","text":"<p>Next, you'll connect another of your 1CC's GPUs nodes to the head node. This other GPU node will be referred to below as the \"worker node.\"</p> <p>In a new terminal, SSH into the worker node, then set environment variables needed for this tutorial by running:</p> <pre><code>export HEAD_IP=HEAD-IP\nexport SHARED_DIR=/home/ubuntu/FILE-SYSTEM-NAME\nexport HF_HOME=\"${SHARED_DIR}/.cache/huggingface\"\n</code></pre> <p>Replace <code>HEAD-IP</code> with the IP address of the head node.</p> <p>Replace <code>FILE-SYSTEM-NAME</code> with the name of your 1CC's persistent storage file system.</p> <p>Run <code>tmux</code> to start a new tmux session. Then, run:</p> <pre><code>sudo bash \"${SHARED_DIR}/run_cluster.sh\" \\\n     vllm/vllm-openai \\\n     \"${HEAD_IP}\" \\\n     --worker \\\n     \"${HF_HOME}\" \\\n     --privileged -e NCCL_IB_HCA=^mlx5_0\n</code></pre> <p>This command connects the worker node to the head node.</p>","tags":["1-click clusters","distributed training"]},{"location":"public-cloud/1-click-clusters/serving-llama-3_1-405b/#check-the-status-of-the-ray-cluster-and-serve-the-llama-31-405b-model","title":"Check the status of the Ray cluster and serve the Llama 3.1 405B model","text":"<p>Still on the worker node, press Ctrl + B, then press Ctrl + C to open a new tmux window. Then, run:</p> <pre><code>sudo docker exec -it node /bin/bash\n</code></pre> <p>Check the status of the Ray cluster by running:</p> <pre><code>ray status\n</code></pre> <p>You should see output similar to:</p> <pre><code>======== Autoscaler status: 2024-07-25 00:20:50.831620 ========\nNode status\n---------------------------------------------------------------\nActive:\n 1 node_d86d9f0f1894c2e463d8168530f6745e32beb08ddf3b908d229d8527\n 1 node_37af7f860c4bab2e035b5a55ba06e2e49dba9fa891d65f8264648804\nPending:\n (no pending nodes)\nRecent failures:\n (no failures)\n\nResources\n---------------------------------------------------------------\nUsage:\n 0.0/416.0 CPU\n 16.0/16.0 GPU (16.0 used of 16.0 reserved in placement groups)\n 0B/3.43TiB memory\n 0B/19.46GiB object_store_memory\n\nDemands:\n (no resource demands)\n</code></pre> <p>This output shows 2 active nodes (the head node and the worker node) and 16 GPUs in the Ray cluster.</p> <p>Press Ctrl + B, then press Ctrl + C to open a new tmux window.</p> <p>Obtain the name of the Llama 3.1 405B model snapshot by running:</p> <pre><code>ls /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-405B-Instruct/snapshots\n</code></pre> <p>Then, run the following command to begin serving the Llama 3.1 405B model:</p> <pre><code>vllm serve \"/root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-405B-Instruct/snapshots/SNAPSHOT\" --tensor-parallel-size 8 --pipeline-parallel-size 2\n</code></pre> <p>Replace <code>SNAPSHOT</code> with the name of the Llama 3.1 405B model snapshot. The name of the snapshot should be similar to <code>e04e3022cdc89bfed0db69f5ac1d249e21ee2d30</code>.</p> <p>Note</p> <p>It can take 15 minutes or more before the model is loaded onto the GPUs and ready to be served.</p> <p>You should begin seeing output similar to:</p> <pre><code>INFO 07-25 04:17:41 api_server.py:219] vLLM API server version 0.5.3.post1\nINFO 07-25 04:17:41 api_server.py:220] args: Namespace(model_tag='/root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-405B-Instruct/snapshots/e04e3022cdc89bfed0db69f5ac1d249e21ee2d30', host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='/root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-405B-Instruct/snapshots/e04e3022cdc89bfed0db69f5ac1d249e21ee2d30', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=None, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=2, tensor_parallel_size=8, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=False, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, num_speculative_tokens=None, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, engine_use_ray=False, disable_log_requests=False, max_log_len=None, dispatch_function=&lt;function serve at 0x7de812d13520&gt;)\nINFO 07-25 04:17:41 config.py:715] Defaulting to use ray for distributed inference\n</code></pre> <p>The Llama 3.1 405B model is ready to be served once you see output similar to:</p> <pre><code>INFO:     Started server process [24469]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n</code></pre>","tags":["1-click clusters","distributed training"]},{"location":"public-cloud/1-click-clusters/serving-llama-3_1-405b/#test-the-llama-31-405b-model","title":"Test the Llama 3.1 405B model","text":"<p>Still on the worker node, press Ctrl + B, then press Ctrl + C to open a new tmux window.</p> <p>Then, run:</p> <pre><code>python3 -m venv llama-3.1\nsource llama-3.1/bin/activate\npip install -U openai\n\ncurl -o ${SHARED_DIR}/inference_test.py 'https://raw.githubusercontent.com/vllm-project/vllm/main/examples/openai_chat_completion_client.py'\n</code></pre> <p>These commands:</p> <ol> <li>Create and activate a    Python virtual environment    on the worker node.</li> <li>Download the Open AI chat completion client.</li> <li> <p>Finally, to test the Llama 3.1 405B model, run:</p> <pre><code>python3 ${SHARED_DIR}/inference_test.py\n</code></pre> <p>You should see output similar to:</p> <pre><code>Chat completion results:\nChatCompletion(id='chat-8eba7fa7e2f7442aafa82a1683bfc77f', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The 2020 World Series was played at Globe Life Field in Arlington, Texas. This was a neutral site due to COVID-19 restrictions and was also referred to as a \"bubble\" environment.', role='assistant', function_call=None, tool_calls=[]), stop_reason=None)], created=1721884178, model='/root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-405B-Instruct/snapshots/e04e3022cdc89bfed0db69f5ac1d249e21ee2d30', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=41, prompt_tokens=59, total_tokens=100))\n</code></pre> </li> </ol>","tags":["1-click clusters","distributed training"]},{"location":"public-cloud/1-click-clusters/support/","title":"Support","text":"<p>At Lambda, we recognize that exceptional support is critical to maximizing the value of your 1-Click Cluster (1CC) deployment. Our world-class support team is dedicated to ensuring your success at every stage, from deployment to daily operations.</p>","tags":["1-click-clusters","support"]},{"location":"public-cloud/1-click-clusters/support/#support-team","title":"Support team","text":"<p>When you choose Lambda, you gain access to a dedicated support team of seasoned professionals with deep expertise in AI/ML infrastructure. This team includes:</p> <ul> <li>Customer Success Manager (CSM): Your main point of contact post-sales, responsible for ensuring the delivery of your solution and your overall satisfaction.</li> <li>Technical Account Manager(TAM): An expert specialist who understands the specifics of your deployment, provides ongoing technical guidance, and escalates any complex issues. </li> <li>Machine Learning Expert (MLE): An AI/ML expert within Lambda will provide guidance on how to integrate and scale AI workloads.    </li> <li>Support Engineering: Lambda\u2019s Support team, available 24/7 through our ticketing portal, is well-versed in 1CC and will be able to respond to any technical support request, escalating issues and incidents early and often.</li> </ul>","tags":["1-click-clusters","support"]},{"location":"public-cloud/1-click-clusters/support/#support-scope","title":"Support scope","text":"<p>Lambda classifies incoming support tickets into three categories:</p> <ul> <li>In scope</li> <li>Best effort</li> <li>Out of scope</li> </ul>","tags":["1-click-clusters","support"]},{"location":"public-cloud/1-click-clusters/support/#in-scope","title":"In scope","text":"<ul> <li>Hardware and Infrastructure: Full support for CPU/GPU VMs, physical hosts, and networking components.  </li> <li>Software Environment: Assistance with Lambda Stack, OFED drivers, Jupyter Notebooks, and essential ML tools like NCCL and Open MPI.  </li> <li>Networking and Storage: Management and troubleshooting for Ethernet and InfiniBand networks, as well as persistent and local storage.  </li> <li>Slurm Installation: Guidance on Slurm setup and basic troubleshooting to streamline your job scheduling processes.  </li> <li>Managed Kubernetes*: If purchased as an add-on, our team of Kubernetes experts will be there to help you.</li> </ul>","tags":["1-click-clusters","support"]},{"location":"public-cloud/1-click-clusters/support/#best-effort","title":"Best effort","text":"<p>Our Support team is dedicated to delivering world-class customer service and technical expertise. We empower our engineers to go the extra mile, even when it means stepping beyond the standard scope of our support and engineered products to provide innovative solutions.</p> <p>In these exceptional cases, we ensure our customers understand that while we strive to help, there may be no guaranteed outcome. Any solutions we provide under these circumstances won't be fully supported in the future, and Lambda cannot assume responsibility for any potential impacts.</p>","tags":["1-click-clusters","support"]},{"location":"public-cloud/1-click-clusters/support/#out-of-scope","title":"Out of scope","text":"<p>Some support requests cannot be supported, such as:</p> <ul> <li>Troubleshooting customer code  </li> <li>3rd party applications/software installed after cluster handoff  </li> <li>Network/VPN connections to your cluster</li> </ul>","tags":["1-click-clusters","support"]},{"location":"public-cloud/1-click-clusters/support/#sla","title":"SLA","text":"<p>Our focus on prompt and reliable service is supported by the clearly established response times in our agreements:</p> Incident Level Definition Initial Response Time Severity 1 A critical Services problem in which the Services (i) are down, inoperable, inaccessible, or unavailable, (ii) otherwise materially cease operation, or (iii) perform or fail to perform so as to prevent useful work from being done. 4 hours Severity 2 A Services problem in which the Services (i) are severely limited or major functions are performing improperly, and the situation is significantly impacting certain portions of the Services users\u2019 operations or productivity, or (ii) have been interrupted but recovered, and there is high risk of recurrence. 8 hours Severity 3 A minor or cosmetic Services problem that (i) is an irritant, affects non-essential functions, or has minimal business operations impact, (ii) is localized or has isolated impact, (iii) is an operational nuisance, (iv) results in documentation errors, or (v) is otherwise not Severity 1 or Severity 2, but represents a failure of services to conform to specifications provided 10 hours Service Request Requests for action or tasks that are not generated by an incident. 24 hours","tags":["1-click-clusters","support"]},{"location":"public-cloud/on-demand/","title":"Overview","text":"<p>On-Demand Cloud (ODC) provides on-demand access to Linux-based, GPU-backed virtual machine instances.</p>"},{"location":"public-cloud/on-demand/#instance-types","title":"Instance types","text":"<p>ODC offers a variety of predefined instance types to support different workload requirements. Available GPUs include the state-of-the-art NVIDIA H100 Tensor Core GPU, NVIDIA A100 Tensor Core GPU, and NVIDIA A10 Tensor Core GPU, as well as several earlier models. Each instance you create is tied to a specific geographical region. For a list of available regions, see the Regions section below.</p> <p>Select instance types are backed by GPUs that feature NVIDIA SXM. SXM offers improved bandwidth between the NVIDIA GPUs in a single physical server.</p> <p>Warning</p> <p>Lambda prohibits cryptocurrency mining on ODC instances.</p> <p>As of October 2024, ODC offers the following instance types:</p> GPU Number of GPUs VRAM vCPU cores RAM Root volume size H100 SXM 8 80 GB 208 1800 GiB 26 TiB 8 80 GB 208 1800 GiB 22 TiB 4 80 GB 104 900 GiB 11 TiB 2 80 GB 52 450 GiB 5.5 TiB 1 80 GB 26 225 GiB 2.75 TiB H100 PCIe 1 80 GB 26 225 GiB 1 TiB A100 SXM 8 80 GB 240 1800 GiB 19.5 TiB 8 40 GB 124 1800 GiB 5.8 TiB 1 40 GB 30 220 GiB 512 GiB A100 PCIe 4 40 GB 120 900 GiB 1 TiB 2 40 GB 60 450 GiB 1 TiB 1 40 GB 30 225 GiB 512 GiB A10 1 24 GB 30 226 GiB 1.3 TiB A6000 4 48 GB 56 400 GiB 1 TiB 2 48 GB 28 200 GiB 1 TiB 1 48 GB 14 100 GiB 512 GiB Tesla V100 8 16 GB 88 448 GiB 5.8 TiB RTX 6000 1 24 GB 14 46 GiB 512 GiB"},{"location":"public-cloud/on-demand/#storage","title":"Storage","text":""},{"location":"public-cloud/on-demand/#root-volume","title":"Root volume","text":"<p>Each new instance comes with a root volume of a predefined size. The specific volume size depends on which instance type you choose. To see which volume sizes are associated with each instance type, check out the instance type table above.</p>"},{"location":"public-cloud/on-demand/#filesystems","title":"Filesystems","text":"<p>When you create a new instance, you're prompted to attach a filesystem. A filesystem is a unit of networked persistent storage you can connect to your instance. Filesystems are typically several orders of magnitude larger than your root volume, and are an ideal place to store both your instance state and your large datasets.</p> <p>To use a filesystem with your instance, you must attach it during the instance creation process. The filesystem must also reside in the same region as your instance.</p> <p>For more information about filesystems, see Filesystems.</p>"},{"location":"public-cloud/on-demand/#mount-point","title":"Mount point","text":"<p>When you mount a filesystem to your instance, the filesystem is available at:</p> <pre><code>/home/ubuntu/&lt;FILESYSTEM_NAME&gt;\n</code></pre>"},{"location":"public-cloud/on-demand/#billing","title":"Billing","text":"<p>Filesystems are billed per GB used per month, in increments of one hour. If you delete an instance but not its associated filesystem, you'll continue to be billed for the filesystem. For more details on filesystem pricing, see Filesystems.</p>"},{"location":"public-cloud/on-demand/#size-limits","title":"Size limits","text":"<p>ODC filesystem size limits vary by region. In general, filesystems have a capacity of 8 EB (8,000,000 TB), and you can have a total of 24 filesystems. In the Texas, USA (us-south-1) region, filesystem size is limited to 10 TB.</p>"},{"location":"public-cloud/on-demand/#network","title":"Network","text":""},{"location":"public-cloud/on-demand/#connection-options","title":"Connection options","text":"<p>You connect to your instance through a standard SSH connection. For information on creating and managing SSH keys, see Dashboard &gt; Add, generate, and delete SSH keys and Cloud API &gt; Managing SSH keys.</p>"},{"location":"public-cloud/on-demand/#firewall-defaults","title":"Firewall defaults","text":"<p>You can define inbound TCP and UDP firewall rules on the Firewall page in the Lambda Cloud dashboard. By default, only port 22 (SSH) is open.</p> <p>ODC allows ICMP traffic by default, as many network diagnostic tools rely on ICMP to determine where connectivity issues are occurring. If you'd prefer to restrict ICMP traffic, you can do so on the Firewall page.</p>"},{"location":"public-cloud/on-demand/#regions","title":"Regions","text":"<p>Lambda resources are hosted in multiple locations worldwide. Not every instance type will be available in every region.</p> Region Physical location asia-northeast-1 Tokyo, Japan asia-northeast-2 Osaka, Japan asia-south-1 India europe-central-1 Germany me-west-1 Israel us-east-1 Virginia, USA us-east-2 Washington DC, USA us-midwest-1 Illinois, USA us-south-1 Texas, USA us-south-2 North Texas, USA us-south-3 Central Texas, USA us-west-1 California, USA us-west-2 Arizona, USA us-west-3 Utah, USA"},{"location":"public-cloud/on-demand/#instance-management","title":"Instance management","text":""},{"location":"public-cloud/on-demand/#dashboard","title":"Dashboard","text":"<p>In addition to the firewall settings mentioned earlier, you can manage your instances, filesystems, SSH keys, API keys, and more through the Lambda Cloud dashboard. For more information, see Dashboard.</p>"},{"location":"public-cloud/on-demand/#cloud-api","title":"Cloud API","text":"<p>You can perform administrative tasks such as creating, restarting, listing, and terminating your instances through the Lambda Cloud API. For more details, see Cloud API.</p>"},{"location":"public-cloud/on-demand/#preinstalled-software","title":"Preinstalled software","text":"<p>Each ODC instance runs Ubuntu 22.04 LTS. Lambda also preinstalls the Lambda Stack, a standard set of AI/ML-related drivers, tools, and frameworks, on each instance:</p> <ul> <li>NVIDIA tools, libraries, and drivers: CUDA, cuDNN, NCCL, NVIDIA container   toolkit, NVIDIA driver</li> <li>Deep learning frameworks and libraries: TensorFlow, torchvision, Keras,   PyTorch\u00ae, JAX, Triton</li> <li>Dev tools: Git, Vim, Emacs, Valgrind, tmux, screen, htop, build-essential</li> </ul> <p>For more information on Lambda Stack, see Lambda Stack.</p> <p>In addition, each ODC instance provides a JupyterLab installation for creating and managing Jupyter notebooks. You can access your instance's JupyterLab by visiting the Instances page in the Lambda Cloud dashboard and clicking Cloud IDE in your instance's row.</p>"},{"location":"public-cloud/on-demand/#next-steps","title":"Next steps","text":"<ul> <li>Visit the Lambda Cloud portal</li> <li>Explore the Lambda Cloud API</li> </ul>"},{"location":"public-cloud/on-demand/billing/","title":"Billing","text":""},{"location":"public-cloud/on-demand/billing/#how-are-on-demand-instances-billed","title":"How are on-demand instances billed?","text":"<p>Before you can launch on-demand instances, you need to add a credit card to your account using the dashboard. We'll make a $10 pre-authorization charge to make sure the card is valid, similar to how gas stations and hotels do. The charge will be refunded in a few days.</p> <p>On-demand instances are billed in one-minute increments with no minimum. This means, for example, you can use instances for only one minute and be billed for only one minute. Billing begins when instances successfully boot and pass our health checks.</p> <p>Warning</p> <p>You will be billed for all minutes that an instance is running, even if the instance isn't actively being used.</p> <p>The GPU Cloud dashboard allows you to view your resource usage.</p> <p>Invoices are sent weekly for the previous week's usage.</p> <p>Note</p> <p>On-demand instances require us to maintain excess capacity at all times so we can meet the changing workloads of our customers. For this reason, on-demand instances are priced higher than reserved instances.</p> <p>Conversely, we offer reserved GPU Cloud instances at a significant savings over on-demand instances, since they allow us to more accurately determine our capacity needs ahead of time.</p> <p>The GPU Cloud dashboard allows you to view your resource usage .</p> <p>Invoices are sent weekly for the previous week's usage.</p>"},{"location":"public-cloud/on-demand/billing/#how-are-file-systems-billed","title":"How are file systems billed?","text":"<p>File systems are billed per GB used per month, in increments of 1 hour.</p> <p>For example, based on the price of $0.20 per GB used per month:</p> <ul> <li>If you use 1,000 GB of your file system capacity for an entire month (30 days,   or 720 hours), you'll be billed $200.00.</li> <li>If you use 1,000 GB of your file system capacity for a single day (24 hours),   you'll be billed $6.67.</li> </ul> <p>Note</p> <p>The actual price of persistent storage will be displayed when you create your file system.</p>"},{"location":"public-cloud/on-demand/billing/#how-do-i-change-my-payment-method-and-billing-address","title":"How do I change my payment method and billing address?","text":"<p>Use the dashboard to change your payment method and billing address .</p>"},{"location":"public-cloud/on-demand/billing/#do-i-have-to-pay-sales-tax-for-usage-of-lambda-gpu-cloud","title":"Do I have to pay sales tax for usage of Lambda GPU Cloud?","text":"<p>Sales tax is charged for usage of Lambda GPU Cloud.</p> <p>The sales tax you're charged is based on the billing address associated with your account. For this reason, it's important that you always keep your billing address up to date  using the Cloud dashboard.</p> <p>Note</p> <p>If you enter a business billing address outside the United States, you'll need to fill in the VAT/GST # field.</p> <p>See the European Commission's FAQs  to learn more about VAT identification numbers.</p> <p>Warning</p> <p>You won't be able to launch new instances if you don't have a billing address associated with your account.</p>"},{"location":"public-cloud/on-demand/billing/#why-is-my-card-being-declined","title":"Why is my card being declined?","text":"<p>Common reasons why card transactions are declined include:</p>"},{"location":"public-cloud/on-demand/billing/#the-card-is-a-prepaid-card-or-debit-card","title":"The card is a prepaid card or debit card","text":"<p>We don't accept debit cards or prepaid cards. We only accept major credit cards.</p>"},{"location":"public-cloud/on-demand/billing/#the-purchase-is-being-made-from-a-country-we-dont-support","title":"The purchase is being made from a country we don't support","text":"<p>We currently only support customers in the following regions:</p> <ul> <li>United States</li> <li>Canada</li> <li>Chile</li> <li>Iceland</li> <li>United Arab Emirates</li> <li>Saudi Arabia</li> <li>South Africa</li> <li>Israel</li> <li>Taiwan</li> <li>South Korea</li> <li>Japan</li> <li>Singapore</li> <li>Australia</li> <li>New Zealand</li> <li>United Kingdom</li> <li>Switzerland</li> <li>European Union (except for Romania)</li> </ul>"},{"location":"public-cloud/on-demand/billing/#the-purchase-is-being-made-while-youre-connected-to-a-vpn","title":"The purchase is being made while you're connected to a VPN","text":"<p>Purchases made while using a VPN are flagged as suspicious.</p>"},{"location":"public-cloud/on-demand/billing/#the-card-issuer-is-denying-our-pre-authorization-charge","title":"The card issuer is denying our pre-authorization charge","text":"<p>We make a $10 pre-authorization charge to a card before accepting it for payment. If the card issuer denies the pre-authorization charge, then we can't accept the card for payment.</p>"},{"location":"public-cloud/on-demand/billing/#wrong-cvv-or-zip-code-is-being-entered","title":"Wrong CVV or ZIP Code is being entered","text":"<p>Card purchases won't go through if the CVV (security code) is entered incorrectly. Also, card purchases will be denied if the ZIP Code doesn't match with the card billing address.</p> <p>If none of these are applicable to you, contact the Lambda Support team  for help.</p>"},{"location":"public-cloud/on-demand/billing/#what-happens-to-my-account-if-i-dont-pay-an-invoice","title":"What happens to my account if I don't pay an invoice?","text":"<p>Warning</p> <p>If an invoice remains unpaid after we've made 4 attempts to charge the card on file, we may suspend your account.</p> <p>If your account is suspended, your running instances may be terminated and your files may be deleted without prior notice.</p> <p>Eventually, all of your instances will be terminated and all of your persistent storage file systems will be deleted.</p> <p>Your account will be permanently banned from Lambda GPU Cloud. Your account will be referred for collection. Legal action may be taken against you.</p>"},{"location":"public-cloud/on-demand/billing/#how-are-refunds-given","title":"How are refunds given?","text":"<p>Refunds are given in the form of credit toward future Cloud usage only. The credit can't be redeemed for cash (for example, refund to a card) and can't be used toward purchases of other Lambda products.</p> <p>The credit expires 12 months after it is given.</p> <p>The credit is nontransferable and is subject to the Lambda Cloud Terms of Service .</p>"},{"location":"public-cloud/on-demand/billing/#can-you-provide-an-estimate-of-how-much-a-job-will-cost","title":"Can you provide an estimate of how much a job will cost?","text":"<p>We can't estimate how much your job will cost or how long it'll take to complete on one of our instances. This is because we don't know the details of your job, such as how your program works.</p> <p>However, the performance of our instances is close to what you'd expect from bare metal machines with the same GPUs.</p> <p>In order to estimate how much your job will cost or how long it'll take to complete, we suggest you create an instance and benchmark your program.</p> <p>Tip</p> <p>Check out our GPU benchmarks  to form a general idea of the performance provided by our instances. Keep in mind that real-world performance doesn't always match the performance provided by benchmarks.</p> <p>For help benchmarking or optimizing your ML jobs, contact our Machine Learning team .</p>"},{"location":"public-cloud/on-demand/dashboard/","title":"Using the Lambda Public Cloud dashboard","text":"<p>The dashboard  makes it easy to get started using Lambda GPU Cloud.</p> <p>From the dashboard, you can:</p> <ul> <li>Launch, restart, and terminate   instances</li> <li>Create and manage persistent storage file   systems</li> <li>Add, generate, and delete SSH keys</li> <li>Generate and delete API keys</li> <li>Use the Demos feature</li> <li>View usage</li> <li>Manage a Team</li> <li>Modify account settings</li> </ul>","tags":["1-click clusters","on-demand cloud"]},{"location":"public-cloud/on-demand/dashboard/#launch-restart-and-terminate-instances","title":"Launch, restart, and terminate instances","text":"","tags":["1-click clusters","on-demand cloud"]},{"location":"public-cloud/on-demand/dashboard/#launch-instances","title":"Launch instances","text":"<p>To launch an instance:</p> <ol> <li> <p>Click Instances      in the left sidebar of the dashboard.</p> <p>Then, click Launch instance at the top-right of the dashboard.</p> </li> <li> <p>Click the instance type that you want to launch.</p> </li> <li> <p>Click the region in which you want to launch the instance.</p> </li> <li> <p>Click the persistent storage file     system that you want to     attach to your instance.</p> <p>If you don't want to or can't attach a persistent storage file system to   your instance, click Don\u2019t attach a filesystem.</p> </li> <li> <p>Select the SSH key that you want to use     for your instance. Then, click Launch instance.</p> <p>Tip</p> <p>You can add additional SSH keys to your instance once your instance has launched.</p> </li> <li> <p>Review the license agreements and terms of service. If you agree to them,     click I agree to the above to launch your instance.</p> </li> </ol> <p>In the dashboard, you should now see your instance listed. Once your instance has finished booting, you\u2019ll be provided with the details needed to begin using your instance.</p> <p>Tip</p> <p>You can also launch instances using the Cloud API.</p> <p>You can also use the Cloud API to get details of a running instance.</p>","tags":["1-click clusters","on-demand cloud"]},{"location":"public-cloud/on-demand/dashboard/#restart-instances","title":"Restart instances","text":"<p>Restart instances by clicking the checkboxes next to the instances you want to restart. Then, click Restart at the top-right of the dashboard.</p>","tags":["1-click clusters","on-demand cloud"]},{"location":"public-cloud/on-demand/dashboard/#terminate-instances","title":"Terminate instances","text":"<p>Terminate instances by clicking the checkboxes next to the instances you want to terminate. Then, click Terminate at the top-right of the dashboard.</p> <p>When prompted to do so, type in erase data on instance, then click Terminate instances.</p> <p>Tip</p> <p>You can also terminate instances using the Cloud API.</p>","tags":["1-click clusters","on-demand cloud"]},{"location":"public-cloud/on-demand/dashboard/#create-and-manage-persistent-storage-file-systems","title":"Create and manage persistent storage file systems","text":"","tags":["1-click clusters","on-demand cloud"]},{"location":"public-cloud/on-demand/dashboard/#create-a-persistent-storage-file-system","title":"Create a persistent storage file system","text":"<p>To create a persistent storage file system:</p> <ol> <li> <p>Click Storage      in the left sidebar of the dashboard.</p> <p>Then, click Create filesystem at the top-right of the dashboard.</p> </li> <li> <p>Enter a name and select a region for your file system. Then click Create     filesystem.</p> </li> </ol> <p>You should now see your persistent storage file system listed in the dashboard.</p>","tags":["1-click clusters","on-demand cloud"]},{"location":"public-cloud/on-demand/dashboard/#add-generate-and-delete-ssh-keys","title":"Add, generate, and delete SSH keys","text":"","tags":["1-click clusters","on-demand cloud"]},{"location":"public-cloud/on-demand/dashboard/#add-or-generate-an-ssh-key","title":"Add or generate an SSH key","text":"","tags":["1-click clusters","on-demand cloud"]},{"location":"public-cloud/on-demand/dashboard/#add-an-existing-ssh-key","title":"Add an existing SSH key","text":"<ol> <li> <p>Click SSH keys      in the left sidebar of the dashboard.</p> <p>Then, click Add SSH key at the top-right of the dashboard.</p> </li> <li> <p>In the text input box, paste your public SSH key. Enter a name for your key,    then click Add SSH key.</p> </li> </ol>","tags":["1-click clusters","on-demand cloud"]},{"location":"public-cloud/on-demand/dashboard/#generate-a-new-ssh-key","title":"Generate a new SSH key","text":"<p>Instead of pasting your public SSH key as instructed, above, click Generate a new SSH key. Type in a name for your key, then click Create.</p> <p>The private key for your new SSH key will automatically download.</p> <p>Tip</p> <p>You can also use the Cloud API to add and generate SSH keys.</p>","tags":["1-click clusters","on-demand cloud"]},{"location":"public-cloud/on-demand/dashboard/#delete-ssh-keys","title":"Delete SSH keys","text":"<p>Delete SSH keys by clicking Delete at the far-right of the SSH key you want to delete.</p>","tags":["1-click clusters","on-demand cloud"]},{"location":"public-cloud/on-demand/dashboard/#generate-and-delete-api-keys","title":"Generate and delete API keys","text":"","tags":["1-click clusters","on-demand cloud"]},{"location":"public-cloud/on-demand/dashboard/#generate-api-keys","title":"Generate API keys","text":"<p>Generate API keys by clicking API keys  in the left sidebar of the dashboard.</p> <p>Then, click Generate API Key at the top-right of the dashboard.</p>","tags":["1-click clusters","on-demand cloud"]},{"location":"public-cloud/on-demand/dashboard/#delete-api-keys","title":"Delete API keys","text":"<p>Delete API keys by clicking Delete at the far-right of the API key you want to delete.</p>","tags":["1-click clusters","on-demand cloud"]},{"location":"public-cloud/on-demand/dashboard/#use-the-demos-feature","title":"Use the Demos feature","text":"<p>Use the Demos feature by clicking Demos  in the left sidebar of the dashboard.</p>","tags":["1-click clusters","on-demand cloud"]},{"location":"public-cloud/on-demand/dashboard/#view-usage","title":"View usage","text":"<p>View usage information by clicking Usage  in the left sidebar of the dashboard.</p>","tags":["1-click clusters","on-demand cloud"]},{"location":"public-cloud/on-demand/dashboard/#manage-a-team","title":"Manage a Team","text":"<p>Click Team  at the bottom of the left sidebar to access the Team feature.</p> <p>Learn how to manage a Team by reading our FAQ on getting started with the Team feature.</p>","tags":["1-click clusters","on-demand cloud"]},{"location":"public-cloud/on-demand/dashboard/#modify-account-settings","title":"Modify account settings","text":"<p>Click Settings  at the bottom of the left sidebar to modify your account settings, including your password and payment method.</p>","tags":["1-click clusters","on-demand cloud"]},{"location":"public-cloud/on-demand/demos/","title":"Demos","text":"<p>The Demos feature allows you to easily share your Gradio machine learning app (demo) both publicly and privately.</p> <p>To get started using the Demos feature, you need to:</p> <ol> <li>Add a demo to your Lambda GPU Cloud account.</li> <li>Host your demo on a new instance.</li> </ol> <p>Note</p> <p>It currently isn\u2019t possible to host a demo on an existing instance.</p> <p>Note</p> <p>The new instance hosting your demo can be used like any other Lambda GPU Cloud on-demand instance. For example, you can SSH into the instance and open Jupyter Notebook on the instance.</p> <p>As with other Lambda GPU Cloud on-demand instances, you\u2019re billed for all of the time the instance for your demo is running.</p> <p>Note</p> <p>The Demos feature can be hosted on multi-GPU instance types. However, Demos uses only one of the GPUs.</p> <p>Also, demos currently can\u2019t be hosted on H100 instances.</p>"},{"location":"public-cloud/on-demand/demos/#add-a-demo-to-your-lambda-gpu-cloud-account","title":"Add a demo to your Lambda GPU Cloud account","text":"<ol> <li> <p>In the left sidebar of the dashboard, click Demos. Then, click the Add demo button at the top-right of the dashboard.</p> <p>The Add a demo dialog will appear.</p> </li> <li> <p>Under Demo Source URL, enter the URL of the Git repository containing your demo\u2019s source code.</p> <p>Note</p> <p>The Demos feature looks in your Git repository for a file named <code>README.md</code>. If the file doesn\u2019t exist, or if the file doesn\u2019t contain the required properties, you\u2019ll receive a Demo misconfigured error.</p> <p>The <code>README.md</code> must have at the top a YAML block containing the following:</p> <pre><code>---\nsdk: gradio\nsdk_version: GRADIO-VERSION\napp_file: PATH-TO-APP-FILE\n---\n</code></pre> <p>Replace GRADIO-VERSION with the version of Gradio your demo is built with, for example, <code>3.24.1</code>.</p> <p>Replace PATH-TO-APP-FILE with the path to your Gradio application file (the file containing the Gradio interface code), relative to the root of your Git repository. For example, if your Gradio application file is named <code>app.py</code> and is located in the root directory of your Git repository, replace PATH-TO-APP-FILE with <code>app.py</code>.</p> <p>Properties other than <code>sdk</code>, <code>sdk_version</code>, and <code>app_file</code> are ignored by the Demos feature.</p> <p>Tip</p> <p>If you don\u2019t yet have your own demo, you can try the Demos feature using the demos created by Lambda\u2019s Machine Learning team. Demos created by Lambda\u2019s Machine Learning team include:</p> <ul> <li>Stable Diffusion Image Variations</li> <li>Image Mixer</li> <li>Avatar text to image 3. Under Visibility, choose:</li> <li>Public if you want to list your demo in the library of public models shared by the Lambda community.</li> <li>Unlisted if you want your demo accessible only by those who know your demo\u2019s URL. 4. Under Name, give your demo a name. If you choose to make your demo public, the name of your demo will appear in the Lambda library of public models. The name of your demo will also appear in your demo\u2019s URL. 5.  (Optional) Under Description, enter a description for your demo.</li> </ul> <p>The description shows under the name of your demo in your library of demos. If your demo is public, the description also shows under the name of your demo in the Lambda library of public models.</p> <p>Note</p> <p>You can\u2019t change the name or description of your demo once you add it. However, you can delete your demo then add it again. 6.  Click Add demo, then follow the prompts to launch a new instance to host your demo.</p> </li> </ol> <p>Tip</p> <p>To host a demo that\u2019s already added to your account, in the Demos dashboard, find the row containing the demo you want to host, then click Host.</p> <p>Your new instance will take several minutes to launch and for your demo to become accessible.</p> <p>Note</p> <p>The link to your demo might temporarily appear in the Instances dashboard, then disappear. This is expected behavior and doesn\u2019t mean your instance or demo is broken.</p> <p>The models used by demos are often several gigabytes in size, and can take 5 to 15 minutes to download and load.</p> <ol> <li>Once your instance is launched and your demo is accessible, a link with your demo\u2019s name will appear under the Demo column. Click the link to access your demo.</li> </ol>"},{"location":"public-cloud/on-demand/demos/#troubleshooting-demos","title":"Troubleshooting demos","text":"<p>If you experience trouble accessing your demo, the Demos logs can be helpful for troubleshooting.</p> <p>To view the Demos log files, SSH into your instance or open a terminal in Jupyter Notebook, then run:</p> <pre><code>sudo bash -c 'for f in /root/virt-sysprep-firstboot.log ~demo/bootstrap.log; do printf \"### BEGIN $f\\n\\n\"; cat $f; printf \"\\n### END $f\\n\\n\"; done &gt; demos_debug_logs.txt; printf \"### BEGIN journalctl -u lambda-demos.service\\n\\n$(journalctl -u lambda-demos.service)\\n\\n### END journalctl -u lambda-demos.service\" &gt;&gt; demos_debug_logs.txt'\n</code></pre> <p>This command will produce a file named <code>demos_debug_logs.txt</code> containing the logs for the Demos feature. You can review the logs from within your instance by running <code>less demos_debug_logs.txt</code>. Alternatively, you can download the file locally to review or share.</p> <p>Note</p> <p>The Lambda Support team provides only basic support for the Demos feature. However, assistance might be available in the community forum.</p> <p>If you\u2019re experiencing problems using the Demos feature, running the above command and providing the <code>demos_debug_logs.txt</code> file to the Support team can help with future improvements to the Demos feature.</p> <p>Here are some examples of how problems present in logs:</p>"},{"location":"public-cloud/on-demand/demos/#misconfigured-readmemd-file","title":"Misconfigured README.md file","text":"<pre><code>### BEGIN /home/demo/bootstrap.log\n\nCloning into '/home/demo/source'...\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 17, in &lt;module&gt;\n  File \"&lt;stdin&gt;\", line 15, in load\n  File \"pydantic/main.py\", line 526, in pydantic.main.BaseModel.parse_obj\n  File \"pydantic/main.py\", line 341, in pydantic.main.BaseModel.__init__\npydantic.error_wrappers.ValidationError: 3 validation errors for Metadata\nsdk\n  field required (type=value_error.missing)\nsdk_version\n  field required (type=value_error.missing)\napp_file\n  field required (type=value_error.missing)\nCreated symlink /etc/systemd/system/multi-user.target.wants/lambda-demos-error-server.service \u2192 /etc/systemd/system/lambda-demos-error-server.service.\nBootstrap failed: misconfigured\n\n### END /home/demo/bootstrap.log\n</code></pre>"},{"location":"public-cloud/on-demand/demos/#not-a-gradio-app","title":"Not a Gradio app","text":"<pre><code>### BEGIN /home/demo/bootstrap.log\n\nCloning into '/home/demo/source'...\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 17, in &lt;module&gt;\n  File \"&lt;stdin&gt;\", line 15, in load\n  File \"pydantic/main.py\", line 526, in pydantic.main.BaseModel.parse_obj\n  File \"pydantic/main.py\", line 341, in pydantic.main.BaseModel.__init__\npydantic.error_wrappers.ValidationError: 2 validation errors for Metadata\nsdk\n  unexpected value; permitted: 'gradio' (type=value_error.const; given=docker; permitted=('gradio',))\nsdk_version\n  field required (type=value_error.missing)\nCreated symlink /etc/systemd/system/multi-user.target.wants/lambda-demos-error-server.service \u2192 /etc/systemd/system/lambda-demos-error-server.service.\nBootstrap failed: misconfigured\n\n### END /home/demo/bootstrap.log\n</code></pre>"},{"location":"public-cloud/on-demand/getting-started/","title":"Getting started","text":"","tags":["1-click clusters","automation","on-demand cloud"]},{"location":"public-cloud/on-demand/getting-started/#can-my-data-be-recovered-once-ive-terminated-my-instance","title":"Can my data be recovered once I've terminated my instance?","text":"<p>Warning</p> <p>We cannot recover your data once you've terminated your instance! Before terminating an instance, make sure to back up all data that you want to keep.</p> <p>If you want to save data even after you terminate your instance, create a persistent storage file system .</p> <p>Note</p> <p>The persistent storage file system must be attached to your instance before you start your instance. The file system cannot be attached to your instance after you start your instance.</p> <p>When you create a file system, a directory with the name of your file system is created in your home directory. For example, if the name of your file system is PERSISTENT-FILE-SYSTEM, the directory is created at <code>/home/ubuntu/PERSISTENT-FILE-SYSTEM</code>. Data not stored in this directory is erased once you terminate your instance and cannot be recovered.</p>","tags":["1-click clusters","automation","on-demand cloud"]},{"location":"public-cloud/on-demand/getting-started/#can-i-pause-my-instance-instead-of-terminating-it","title":"Can I pause my instance instead of terminating it?","text":"<p>It currently isn't possible to pause (suspend) your instance rather than terminating it. But, this feature is in the works.</p> <p>Until this feature is implemented, you can use persistent storage file systems to imitate some of the benefits of being able to pause your instance.</p>","tags":["1-click clusters","automation","on-demand cloud"]},{"location":"public-cloud/on-demand/getting-started/#do-you-support-kubernetes-k8s","title":"Do you support Kubernetes (K8s)?","text":"<p>Yes. You can install and use Kubernetes, also known as K8s, on on-demand instances and 1-Click Clusters .</p> <p>Additionally, Managed Kubernetes and Pre-Installed Kubernetes are available for 1-Click Clusters and Reserved Cloud .</p> <p>Managed Kubernetes includes:</p> <ul> <li>Kubernetes installation and upgrades.</li> <li>Control plane maintenance and high-availability.</li> <li>NVIDIA GPU Operator      installed and configured to deploy and manage NVIDIA GPUs in a Kubernetes   cluster.</li> <li>Detecting node failures, node pool adjustment and failed hardware replacement.</li> <li>Gathering chassis and cluster metrics and proactive monitoring.</li> </ul> <p>See our  Managed Kubernetes Product Outline to learn more.</p>","tags":["1-click clusters","automation","on-demand cloud"]},{"location":"public-cloud/on-demand/getting-started/#why-cant-my-program-find-the-nvidia-cudnn-library","title":"Why can't my program find the NVIDIA cuDNN library?","text":"<p>Unfortunately, the NVIDIA cuDNN license limits how cuDNN can be used on our instances.</p> <p>On our instances, cuDNN can only be used by the PyTorch\u00ae framework and TensorFlow library installed as part of Lambda Stack.</p> <p>Other software, including PyTorch and TensorFlow installed outside of Lambda Stack, won't be able to find and use the cuDNN library installed on our instances.</p> <p>Tip</p> <p>Software outside of Lambda Stack usually looks for the cuDNN library files in <code>/usr/lib/x86_64-linux-gnu</code>. However, on our instances, the cuDNN library files are in <code>/usr/lib/python3/dist-packages/tensorflow</code>.</p> <p>Creating symbolic links, or \"symlinks,\" for the cuDNN library files might allow your program to find the cuDNN library on our instances.</p> <p>Run the following command to create symlinks for the cuDNN library files:</p> <pre><code>for cudnn_so in /usr/lib/python3/dist-packages/tensorflow/libcudnn*; do\n  sudo ln -s \"$cudnn_so\" /usr/lib/x86_64-linux-gnu/\ndone\n</code></pre>","tags":["1-click clusters","automation","on-demand cloud"]},{"location":"public-cloud/on-demand/getting-started/#how-do-i-open-jupyter-notebook-on-my-instance","title":"How do I open Jupyter Notebook on my instance?","text":"<p>To open Jupyter Notebook on your instance:</p> <ol> <li>In the GPU instances dashboard    ,    find the row for your instance.</li> <li>Click Launch in the Cloud IDE column.</li> </ol> <p>Tip</p> <p>Watch Lambda's GPU Cloud Tutorial with Jupyter Notebook  video on YouTube to learn more about using Jupyter Notebook on Lambda GPU Cloud instances.</p>","tags":["1-click clusters","automation","on-demand cloud"]},{"location":"public-cloud/on-demand/getting-started/#how-do-i-upgrade-python","title":"How do I upgrade Python?","text":"<p>Warning</p> <p>Upgrading Python, that is, replacing the preinstalled Python version with a newer version, will break your instance.</p> <p>Instead of upgrading Python, you should install your desired version of Python alongside the preinstalled version, and use your desired version in a virtual environment.</p> <p>To install another version of Python alongside the preinstalled version:</p> <ol> <li> <p>Run the following command:</p> <pre><code>sudo apt -y update &amp;&amp; sudo apt -y install pythonVERSION-full\n</code></pre> <p>Replace <code>VERSION</code> with the Python version you want to install, for   example, <code>3.13</code>. Make sure <code>-full</code> is appended to the Python version,   otherwise, you won't have the <code>venv</code> module needed to create Python   virtual environments.</p> <p>As a complete example, if you want to install Python version 3.13, run:</p> <pre><code>sudo apt -y update &amp;&amp; sudo apt -y install python3.13-full\n</code></pre> </li> <li> <p>Run the following command to create a Python virtual environment:</p> <pre><code>pythonVERSION -m venv VENV-NAME\n</code></pre> <p>Replace <code>VERSION</code> with the Python version you installed in the previous   step. Replace <code>VENV-NAME</code> with the name you want to give your virtual   environment.</p> </li> <li> <p>Run the following command to activate the virtual environment:</p> <pre><code>source VENV-NAME/bin/activate\n</code></pre> <p>Replace <code>VENV-NAME</code> with the name you gave your virtual environment.</p> <p>As a complete example, if you want to create a virtual environment named   <code>my-virtual-environment</code> using Python version 3.13 (installed in the example   in the previous step), run:</p> <pre><code>python3.13 -m venv my-virtual-environment\nsource my-virtual-environment/bin/activate\n</code></pre> </li> <li> <p>Run the following command to confirm that your virtual environment is using your desired Python    version:</p> <pre><code>python --version\n</code></pre> </li> </ol>","tags":["1-click clusters","automation","on-demand cloud"]},{"location":"public-cloud/on-demand/getting-started/#can-i-upgrade-to-the-latest-ubuntu-release","title":"Can I upgrade to the latest Ubuntu release?","text":"<p>Warning</p> <p>Do not run <code>sudo do-release-upgrade</code> or try to upgrade to the latest Ubuntu release. Doing so will break Jupyter Notebook and unless you have SSH access to your instance, Lambda Support  won't be able to help you recover your data.</p> <p>Jupyter Notebook on our instances is configured and tested for the preinstalled version of Python. Upgrading to the latest Ubuntu release will replace the preinstalled version of Python and make Jupyter Notebook inaccessible.</p>","tags":["1-click clusters","automation","on-demand cloud"]},{"location":"public-cloud/on-demand/getting-started/#is-it-possible-to-use-more-than-one-ssh-key","title":"Is it possible to use more than one SSH key?","text":"<p>It's possible to allow more than one SSH key to access your instance. To do so, you need to add public keys to <code>~/.ssh/authorized_keys</code>. You can do this with the <code>echo</code> command.</p> <p>Tip</p> <p>You can also import SSH keys from GitHub.</p> <p>Note</p> <p>This FAQ assumes that you've already generated another SSH key pair, that is, a private key and a public key.</p> <p>Public keys look like this:</p> <pre><code>ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIK5HIO+OQSyFjz0clkvg+48YAihYMo5J7AGKiq+9Alg8 user@hostname\n</code></pre> <p>SSH into your instance as you normally do and run:</p> <pre><code>echo 'PUBLIC-KEY' &gt;&gt; ~/.ssh/authorized_keys\n</code></pre> <p>Replace <code>PUBLIC-KEY</code> with the public key you want to add to your instance. Make sure to keep the single quotes (<code>' '</code>).</p> <p>You should now be able to log into your instance using the SSH key you just added.</p> <p>Tip</p> <p>You can make sure the public key has been added by running:</p> <pre><code>cat ~/.ssh/authorized_keys\n</code></pre> <p>The last line of output should be the public key you just added.</p>","tags":["1-click clusters","automation","on-demand cloud"]},{"location":"public-cloud/on-demand/getting-started/#what-ssh-key-formats-are-supported","title":"What SSH key formats are supported?","text":"<p>You can add SSH keys in the following formats using the dashboard or the Cloud API:</p> <ul> <li>OpenSSH (the format <code>ssh-keygen</code> uses by default when generating keys)</li> <li>RFC4716 (the format PuTTYgen uses when you save a public key)</li> <li>PKCS8</li> <li>PEM</li> </ul> <p>Note</p> <ul> <li> <p>OpenSSH keys look like:</p> <pre><code>ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIK5HIO+OQSyFjz0clkvg+48YAihYMo5J7AGKiq+9Alg8 foo@bar\n</code></pre> </li> <li> <p>RFC4716 keys begin with:</p> <pre><code>---- BEGIN SSH2 PUBLIC KEY ----\n</code></pre> </li> <li> <p>PKCS8 keys begin with:</p> <pre><code>-----BEGIN PUBLIC KEY-----\n</code></pre> </li> <li> <p>PEM keys begin with, for example:</p> <pre><code>-----BEGIN RSA PUBLIC KEY-----\n</code></pre> </li> </ul>","tags":["1-click clusters","automation","on-demand cloud"]},{"location":"public-cloud/on-demand/getting-started/#how-long-does-it-take-for-instances-to-launch","title":"How long does it take for instances to launch?","text":"<p>Single-GPU instances usually take 3-5 minutes to launch.</p> <p>Multi-GPU instances usually take 10-15 minutes to launch.</p> <p>Note</p> <p>Jupyter Notebook and Demos can take a few minutes after an instance launches to become accessible.</p> <p>Note</p> <p>Billing starts the moment an instance begins booting.</p>","tags":["1-click clusters","automation","on-demand cloud"]},{"location":"public-cloud/on-demand/getting-started/#what-network-bandwidth-does-lambda-gpu-cloud-provide","title":"What network bandwidth does Lambda GPU Cloud provide?","text":"<p>Note</p> <p>Some sites limit transfer speeds. This is known as bandwidth throttling.</p> <p>Lambda GPU Cloud doesn't limit your transfer speeds but can't control other sites' use of bandwidth throttling.</p> <p>Further, real-world network bandwidth depends on a variety of factors, including the total number of connections opened by your applications and overall network utilization.</p>","tags":["1-click clusters","automation","on-demand cloud"]},{"location":"public-cloud/on-demand/getting-started/#utah-usa-region-us-west-3","title":"Utah, USA region (us-west-3)","text":"<p>The bandwidth between instances in our Utah, USA region (us-west-3) can be up to 200 Gbps.</p> <p>The total bandwidth from this region to the Internet can be up to 20 Gbps.</p>","tags":["1-click clusters","automation","on-demand cloud"]},{"location":"public-cloud/on-demand/getting-started/#texas-usa-region-us-south-1","title":"Texas, USA region (us-south-1)","text":"<p>The bandwidth between instances in our Texas, USA region (us-south-1) can be up to 200 Gbps.</p> <p>The total bandwidth from this region to the Internet can be up to 20 Gbps.</p> <p>Note</p> <p>We're in the process of testing the network bandwidth in our other regions.</p>","tags":["1-click clusters","automation","on-demand cloud"]},{"location":"public-cloud/on-demand/getting-started/#how-do-i-learn-my-instances-private-ip-address-and-other-info","title":"How do I learn my instance's private IP address and other info?","text":"<p>You can learn your instance's private IP address with the <code>ip</code> command.</p> <p>You can learn what ports are open on your instance with the <code>nmap</code> command.</p>","tags":["1-click clusters","automation","on-demand cloud"]},{"location":"public-cloud/on-demand/getting-started/#learn-your-instances-private-ip-address","title":"Learn your instance's private IP address","text":"<p>To learn your instance's private IP address, SSH into your instance and run:</p> <pre><code>ip -4 -br addr show enp5s0\n</code></pre> <p>The above command will output, for example:</p> <pre><code>enp5s0           UP             10.19.51.71/20\n</code></pre> <p>In the above example, the instance's private IP address is 10.19.51.71.</p> <p>Tip</p> <p>If you want your instance's private IP address and only that address, run the following command instead:</p> <pre><code>ip -4 -br addr show enp5s0 | grep -Eo '(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)'\n</code></pre> <p>The above command will output, for example:</p> <pre><code>10.19.51.71\n</code></pre>","tags":["1-click clusters","automation","on-demand cloud"]},{"location":"public-cloud/on-demand/getting-started/#learn-what-ports-on-your-instance-are-publicly-accessible","title":"Learn what ports on your instance are publicly accessible","text":"<p>You can use Nmap to learn what ports on your instance are publicly accessible, that is, reachable over the Internet.</p> <p>Note</p> <p>The instructions, below, assume you're running Ubuntu on your computer.</p> <p>First, install Nmap on your computer (not on your instance) by running:</p> <pre><code>sudo apt install -y nmap\n</code></pre> <p>Next, run:</p> <pre><code>nmap -Pn INSTANCE-IP-ADDRESS\n</code></pre> <p>Replace <code>INSTANCE-IP-ADDRESS</code> with your instance's IP address, which you can get from the Cloud dashboard .</p> <p>The command will output, for example:</p> <pre><code>Starting Nmap 7.80 ( https://nmap.org ) at 2023-01-11 13:22 PST\nNmap scan report for 129.159.46.35\nHost is up (0.041s latency).\nNot shown: 999 filtered ports\nPORT   STATE SERVICE\n22/tcp open  ssh\n\nNmap done: 1 IP address (1 host up) scanned in 6.42 seconds\n</code></pre> <p>In the above example, TCP port 22 (SSH) is publicly accessible.</p> <p>Note</p> <p>If <code>nmap</code> doesn\u2019t show TCP/22 (SSH) or any other ports open, your:</p> <ul> <li>Instance might be terminated. Check the GPU Instances dashboard      to find out.</li> <li>Firewall rules might be blocking incoming connections to your instance.</li> </ul> <p>Note</p> <p><code>nmap -Pn INSTANCE-IP-ADDRESS</code> only scans the 1,000 most common TCP ports.</p>","tags":["1-click clusters","automation","on-demand cloud"]},{"location":"public-cloud/on-demand/getting-started/#how-do-i-close-my-account","title":"How do I close my account?","text":"<p>To close your Lambda GPU Cloud account:</p> <ol> <li> <p>Back up all of your data on your instances as well as in your persistent    storage file systems.</p> <p>!!! tip</p> <p>You can use rsync to back up your data.</p> </li> <li> <p>Terminate all of your instances from the Cloud dashboard or using the    Cloud API.</p> </li> <li> <p>Delete all of your persistent storage file systems.</p> </li> <li> <p>In the Cloud dashboard, under    Settings , click Close my    account. Carefully read the warning in the dialog box that appears. To    proceed with closing your account, type in close account, then click    Close account.</p> </li> </ol>","tags":["1-click clusters","automation","on-demand cloud"]},{"location":"public-cloud/on-demand/guest-agent/","title":"Installing the guest agent and getting started with Prometheus and Grafana","text":"","tags":["1-click clusters","on-demand cloud"]},{"location":"public-cloud/on-demand/guest-agent/#introduction","title":"Introduction","text":"<p>Note</p> <p>The guest agent is currently an alpha release. The guest agent is under active development and might contain bugs, incomplete features, and other issues that might affect performance, security, and reliability. The guest agent currently should only be used for testing and evaluation.</p> <p>The guest agent currently shouldn't be used in production environments.</p> <p>Please report any bugs you encounter to Lambda's Support team .</p> <p>You can install the guest agent on your Lambda Public Cloud  on-demand instances to gather metrics such as GPU and CPU utilization.</p> <p>In this tutorial, you'll install the guest agent and set up Prometheus  and Grafana  with an example dashboard so you can visualize the collected metrics.</p>","tags":["1-click clusters","on-demand cloud"]},{"location":"public-cloud/on-demand/guest-agent/#install-the-guest-agent","title":"Install the guest agent","text":"<p>To install the guest agent on an on-demand instance:</p> <p>First, SSH into your instance by running:</p> <pre><code>ssh ubuntu@IP-ADDRESS -L 3000:localhost:3000\n</code></pre> <p>Replace <code>IP-ADDRESS</code> with the actual IP address of your instance.</p> <p>Note</p> <p>The <code>-L 3000:localhost:3000</code> option enables local port forwarding. Local port forwarding is needed to access the Grafana dashboard you'll create in a later step. See the SSH man page to learn more.</p> <p>Then, download and install the guest agent by running:</p> <pre><code>curl -L https://lambdalabs-guest-agent.s3.us-west-2.amazonaws.com/scripts/install.sh | sudo bash\n</code></pre>","tags":["1-click clusters","on-demand cloud"]},{"location":"public-cloud/on-demand/guest-agent/#set-up-prometheus-and-grafana","title":"Set up Prometheus and Grafana","text":"<p>To set up Prometheus and Grafana:</p> <ol> <li> <p>Clone the Awesome Compose GitHub repository        and change into the <code>awesome-compose/prometheus-grafana</code> directory by running:</p> <pre><code>git clone https://github.com/docker/awesome-compose.git &amp;&amp; cd awesome-compose/prometheus-grafana\n</code></pre> </li> <li> <p>Obtain the private IP address of your instance by running:</p> <pre><code>ip -4 -br addr show eno1 | grep -Eo '(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)'\n</code></pre> </li> <li> <p>Edit the <code>prometheus/prometheus.yml</code> file.</p> <p>Under <code>targets</code>, change <code>localhost:9090</code> to <code>PRIVATE-IP-ADDRESS:9101</code>.</p> <p>Replace <code>PRIVATE-IP-ADDRESS</code> with the private IP address of your instance, which you obtained in the previous step.</p> <p>Note</p> <p>Make sure you're changing both the host and the port. It's frequently overlooked that the port is being changed as well as the host.</p> <p>In the <code>prometheus.yml</code> file, the <code>scrape_configs</code> key should look like:</p> <pre><code>scrape_configs:\n- job_name: prometheus\n  honor_timestamps: true\n  scrape_interval: 15s\n  scrape_timeout: 10s\n  metrics_path: /metrics\n  scheme: http\n  static_configs:\n  - targets:\n    - PRIVATE-IP-ADDRESS:9101\n</code></pre> </li> <li> <p>Edit the <code>compose.yaml</code> file and set <code>GF_SECURITY_ADMIN_PASSWORD</code> to a strong    password.</p> <p>Tip</p> <p>You can generate a strong password by running:</p> <pre><code>openssl rand -base64 16\n</code></pre> </li> <li> <p>Start Prometheus and Grafana containers on your instance by running:</p> <pre><code>sudo docker compose up -d\n</code></pre> </li> <li> <p>In your web browser, go to http://localhost:3000     and log    into Grafana. For the username, enter <code>admin</code>. For the password, enter the    password you set earlier.</p> </li> <li> <p>At the top-right of the dashboard, click the +. Then, choose Import    dashboard.</p> <p></p> </li> <li> <p>In the Import via dashboard JSON model field, enter the example JSON    model        prepared for this tutorial, then click Load. In the following screen,    click Import.</p> </li> <li> <p>You'll see a Grafana dashboard displaying:</p> <ul> <li>CPU usage</li> <li>GPU utilization</li> <li>GPU power draw</li> <li>InfiniBand transfer rates</li> <li>local storage transfer rates</li> </ul> <p></p> <p>Note</p> <p>On-demand instances, unlike 1-Click Clusters, don't use InfiniBand fabric. Accordingly, the InfiniBand transfer rates will always be zero.</p> </li> </ol>","tags":["1-click clusters","on-demand cloud"]},{"location":"tags/","title":"Tags","text":"","tags":["1-click clusters","on-demand cloud"]},{"location":"tags/#1-click-clusters","title":"1-click clusters","text":"<ul> <li>Tutorial: Getting started with training a machine learning model</li> <li>Cloud API</li> <li>Firewalls</li> <li>Teams</li> <li>Introduction</li> <li>Security posture</li> <li>How to serve the Llama 3.1 405B model using a Lambda 1-Click Cluster</li> <li>Using the Lambda Public Cloud dashboard</li> <li>Getting started</li> <li>Installing the guest agent and getting started with Prometheus and Grafana</li> </ul>","tags":["1-click clusters","on-demand cloud"]},{"location":"tags/#1-click-clusters_1","title":"1-click-clusters","text":"<ul> <li>Support</li> </ul>","tags":["1-click clusters","on-demand cloud"]},{"location":"tags/#virtualization","title":"Virtualization","text":"<ul> <li>Virtual environments and Docker containers</li> </ul>","tags":["1-click clusters","on-demand cloud"]},{"location":"tags/#api","title":"api","text":"<ul> <li>Deploying Llama 3.2 3B in a Kubernetes (K8s) cluster</li> <li>Integrating Lambda Chat into VS Code</li> <li>Deploying models with dstack</li> <li>Using SkyPilot to deploy a Kubernetes cluster</li> <li>Using the Lambda Chat Completions API</li> </ul>","tags":["1-click clusters","on-demand cloud"]},{"location":"tags/#automation","title":"automation","text":"<ul> <li>Tutorial: Getting started with training a machine learning model</li> <li>Cloud API</li> <li>Teams</li> <li>Getting started</li> </ul>","tags":["1-click clusters","on-demand cloud"]},{"location":"tags/#distributed-training","title":"distributed training","text":"<ul> <li>Introduction</li> <li>How to serve the Llama 3.1 405B model using a Lambda 1-Click Cluster</li> </ul>","tags":["1-click clusters","on-demand cloud"]},{"location":"tags/#docker","title":"docker","text":"<ul> <li>Serving the Llama 3.1 8B and 70B models using Lambda Cloud on-demand instances</li> </ul>","tags":["1-click clusters","on-demand cloud"]},{"location":"tags/#generative-ai","title":"generative ai","text":"<ul> <li>How to serve the FLUX.1 prompt-to-image models using Lambda Cloud on-demand instances</li> </ul>","tags":["1-click clusters","on-demand cloud"]},{"location":"tags/#kubernetes","title":"kubernetes","text":"<ul> <li>Deploying Llama 3.2 3B in a Kubernetes (K8s) cluster</li> <li>Using KubeAI to deploy Nous Research's Hermes 3 and other LLMs</li> <li>Using SkyPilot to deploy a Kubernetes cluster</li> </ul>","tags":["1-click clusters","on-demand cloud"]},{"location":"tags/#llama","title":"llama","text":"<ul> <li>Deploying Llama 3.2 3B in a Kubernetes (K8s) cluster</li> <li>Using KubeAI to deploy Nous Research's Hermes 3 and other LLMs</li> <li>Serving the Llama 3.1 8B and 70B models using Lambda Cloud on-demand instances</li> <li>Integrating Lambda Chat into VS Code</li> <li>Deploying models with dstack</li> <li>Using the Lambda Chat Completions API</li> </ul>","tags":["1-click clusters","on-demand cloud"]},{"location":"tags/#llm","title":"llm","text":"<ul> <li>Deploying Llama 3.2 3B in a Kubernetes (K8s) cluster</li> <li>Using KubeAI to deploy Nous Research's Hermes 3 and other LLMs</li> <li>Serving the Llama 3.1 8B and 70B models using Lambda Cloud on-demand instances</li> <li>Integrating Lambda Chat into VS Code</li> <li>Deploying models with dstack</li> <li>Using the Lambda Chat Completions API</li> </ul>","tags":["1-click clusters","on-demand cloud"]},{"location":"tags/#managed-kubernetes","title":"managed kubernetes","text":"<ul> <li>Overview</li> <li>Getting started</li> </ul>","tags":["1-click clusters","on-demand cloud"]},{"location":"tags/#on-demand-cloud","title":"on-demand cloud","text":"<ul> <li>Tutorial: Getting started with training a machine learning model</li> <li>Cloud API</li> <li>Firewalls</li> <li>Teams</li> <li>Using the Lambda Public Cloud dashboard</li> <li>Getting started</li> <li>Installing the guest agent and getting started with Prometheus and Grafana</li> </ul>","tags":["1-click clusters","on-demand cloud"]},{"location":"tags/#private-cloud","title":"private cloud","text":"<ul> <li>Introduction</li> <li>Overview</li> <li>Getting started</li> </ul>","tags":["1-click clusters","on-demand cloud"]},{"location":"tags/#programming","title":"programming","text":"<ul> <li>Integrating Lambda Chat into VS Code</li> </ul>","tags":["1-click clusters","on-demand cloud"]},{"location":"tags/#scalar","title":"scalar","text":"<ul> <li>Getting started</li> </ul>","tags":["1-click clusters","on-demand cloud"]},{"location":"tags/#security","title":"security","text":"<ul> <li>Security posture</li> </ul>","tags":["1-click clusters","on-demand cloud"]},{"location":"tags/#stable-diffusion","title":"stable diffusion","text":"<ul> <li>How to serve the FLUX.1 prompt-to-image models using Lambda Cloud on-demand instances</li> </ul>","tags":["1-click clusters","on-demand cloud"]},{"location":"tags/#storage","title":"storage","text":"<ul> <li>Filesystems</li> </ul>","tags":["1-click clusters","on-demand cloud"]},{"location":"tags/#support","title":"support","text":"<ul> <li>Support</li> </ul>","tags":["1-click clusters","on-demand cloud"]},{"location":"tags/#troubleshooting","title":"troubleshooting","text":"<ul> <li>Troubleshooting and debugging</li> <li>Using the Lambda bug report to troubleshoot your system</li> </ul>","tags":["1-click clusters","on-demand cloud"]},{"location":"tags/#vector","title":"vector","text":"<ul> <li>Getting started</li> <li>Troubleshooting Workstations and Desktops</li> </ul>","tags":["1-click clusters","on-demand cloud"]},{"location":"tags/#vector-one","title":"vector one","text":"<ul> <li>Getting started</li> <li>Troubleshooting Workstations and Desktops</li> </ul>","tags":["1-click clusters","on-demand cloud"]},{"location":"tags/#vector-pro","title":"vector pro","text":"<ul> <li>Getting started</li> <li>Troubleshooting Workstations and Desktops</li> </ul>","tags":["1-click clusters","on-demand cloud"]}]}